---
title: "01 - Introduction to nn2poly"
output:
  rmarkdown::html_vignette:
    toc: yes
    fig_width: 6
vignette: >
  %\VignetteIndexEntry{01 - Introduction to nn2poly}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "figure/nn2poly-01-"
)
```



# Overall package goal

The main objective of `nn2poly` is to obtain a representation of a feed forward artificial neural network (like a multilayered perceptron) in terms of a polynomial representation. The coefficients of such polynomials are obtained by applying a Taylor expansion at each activation function in the neural network, and combining that with its trained weights and combinatorial properties. More information with the theoretical insights about the underlying mathematical process used to build this relationship can be found in the following references:
* Initial development of the idea for a single hidden layer neural network in this [article](https://doi.org/10.1016/j.neunet.2021.04.036) or its free access [arXiv preprint](https://doi.org/10.48550/arXiv.2102.03865).
* Extension to deeper layers and proper formulation of the NN2Poly method in this [arXiv preprint](https://doi.org/10.48550/arXiv.2112.11397).


# Simple example

This vignette presents the basic usage of the package with a simple regression case, using `tensorflow` to train a simple neural network on simulated polynomial data and then using `nn2poly` to obtain the desired polynomial that approximately represents the neural network.

Please note that the approximations employed by NN2Poly require that some constraints are imposed on the neural network. In this example we will not delve into this matter, but further information and tools regarding this step can be found in `vignette("nn2poly-02-constraints")`, where also uses the auxiliary package `nn2poly.tools` is also introduced.

Furthermore, note that `nn2poly` is not limited to a special deep learning framework (like `tensorflow` or `pytorch`), asthe only requisite is to be able to get the neural networks weights in the desired matrix (or list of matrices) form to be used as input.




```{r setup}
library(nn2poly)
library(keras)
tensorflow::tf$random$set_seed(42)
```



## Data generation

In order to show the most common application of `nn2poly`, we will be solving a regression problem on the Boston dataset, included in the `keras`package.

```{r}
boston_housing <- dataset_boston_housing()

c(train_x, train_y) %<-% boston_housing$train
c(test_x, test_y) %<-% boston_housing$test
```

The data needs to be scaled, both for training the NN and for the `nn2poly` algorithm to work properly. In the theoretical foundation of this method, the scaling assumed is to the $[-1,1]$ interval, so we will use it: 

```{r}
# Join the predictor variables (x) and the response (y)
train <- as.data.frame(train_x)
train$Y <- matrix(train_y, ncol = 1)

test <- as.data.frame(test_x)
test$Y <- matrix(test_y, ncol = 1)

# Use the train data to obtain the scaling parameters, then apply them both to 
# test and train
maxs <- apply(train, 2, max)
mins <- apply(train, 2, min) 

train <- as.data.frame(scale(train, 
                             center = mins + (maxs - mins) / 2, 
                             scale = (maxs - mins) / 2))
test <- as.data.frame(scale(test, 
                             center = mins + (maxs - mins) / 2, 
                             scale = (maxs - mins) / 2))

# Divide again in x and y
train_x <- as.matrix(subset(train, select = -c(Y)))
train_y <- as.matrix(train$Y)

test_x <- as.matrix(subset(test, select = -c(Y)))
test_y <- as.matrix(test$Y)

# Define the dimension p of the problem:
p <- dim(train_x)[2]
```


# Original neural network

The method is expected to be applied to a given trained densely connected feed forward neural network (NN from now on), also referred as multilayer perceptron (MLP). Therefore, this step is completely **optional** and can be skipped if any preferred method has been used to train a NN and there is an already given NN and its weights. 

In order to present an example, here we will create and train a NN. Our choice will be to use the `keras` framework to build and train it.

----
*Note*: It is important to note that in order to avoid asymptotic behavior of the method, it is useful to impose some kind of constraint when training the neural network weights. This is covered in VIGNETTE 2
----

First, we build the model

```{r}
nn <- keras_model_sequential()

nn %>% layer_dense(units = 30,
                  activation = "softplus",
                  input_shape = p)
  
nn %>% layer_dense(units = 30,
                  activation = "softplus")

nn %>% layer_dense(units = 1,
                  activation = "linear")

nn
```

Compile the model:
```{r}
compile(nn,
        loss = "mse",
        optimizer = optimizer_adam(),
        metrics = "mse")
```

And train it:
```{r}
history <- fit(nn,
               train_x,
               train_y,
               verbose = 0,
               epochs = 200,
               validation_split = 0.3
)
```

We can visualize the training process:
```{r history}
plot(history)
```

----
Visualización de la NN, cambiar la funcion:
----


```{r comparison-y-nn}
# Obtain the predicted values with the NN to compare them
prediction_NN <- predict(nn, test_x)

plot_NN_PR_comparison(test_y, prediction_NN)
```


# Obtaining the polynomial regression

After the NN has been trained, using any chosen method by the user, the parameters have to be extracted and reshaped, if needed, to match the expected input of the function `nn2poly_algorithm()`. This input consists in the following objects:

* `weights_list` A list of matrices with a weight matrix at each layer. The weights matrices should be of dimension ((1+input) * output) where the first row corresponds to the bias vector, and the rest of the rows correspond to each of the ordered vector weights associated to each input.
* `af_string_list` A list of strings with the names of the activation functions at each layer.
* `q_taylor_vector` A vector of integers containing the order of the Taylor expansion performed at each layer. If the output layer has a linear activation function, then the last value should be 1.

Following the example of the NN that we created previously, we need to extract its weights and biases and reshape them. Particularly, the `keras` framework by default separates kernel weights matrices of dimension (input * output) and bias vectors (1 * output), so we need to add the bias as the first row of a matrix ((1+input) * output).


```{r}
keras_weights <- keras::get_weights(nn)

# Due to keras giving weights separated from the bias, we have twice the 
# elements that we want:
n <- length(keras_weights)/2
nn_weights <- vector(mode = "list", length = n)
for (i in 1:n){
  nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]])
}
```


The activation functions that we used can be stored as:
```{r}
af_string_list <- list("softplus","softplus", "linear")
```


And finally the order of the Taylor approximation that we are going to choose is 3 at each hidden layer.

```{r}
q_taylor_vector <- c(2, 2,  1) 
```

When the input is in the desired shape, the method can be applied finally:

```{r}
all_layers_coeffs <- nn2poly_algorithm(
  weights_list = nn_weights,
  af_string_list = af_string_list,
  q_taylor_vector = q_taylor_vector
)

labels <- all_layers_coeffs[[length(all_layers_coeffs)]][[1]]
coeffs <- all_layers_coeffs[[length(all_layers_coeffs)]][[2]]
```

---
TODO: Poner opción para que la función no guarde los coeficientes intermedios
---

We can have a glimpse at how the coefficients of the pollynomial regression (PR) are stored.
```{r}
coeffs[1:7]
```


# Visualising the results

After using the algorithm, it is advisable to always check that the predictions obtained with the new polynomial regression do not differ too much from the original neural network predictions (and in case they differ, we can also try to find why by checking the Taylor expansions).To help with that, a couple of functions are included that allow us to plot the results.

First of all, after obtaining the PR coefficients, we want to use them to predict the response variable $Y$, which can be done with the function `evaluate_PR()`:

```{r}
# Obtain the predicted values for the test data with our Polynomial Regression
n_test <- length(test_y)
prediction_PR <- rep(0, n_test)

for (i in 1:n_test) {
  prediction_PR[i] <- eval_poly(test[i, seq(p)], labels = labels, coeffs = coeffs)
}

# Obtain the predicted values with the NN to compare them
prediction_NN <- predict(nn, test_x)
```


A simple plot comparing the PR and NN predictions can be obtained with `plot_NN_PR_comparison()`, where the red diagonal line represents where a perfect relationship between the NN and the PR would be obtained. In this example, as the theoretical weights constraints have not been imposed, we can observe how the approximation is not perfect:


```{r comparison-pr-nn}
plot_NN_PR_comparison(prediction_PR, prediction_NN)
```

Finally, a convenient plot to show how the algorithm is affected by each layer can be obtained with `plot_taylor_and_activation_potentials()`, where the activation potentials at each neuron are computed and presented over the Taylor expansion approximation of the activation function at each layer:

```{r potentials}
plot_taylor_and_activation_potentials(data = train,
                                    weights_list = nn_weights,
                                    af_string_list = af_string_list,
                                    q_taylor_vector = q_taylor_vector)
```



















