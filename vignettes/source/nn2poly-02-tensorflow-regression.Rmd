---
title: "02 - Regression example using tensorflow"
author: "Pablo Morala"
output:
  rmarkdown::html_vignette:
    toc: yes
    fig_width: 6
date: "`r Sys.Date()`"
vignette: >
  %\VignetteIndexEntry{02 - Regression example using tensorflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE, warning = FALSE, error = FALSE,
  fig.cap = "",
  fig.path = "figure/nn2poly-02-"
)
```

# This vignette's goal

After showing how to use `nn2poly` in its default version in `vignette("nn2poly-01-introduction")`, here we will present how to use specific methods related to `keras` and `tensorflow` that allow for an easier and smoother use of `nn2poly` with that deep learning framework. Furthermore, we will sow how to impose the needed weight constraints in `tensorflow` during training to have accurate results and compare those results with an unconstrained neural network.

In this vignette we will focus on a simple regression example and a classification one will be covered in `vignette("nn2poly-03-tensorflow-classification")`. 

```{r setup}
library(nn2poly)
library(keras) # Note that this also loads tensorflow.

# This sets all needed seeds
tensorflow::set_random_seed(1)
```


We will solve again two simple examples, regression and classification.

# Simple regression example

## Simulated data generation

We will simulate polynomial data from the following polynomial: $4x_1 - 3 x_2x_3$. Data needs to be scaled to the $[-1,1]$ interval.

```{r reg-data-polynomial}
# Define the desired polynomial for the simulated data
polynomial <- list()
polynomial$labels <- list(c(1), c(2,3))
polynomial$values <- c(4,-3)

```

```{r reg-data-generation}
# Define number of variables p and sample n
p <- 3
n_sample <- 500

# Predictor variables
X <- matrix(0,n_sample,p)
for (i in 1:p){
  X[,i] <- rnorm(n = n_sample,0,1)
}

# Response variable + small error term
Y <- as.vector(eval_poly(X,polynomial)) + stats::rnorm(n_sample, 0, 0.1)

# Store all as a data frame
data <- as.data.frame(cbind(X, Y))
head(data)


```

```{r reg-data-scaling-split}
# Data scaling
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))

# Divide in train (0.75) and test (0.25)
index <- sample(1:nrow(data), round(0.75 * nrow(data)))
train <- data[index, ]
test <- data[-index, ]

train_x <- as.matrix(train[,-(p+1)])
train_y <- as.matrix(train[,(p+1)])

test_x <- as.matrix(test[,-(p+1)])
test_y <- as.matrix(test[,(p+1)])

plot(data)
```


## Original neural networks

We will build and train two different neural networks (NNs), one with unconstrained weights (`nn1`) and another one imposing a constraint on the weights (`nn2`).

Different constraints can be tested, but the suggested constraint based on our theoretical and empirical evaluation is to use the L1 norm equal to 1, constraining each vector of weights + bias arriving to a neuron to satisfy that their L1 norm is equal or less than 1.

### Build NN 1, unconstrained

This NN will be built using standard tensorflow and keras practices, in this case with a sequential keras model without any constraint on the weights.


```{r reg-nn1-build}
nn1 <- keras_model_sequential()

nn1 %>% layer_dense(units = 100,
                  activation = "tanh",
                  input_shape = p)

nn1 %>% layer_dense(units = 100,
                  activation = "tanh")

nn1 %>% layer_dense(units = 100,
                  activation = "tanh")

nn1 %>% layer_dense(units = 1,
                  activation = "linear")

nn1
```


### Build NN 2, constrained

In order to implement the desired constraints, we provide the `add_constraints()` function, that takes the structure of a given NN (has to be a feed forward dense NN) and modifies its layers to include the constraints. This is needed because default constraints implemented in `keras` do not support to impose a constraint at the same time on the weights and the bias and have to be combined with a custom layer.

Our implementation is such that the bias on each neuron is included in the weights vector incident on that neuron, meaning that if the previous layer had $h$ neurons, then the considered weight vector including the bias at a given neuron would have dimension $h+1$, having the bias as it first element. Currently, L1 norm and L2 norm equal to 1 are implemented as options.

Note that L1 norm equal to 1 when scaling the input data to the $[-1,1]$ interval is the recommended option.

```{r}
nn2 <- add_constraints(nn1, constraint_type = "l1_norm")
nn2
```
Note how the parameters and strcuture are the same, but the layer type has been modified.


### Compile and train both NNs

After building both NNs, we compile and train both of them. Note that, as constraining the weights has trade-off in the learning speed of the NN, the `nn2` needs a higher number of epochs to properly learn from the data.


Compile and train `nn1` the model, and visualize it:

```{r reg-nn1-train}
compile(nn1,
        loss = "mse",
        optimizer = optimizer_adam(),
        metrics = "mse")

history1 <- fit(nn1,
               train_x,
               train_y,
               verbose = 0,
               epochs = 300,
               batch_size = 50,
               validation_split = 0.2
)

plot(history1)
```


```{r reg-nn2-train}
compile(nn2,
        loss = "mse",
        optimizer = optimizer_adam(),
        metrics = "mse")

history2 <- fit(nn2,
               train_x,
               train_y,
               verbose = 0,
               epochs = 5000,
               batch_size = 100,
               validation_split = 0.1
)

plot(history2)
```


```{r}
params <- nn2poly:::get_model_parameters(nn2)
nn2poly:::check_weight_constraints(params$weights_list, maxnorm = list("l1_norm", 1))
```


### Visualize both NN predictions

We can visualize the NNs predictions vs the original Y values.

```{r reg-comparison-y-nn1}
# Obtain the predicted values with the NN to compare them
prediction_NN1 <- predict(nn1, test_x)

# Diagonal plot implemented in the package to quickly visualize and compare predictions
plot_diagonal(x_axis =  prediction_NN1, y_axis =  test_y, xlab = "NN 1 prediction", ylab = "Original Y")
```


```{r reg-comparison-y-nn2}
# Obtain the predicted values with the NN to compare them
prediction_NN2 <- predict(nn2, test_x)

# Diagonal plot implemented in the package to quickly visualize and compare predictions
plot_diagonal(x_axis =  prediction_NN2, y_axis =  test_y, xlab = "NN 2 prediction", ylab = "Original Y")
```

## Using nn2poly to obtain  the polynomial

After the NN has been trained, we can directly call `nn2poly` on the keras model. Therefore, we do not need to build an object with weights and activation functions as in the default case covered in `vignette("nn2poly-01-introduction")`, and can benefit fro 


And finally the order of the Taylor approximation that we are going to choose is 8 at each hidden layer. (The final polynomial order will be limited by `forced_max_Q=3`)

```{r reg-taylor-orders}
q_taylor_vector <- c(8, 8,  1)
```

When the input is in the desired shape, the nn2poly method can be applied:

```{r reg-apply-nn2poly}

final_poly <- nn2poly(object = nn,
                      q_taylor_vector = q_taylor_vector,
                      forced_max_Q = 3)

```



## Obtaining polynomial predictions

```{r reg-polynomial-prediction}

# Obtain the predicted values for the test data with our polynomial
prediction_poly <- as.vector(eval_poly(x = test_x, poly = final_poly))

```


## Visualizing the results

Diagonal plot. In this example, as we have imposed weight constraints the approximation is better.


```{r reg-comparison-polynomial-nn}
plot_diagonal(x_axis =  prediction_NN, y_axis =  prediction_poly, xlab = "NN prediction", ylab = "Polynomial prediction")

```

We can also plot the $n$ most important coefficients in absolute value to compare which variables or interactions are more relevant in the polynomial. Note that, as data should be scaled to the $[-1,1]$ interval, interactions of order 2 or higher would usually need a higher absolute value than the lower order coefficients be more relevant.

```{r reg-n-important}
plot_n_important_coeffs(final_poly, 8)
```


Representing the activation potentials, it can be seen that now they are centered around zero, where nn2poly works best.


```{r reg-potentials}
plot_taylor_and_activation_potentials(data = train,
                                    weights_list = nn_weights,
                                    af_string_list = af_string_list,
                                    q_taylor_vector = q_taylor_vector,
                                    forced_max_Q = 3,
                                    my_max_norm = my_max_norm)
```

# Simple classification example

In this example, instead of a regression problem we will show a classification example, where a NN will be trained to classify species with the `iris` dataset, and then nn2poly will be employed to obtain a polynomial for each species.


## Data preparation

We will load the `iris` and scale the data:

```{r class-data-loading}
# Load the data
data(iris)

# Change response to numeric. In this case, Species was already numeric,
# but this step is needed if it is a factor variable.
iris$Species <- as.numeric(iris$Species)

# Define dimension p (number of predictor variables)
p <- dim(iris)[2] - 1

# Define objective classes
n_class <- max(iris[,(p+1)])

# Move objective classes from (1:3) to (0:2), needed for tensorflow
iris[,(p+1)] <- iris[,(p+1)] - 1
```

```{r class-data-scaling}
# Scale the data in the [-1,1] interval and separate train and test
# Only the predictor variables are scaled, not the response as those will be
# the different classes.
iris_x <- iris[,-(p+1)]
maxs <- apply(iris_x, 2, max)
mins <- apply(iris_x, 2, min)
data_x_scaled <- as.data.frame(scale(iris_x, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))
data <- cbind(data_x_scaled, iris[,(p+1)])

# Divide in train (0.75) and test (0.25)
index <- sample(1:nrow(data), round(0.75 * nrow(data)))
train <- data[index, ]
test <- data[-index, ]

train_x <- as.matrix(train[,-(p+1)])
train_y <- as.matrix(train[,(p+1)])

test_x <- as.matrix(test[,-(p+1)])
test_y <- as.matrix(test[,(p+1)])
```


## Original neural network

We can now train the NN, following the same procedure as in the regression problem and using the `build_keras_model` function which includes the layers with custom constrains.

First we define the parameters and keras hyperparameters:


```{r class-parameters}
# keras hyperparameters
my_loss <- loss_sparse_categorical_crossentropy(from_logits = TRUE)
my_metrics <- "accuracy"
my_optimizer <- optimizer_adam()
my_epochs <- 500
my_validation_split <- 0.2
my_verbose <- 0

# Parameters:
af_string_list <- list("tanh", "tanh", "linear")
h_neurons_vector <- c(50, 50, n_class)
my_max_norm <- list("l1_norm",1)

```

Build the model with the custom constraints, then compile and fit the model:

```{r class-build-model}
nn <- build_keras_model(p,
    af_string_list,
    h_neurons_vector,
    my_max_norm)

# Compile the model
compile(nn,
				loss = my_loss,
				optimizer = my_optimizer,
				metrics = my_metrics
)

# Fit the model
history <- fit(nn,
							 train_x,
							 train_y,
							 verbose = my_verbose,
							 epochs = my_epochs,
							 validation_split = my_validation_split,
							 batch_size = 50
)

nn
```

We can visualize the training process:

```{r class-history}
plot(history)
```

In this case, to asses the NN accuracy we have to transform the nn output into a probability:

```{r class-probability-model}
probability_model <- keras_model_sequential() %>%
  nn() %>%
  layer_activation_softmax() %>%
  layer_lambda(k_argmax)
```

And predict the results for the test data:

```{r class-comparison-y-nn}
# Obtain the predicted classes with the NN to compare them
prediction_NN_class <- predict(probability_model, test_x)

# Also, the linear output can be predicted before the probability model
prediction_NN <- predict(nn, test_x)

```
We can use here a confusion matrix to visualize the results, where we can see that the NN correctly predicts the classes of each observation:

```{r class-confusion-matrix-nn}
# Create a confusion matrix
cm <- caret::confusionMatrix(as.factor(prediction_NN_class), as.factor(test_y))
cm

```


## Using nn2poly to obtain  the polynomial

After the NN has been trained, we need to extract and reshape the parameters as explained in the regression case:

```{r class-weights}
keras_weights <- keras::get_weights(nn)

n <- length(keras_weights)

nn_weights <- keras_weights[1:(n-2)]

# Add last layer as this one has the bias separated, it is not a custom layer
nn_weights[[n-1]] <- rbind(keras_weights[[n]], keras_weights[[n-1]])
```


The activation functions that we used can be stored as:

```{r class-af}
af_string_list <- list("tanh","tanh", "linear")
```


And finally the order of the Taylor approximation that we are going to choose is 8 at each hidden layer. (The final polynomial order will be limited by `forced_max_Q=3`)

```{r class-q-taylor}
q_taylor_vector <- c(8, 8,  1)
```

When the input is in the desired shape, the nn2poly method can be applied:

```{r class-apply-nn2poly}
final_poly <- nn2poly_algorithm(
  weights_list = nn_weights,
  af_string_list = af_string_list,
  q_taylor_vector = q_taylor_vector,
  store_coeffs = FALSE,
  forced_max_Q = 3
)
```



## Obtaining polynomial predictions

```{r class-polynomial-prediction}

# Obtain the predicted values for the test data with our Polynomial Regression
prediction_poly_matrix <- eval_poly(x = test_x, poly = final_poly)

# Define probability model with keras fro the polynomial outputs
probability_poly <- keras_model_sequential() %>%
  layer_activation_softmax() %>%
  layer_lambda(k_argmax)

# Class prediction with the polynomial outputs (one row for each polynomial)
prediction_poly_class <- predict(probability_poly,t(prediction_poly_matrix))

```



## Visualising the results


```{r class-confusion-matrix-poly}

# Confussion matrix between NN class prediction and polynomial class prediction
cm <- caret::confusionMatrix(as.factor(prediction_NN_class), as.factor(prediction_poly_class))
cm

```

We can compare, for each polynomial, its output with the the linear output of the NN. In this case, as constraints have been used in training, the results are much closer than what happened in `vignette("nn2poly-01-introduction")`

```{r class-diagonal-plot}
for (i in 1:3){
  print(
    plot_diagonal(x_axis =  prediction_NN[,i],
                  y_axis =  prediction_poly_matrix[i,],
                  xlab = "NN prediction",
                  ylab = "Polynomial prediction")
        )
}
```

We can also plot the $n$ most important coefficients in absolute value (for each output polynomial) to compare which variables or interactions are more relevant in the polynomial. Note that, as data should be scaled to the $[-1,1]$ interval, interactions of order 2 or higher would usually need a higher absolute value than the lower order coefficients be more relevant.

```{r class-n-important}
plot_n_important_coeffs(final_poly, 8)
```


Finally, activation potentials can be visualized as follows, where it can be seen that the constraints have made the activations to be closer to zero than in the unrestricted case. However, there is still some dispersion in them that creates the slight deviations seen in the diagonal plots.

```{r class-potentials}
plot_taylor_and_activation_potentials(data = train,
                                    weights_list = nn_weights,
                                    af_string_list = af_string_list,
                                    q_taylor_vector = q_taylor_vector,
                                    forced_max_Q = 3,
                                    my_max_norm = my_max_norm)
```







