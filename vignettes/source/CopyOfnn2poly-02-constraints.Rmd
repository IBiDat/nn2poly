---
title: "02 - Constraining the weights in the NN"
author: "Pablo Morala"
output:
  rmarkdown::html_vignette:
    toc: yes
    fig_width: 6
date: "`r Sys.Date()`"
vignette: >
  %\VignetteIndexEntry{02 - Constraining the weights in the NN}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "figure/nn2poly-02-"
)
```

Initial setup:

```{r setup}
library(nn2poly.tools)
library(nn2poly)
library(keras)
library(tensorflow)

# For reproducibility
set.seed(1)
tensorflow::tf$random$set_seed(1)
```





# .---------------------------------------------------------------------


In order to show the most common application of `nn2poly`, we will be solving a regression problem on the Boston dataset, included in the `keras`package.

```{r}
boston_housing <- dataset_boston_housing()

c(train_x, train_y) %<-% boston_housing$train
c(test_x, test_y) %<-% boston_housing$test
```

The data needs to be scaled, both for training the NN and for the `nn2polyPKG` algorithm to work properly. In the theoretical foundation of this method, the scaling assumed is to the $[-1,1]$ interval, so we will use it: 

```{r}
# Join the predictor variables (x) and the response (y)
train <- as.data.frame(train_x)
train$Y <- matrix(train_y, ncol = 1)

test <- as.data.frame(test_x)
test$Y <- matrix(test_y, ncol = 1)

# Use the train data to obtain the scaling parameters, then apply them both to 
# test and train
maxs <- apply(train, 2, max)
mins <- apply(train, 2, min) 

train <- as.data.frame(scale(train, 
                             center = mins + (maxs - mins) / 2, 
                             scale = (maxs - mins) / 2))
test <- as.data.frame(scale(test, 
                             center = mins + (maxs - mins) / 2, 
                             scale = (maxs - mins) / 2))

# Divide again in x and y
train_x <- as.matrix(subset(train, select = -c(Y)))
train_y <- as.matrix(train$Y)

test_x <- as.matrix(subset(test, select = -c(Y)))
test_y <- as.matrix(test$Y)

# Define the dimension p of the problem:
p <- dim(train_x)[2]
```


# Original neural network

The method is expected to be applied to a given trained densely connected feed forward neural network (NN from now on), also referred as multilayer perceptron (MLP). Therefore, this step is completely **optional** and can be skipped if any preferred method has been used to train a NN and there is an already given NN and its weights. 

In order to present an example, here we will create and train a NN. Our choice will be to use the `keras` framework to build and train it.

----
*Note*: It is important to note that in order to avoid asymptotic behavior of the method, it is useful to impose some kind of constraint when training the neural network weights. This is covered in VIGNETTE 2
----

First, we build the model


```{r}

# keras hyperparameters 
  my_loss <- "mse"
  my_metrics <- "mse"
  my_optimizer <- optimizer_rmsprop()
  my_epochs <- 500
  my_validation_split <- 0.3
  my_verbose <- 0

  # Parameters:
af_string_list <- list("softplus", "softplus", "linear")

q_taylor_vector <- c(2, 2, 1) # If the output is linear, the last value should be 1 so it doesnt affect the product

h_neurons_vector <- c(40, 40, 1) # If the output is linear, the last value should be 1

my_max_norm <- list("l2_norm",1)
  
```


```{r}
nn <- build_keras_model(p,
    af_string_list,
    h_neurons_vector,
    my_max_norm)
```

```{r}
# Compile the model
	compile(nn,
					loss = my_loss,
					optimizer = my_optimizer,
					metrics = my_metrics
	)

	# Fit the model
	history <- fit(nn,
								 train_x,
								 train_y,
								 verbose = my_verbose,
								 epochs = my_epochs,
								 validation_split = my_validation_split,
								 batch_size = 500
	)
```



We can visualize the training process:
```{r history}
plot(history)
```

```{r comparison-y-nn}
# Obtain the predicted values with the NN to compare them
prediction_NN <- predict(nn, test_x)

plot_NN_PR_comparison(test_y, prediction_NN)
```

# Obtaining the polynomial regression

After the NN has been trained, using any chosen method by the user, the parameters have to be extracted and reshaped, if needed, to match the expected input of the function `nn2poly_algorithm()`. This input consists in the following objects:

* `weights_list` A list of matrices with a weight matrix at each layer. The weights matrices should be of dimension ((1+input) * output) where the first row corresponds to the bias vector, and the rest of the rows correspond to each of the ordered vector weights associated to each input.
* `af_string_list` A list of strings with the names of the activation functions at each layer.
* `q_taylor_vector` A vector of integers containing the order of the Taylor expansion performed at each layer. If the output layer has a linear activation function, then the last value should be 1.

Following the example of the NN that we created previously, we need to extract its weights and biases and reshape them. Particularly, the `keras` framework by default separates kernel weights matrices of dimension (input * output) and bias vectors (1 * output), so we need to add the bias as the first row of a matrix ((1+input) * output).


```{r}
keras_weights <- keras::get_weights(nn)

# En este caso solo hace falta cambiar lo de la ultima capa
# ARREGLAR ESTO PARA QUE SEA CONSISTENTE
n <- length(keras_weights)

nn_weights <- keras_weights[1:(n-2)]

nn_weights[[n-1]] <- rbind(keras_weights[[n]], keras_weights[[n-1]])


```

When the input is in the desired shape, the method can be applied finally:

```{r}
historical_coeffs <- nn2poly_algorithm(
  weights_list = nn_weights,
  af_string_list = af_string_list,
  q_taylor_vector = q_taylor_vector
)

labels <- historical_coeffs[[length(historical_coeffs)]][[1]]
coeffs <- historical_coeffs[[length(historical_coeffs)]][[2]]
```

----
TODO: Poner opción para que la función no guarde los coeficientes intermedios
----

We can have a glimpse at how the coefficients of the pollynomial regression (PR) are stored.
```{r}
coeffs[1:7]
```


# Visualising the results

After using the algorithm, it is advisable to always check that the predictions obtained with the new polynomial regression do not differ too much from the original neural network predictions (and in case they differ, we can also try to find why by checking the Taylor expansions).To help with that, a couple of functions are included that allow us to plot the results.

First of all, after obtaining the PR coefficients, we want to use them to predict the response variable $Y$, which can be done with the function `eval_poly()`:

```{r}
# Obtain the predicted values for the test data with our Polynomial Regression
n_test <- length(test_y)
prediction_PR <- rep(0, n_test)

for (i in 1:n_test) {
  prediction_PR[i] <- eval_poly(test[i, seq(p)], labels = labels, coeffs = coeffs)
}

# Obtain the predicted values with the NN to compare them
prediction_NN <- predict(nn, test_x)
```


A simple plot comparing the PR and NN predictions can be obtained with `plot_NN_PR_comparison()`, where the red diagonal line represents where a perfect relationship between the NN and the PR would be obtained. In this example, as the theoretical weights constraints have not been imposed, we can observe how the approximation is not perfect:


```{r comparison-pr-nn}
plot_NN_PR_comparison(prediction_PR, prediction_NN)
```

Finally, a convenient plot to show how the algorithm is affected by each layer can be obtained with `plot_taylor_and_activation_potentials()`, where the activation potentials at each neuron are computed and presented over the Taylor expansion approximation of the activation function at each layer:

```{r}
plot <- plot_taylor_and_activation_potentials(data = train,
                                    weights_list = nn_weights,
                                    af_string_list = af_string_list,
                                    q_taylor_vector = q_taylor_vector)
```




```{r}
plot

```






