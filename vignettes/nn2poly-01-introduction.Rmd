---
title: "01 - Introduction to nn2poly"
author: "Pablo Morala"
output:
  rmarkdown::html_vignette:
    toc: yes
    fig_width: 6
date: "`r Sys.Date()`"
vignette: >
  %\VignetteIndexEntry{01 - Introduction to nn2poly}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Initial setup:
```{r setup}
library(nn2poly)
library(keras)
tensorflow::set_random_seed(42)
```


# Overall Goal

The main objective of this package is obtaining a representation of a feed forward neural network in terms of a polynomial regression. This is achieved by applying a Taylor extension at each activation function in the neural network, and combining that with its trained weights, the coefficients of a polynomial regression are obtained.

The main insights about the mathematical process to build this relationship can be found in this [article](https://doi.org/10.1016/j.neunet.2021.04.036) or its free access option through the orginal [arXiv version](https://arxiv.org/abs/2102.03865.).

This vignette presents the direct usage of the package with a simple case. For additional tools to build neural networks with the needed conditions that ensure the correct theoretical behavior (constraining the weights and biases), check the VIGNETTE 2 that also uses the extension package `nn2poly.tools`.


# Data preparation

In order to show the most common application of `nn2poly`, we will be solving a regression problem on the Boston dataset, included in the `keras`package.

```{r}
boston_housing <- dataset_boston_housing()

c(train_x, train_y) %<-% boston_housing$train
c(test_x, test_y) %<-% boston_housing$test
```

The data needs to be scaled, both for training the NN and for the `nn2poly` algorithm to work properly. In the theoretical foundation of this method, the scaling assumed is to the $[-1,1]$ interval, so we will use it: 

```{r}
# Join the predictor variables (x) and the response (y)
train <- as.data.frame(train_x)
train$Y <- matrix(train_y, ncol = 1)

test <- as.data.frame(test_x)
test$Y <- matrix(test_y, ncol = 1)

# Use the train data to obtain the scaling parameters, then apply them both to 
# test and train
maxs <- apply(train, 2, max)
mins <- apply(train, 2, min) 

train <- as.data.frame(scale(train, 
                             center = mins + (maxs - mins) / 2, 
                             scale = (maxs - mins) / 2))
test <- as.data.frame(scale(test, 
                             center = mins + (maxs - mins) / 2, 
                             scale = (maxs - mins) / 2))

# Divide again in x and y
train_x <- as.matrix(subset(train, select = -c(Y)))
train_y <- as.matrix(train$Y)

test_x <- as.matrix(subset(test, select = -c(Y)))
test_y <- as.matrix(test$Y)

# Define the dimension p of the problem:
p <- dim(train_x)[2]
```


# Original neural network

The method is expected to be applied to a given trained densely connected feed forward neural network (NN from now on), also referred as multilayer perceptron (MLP). Therefore, this step is completely **optional** and can be skipped if any preferred method has been used to train a NN and there is an already given NN and its weights. 

In order to present an example, here we will create and train a NN. Our choice will be to use the `keras` framework to build and train it.

----
*Note*: It is important to note that in order to avoid asymptotic behavior of the method, it is useful to impose some kind of constraint when training the neural network weights. This is covered in VIGNETTE 2
----

First, we build the model

```{r}
nn <- keras_model_sequential()

nn %>% layer_dense(units = 30,
                  activation = "softplus",
                  input_shape = p)
  
nn %>% layer_dense(units = 30,
                  activation = "softplus")

nn %>% layer_dense(units = 1,
                  activation = "linear")

nn
```

Compile the model:
```{r}
compile(nn,
        loss = "mse",
        optimizer = optimizer_adam(),
        metrics = "mse")
```

And train it:
```{r}
history <- fit(nn,
               train_x,
               train_y,
               verbose = 0,
               epochs = 50,
               validation_split = 0.3
)
```

We can visualize the training process:
```{r}
plot(history)
```

----
Visualización de la NN, cambiar la funcion:
----


```{r}
# Obtain the predicted values with the NN to compare them
prediction_NN <- predict(nn, test_x)

plot_NN_PR_comparison(test_y, prediction_NN)
```


# Obtaining the polynomial regression

After the NN has been trained, using any chosen method by the user, the parameters have to be extracted and reshaped, if needed, to match the expected input of the function `nn2poly_algorithm()`. This input consists in the following objects:

* `weights_list` A list of matrices with a weight matrix at each layer. The weights matrices should be of dimension ((1+input) * output) where the first row corresponds to the bias vector, and the rest of the rows correspond to each of the ordered vector weights associated to each input.
* `af_string_list` A list of strings with the names of the activation functions at each layer.
* `q_taylor_vector` A vector of integers containing the order of the Taylor expansion performed at each layer. If the output layer has a linear activation function, then the last value should be 1.

Following the example of the NN that we created previously, we need to extract its weights and biases and reshape them. Particularly, the `keras` framework by default separates kernel weights matrices of dimension (input * output) and bias vectors (1 * output), so we need to add the bias as the first row of a matrix ((1+input) * output).


```{r}
keras_weights <- keras::get_weights(nn)

# Due to keras giving weights separated from the bias, we have twice the 
# elements that we want:
n <- length(keras_weights)/2
nn_weights <- vector(mode = "list", length = n)
for (i in 1:n){
  nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]])
}
```


The activation functions that we used can be stored as:
```{r}
af_string_list <- list("softplus","softplus", "linear")
```


And finally the order of the Taylor approximation that we are going to choose is 3 at each hidden layer.

```{r}
q_taylor_vector <- c(2, 2,  1) 
```

When the input is in the desired shape, the method can be applied finally:

```{r}
historical_coeffs <- nn2poly_algorithm(
  weights_list = nn_weights,
  af_string_list = af_string_list,
  q_taylor_vector = q_taylor_vector
)

coeffs <- historical_coeffs[[length(historical_coeffs)]][[1]]
```

---
TODO: Poner opción para que la función no guarde los coeficientes intermedios
---

We can have a glimpse at how the coefficients of the pollynomial regression (PR) are stored.
```{r}
coeffs[,1:7]
```


# Visualising the results

After using the algorithm, it is advisable to always check that the predictions obtained with the new polynomial regression do not differ too much from the original neural network predictions (and in case they differ, we can also try to find why by checking the Taylor expansions).To help with that, a couple of functions are included that allow us to plot the results.

First of all, after obtaining the PR coefficients, we want to use them to predict the response variable $Y$, which can be done with the function `evaluate_PR()`:

```{r}
# Obtain the predicted values for the test data with our Polynomial Regression
n_test <- length(test_y)
prediction_PR <- rep(0, n_test)

for (i in 1:n_test) {
  prediction_PR[i] <- evaluate_PR(test[i, seq(p)], coeffs)
}

# Obtain the predicted values with the NN to compare them
prediction_NN <- predict(nn, test_x)
```


A simple plot comparing the PR and NN predictions can be obtained with `plot_NN_PR_comparison()`, where the red diagonal line represents where a perfect relationship between the NN and the PR would be obtained. In this example, as the theoretical weights constraints have not been imposed, we can observe how the approximation is not perfect:


```{r}
plot_NN_PR_comparison(prediction_PR, prediction_NN)
```

Finally, a convenient plot to show how the algorithm is affected by each layer can be obtained with `plot_taylor_and_synpatic_potentials()`, where the synaptic potentials at each neuron are computed and presented over the Taylor expansion approximation of the activation function at each layer:

```{r}
plot_taylor_and_synpatic_potentials(data = train,
                                    weights_list = nn_weights,
                                    af_string_list = af_string_list,
                                    q_taylor_vector = q_taylor_vector)
```



















