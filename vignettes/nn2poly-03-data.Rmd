---
title: "03 - Generating data to test the implementation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{03 - Generating data to test the implementation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



This vignette will generate data necessary for the package examples and unit tests but does not explicitly showcase the usage of the package functions. 

There will be 4 examples generated and stored as `nn2poly_exampleX` where `X` denotes the number of the example, from 0 to 4.



```r
library(nn2poly)
library(nn2poly.tools)
library(keras)
library(tensorflow)
tensorflow::tf$random$set_seed(42) 
set.seed(42)
```

## Example 0:

This example is a simplified and different from the following ones. The weights will be manually set to be matrices of 1's, so we can manually compute some of the coefficients obtained internally by the algorithm and use them to check it works as expected.

- The parameters for `nn2poly_example0`:
  - Number of variables $p$: 2.
  - Number of hidden layers: 2.
  - Single linear output (regression).
  - Taylor order at each hidden layer: 2.
  - Softplus activation function.



```r

# Main parameters
p <- 2
my_fun <-"softplus"
my_q_taylor <- 2

af_string_list <- list(my_fun, my_fun, "linear")
q_taylor_vector <- c(my_q_taylor, my_q_taylor, 1) # If the output is linear, the last value should be 1 so it doesn't affect the product

af_derivatives_list <- nn2poly::obtain_derivatives_list(af_string_list,q_taylor_vector)

# Directly assgin the wegiths as 1's to compute everything.

weights_list <- vector(mode="list", length= 3)
weights_list[[1]] <- matrix(1,3,2)
weights_list[[2]] <- matrix(1,3,2)
weights_list[[3]] <- matrix(1,3,1)


# Store all the data needed for the examples --------------------------
nn2poly_example0 <- vector(mode="list", length= 0)


nn2poly_example0$weights_list <- weights_list
nn2poly_example0$af_string_list <- af_string_list
nn2poly_example0$af_derivatives_list <- af_derivatives_list
nn2poly_example0$q_taylor_vector <- q_taylor_vector

#save(nn2poly_example0, file ="data/nn2poly_example0.rda")

```

## Example 1:

2 variables, original order 2, 2 hidden layers, taylor order 2, LINEAR (so using Taylor should be exact), 2 hidden neurons per layer. 


```r

# Main parameters
p <- 2
q_original <- 2
my_fun <-"linear"
my_q_taylor <- 2
my_h <- 2

af_string_list <- list(my_fun, my_fun, "linear")
q_taylor_vector <- c(my_q_taylor, my_q_taylor, 1) # If the output is linear, the last value should be 1 so it doesn't affect the product
h_neurons_vector <- c(my_h, my_h,  1) # If the output is linear, the last value should be 1
my_max_norm <- list("l1_norm", 1)
af_derivatives_list <- nn2poly::obtain_derivatives_list(af_string_list,q_taylor_vector)

# Fixed parameters for the data generation
n_sample <- 400
mean_range <- c(-100, 100)
beta_range <- c(-5, 5)
error_var <- 0.2

# Data generation:
data_generated <- generate_normal_data(n_sample, p, q_original, mean_range, beta_range, error_var)
data <- data_generated$data
original_betas <- data_generated$original_betas

# Scale the data in the desired interval and separate train and test
scale_method <- "-1,1"
data_scaled <- scale_data(data, scale_method)
aux <- divide_train_test(data_scaled, train_proportion = 0.75)
train <- aux$train
test <- aux$test

# NN (keras) hyperparameters
my_loss <- "mse"
my_metrics <- "mse"
my_optimizer <- optimizer_adamax()
my_epochs <- 300
my_validation_split <- 0.3
my_verbose <- 0

# NN training ---------------------------------
  
# Divide again in x and y to train the NN with keras
train_x <- as.matrix(subset(train, select = -c(p+1)))
train_y <- as.matrix(subset(train, select = c(p+1)))

test_x <- as.matrix(subset(test, select = -c(p+1)))
test_y <- as.matrix(subset(test, select = c(p+1)))
  
# Build the nn
nn <- build_keras_model(
  p,
  af_string_list,
  h_neurons_vector,
  my_max_norm
)
  
# Compile the model
compile(nn,
        loss = my_loss,
        optimizer = my_optimizer,
        metrics = my_metrics
)
  
# Fit the model
history <- fit(nn,
               train_x,
               train_y,
               verbose = my_verbose,
               epochs = my_epochs,
               validation_split = my_validation_split,
               batch_size = 500
)
  
# Visualize the training process
plot(history)


# Extract the weights:
  keras_weights <- keras::get_weights(nn)
  
  # En este caso solo hace falta cambiar lo de la ultima capa
  # ARREGLAR ESTO PARA QUE SEA CONSISTENTE
  
  n <- length(keras_weights)
  
  if(my_max_norm[[1]]=="no_constraint"){

    n2 <- n/2

    nn_weights <- vector(mode = "list", length = n2)
    for (i in 1:n2){
      nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]])
   }
    
  } else {
    nn_weights <- keras_weights[1:(n - 2)]
    
    nn_weights[[n - 1]] <- rbind(keras_weights[[n]], keras_weights[[n - 1]])
  }
  
weights_list <- nn_weights





# Store all the data needed for the examples --------------------------
nn2poly_example1 <- vector(mode="list", length= 0)


nn2poly_example1$weights_list <- weights_list
nn2poly_example1$coefficients <- original_betas
nn2poly_example1$af_string_list <- af_string_list
nn2poly_example1$af_derivatives_list <- af_derivatives_list
nn2poly_example1$q_taylor_vector <- q_taylor_vector
nn2poly_example1$h_neurons_vector <- h_neurons_vector
nn2poly_example1$train_x <- train_x
nn2poly_example1$train_y <- train_y
nn2poly_example1$test_y <- test_y
nn2poly_example1$test_x <- test_x

#save(nn2poly_example1, file ="data/nn2poly_example1.rda")

```


## Example 2: 

2 variables, original order 2, 2 hidden layers, taylor order 2, softplus, 2 hidden neurons per layer


```r

# Main parameters
p <- 2
q_original <- 2
my_fun <-"softplus"
my_q_taylor <- 2
my_h <- 2

af_string_list <- list(my_fun, my_fun, "linear")
q_taylor_vector <- c(my_q_taylor, my_q_taylor, 1) # If the output is linear, the last value should be 1 so it doesn't affect the product
h_neurons_vector <- c(my_h, my_h,  1) # If the output is linear, the last value should be 1
my_max_norm <- list("l1_norm", 1)
af_derivatives_list <- nn2poly::obtain_derivatives_list(af_string_list,q_taylor_vector)

# Fixed parameters for the data generation
n_sample <- 400
mean_range <- c(-100, 100)
beta_range <- c(-5, 5)
error_var <- 0.2

# Data generation:
data_generated <- generate_normal_data(n_sample, p, q_original, mean_range, beta_range, error_var)
data <- data_generated$data
original_betas <- data_generated$original_betas

# Scale the data in the desired interval and separate train and test
scale_method <- "-1,1"
data_scaled <- scale_data(data, scale_method)
aux <- divide_train_test(data_scaled, train_proportion = 0.75)
train <- aux$train
test <- aux$test

# NN (keras) hyperparameters
my_loss <- "mse"
my_metrics <- "mse"
my_optimizer <- optimizer_adamax()
my_epochs <- 300
my_validation_split <- 0.3
my_verbose <- 0

# NN training ---------------------------------
  
# Divide again in x and y to train the NN with keras
train_x <- as.matrix(subset(train, select = -c(p+1)))
train_y <- as.matrix(subset(train, select = c(p+1)))

test_x <- as.matrix(subset(test, select = -c(p+1)))
test_y <- as.matrix(subset(test, select = c(p+1)))
  
# Build the nn
nn <- build_keras_model(
  p,
  af_string_list,
  h_neurons_vector,
  my_max_norm
)
  
# Compile the model
compile(nn,
        loss = my_loss,
        optimizer = my_optimizer,
        metrics = my_metrics
)
  
# Fit the model
history <- fit(nn,
               train_x,
               train_y,
               verbose = my_verbose,
               epochs = my_epochs,
               validation_split = my_validation_split,
               batch_size = 500
)
  
# Visualize the training process
plot(history)


# Extract the weights:
  keras_weights <- keras::get_weights(nn)
  
  # En este caso solo hace falta cambiar lo de la ultima capa
  # ARREGLAR ESTO PARA QUE SEA CONSISTENTE
  
  n <- length(keras_weights)
  
  if(my_max_norm[[1]]=="no_constraint"){

    n2 <- n/2

    nn_weights <- vector(mode = "list", length = n2)
    for (i in 1:n2){
      nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]])
   }
    
  } else {
    nn_weights <- keras_weights[1:(n - 2)]
    
    nn_weights[[n - 1]] <- rbind(keras_weights[[n]], keras_weights[[n - 1]])
  }
  
weights_list <- nn_weights





# Store all the data needed for the examples --------------------------
nn2poly_example2 <- vector(mode="list", length= 0)


nn2poly_example2$weights_list <- weights_list
nn2poly_example2$coefficients <- original_betas
nn2poly_example2$af_string_list <- af_string_list
nn2poly_example2$af_derivatives_list <- af_derivatives_list
nn2poly_example2$q_taylor_vector <- q_taylor_vector
nn2poly_example2$h_neurons_vector <- h_neurons_vector
nn2poly_example2$train_x <- train_x
nn2poly_example2$train_y <- train_y
nn2poly_example2$test_y <- test_y
nn2poly_example2$test_x <- test_x

#save(nn2poly_example2, file ="data/nn2poly_example2.rda")

```



## Example 3: 

3 variables, original order 3, 3 hidden layers, taylor order 3, tanh, 50 neurons per layer


```r

# Main parameters
p <- 3
q_original <- 3
my_fun <-"tanh"
my_q_taylor <- 3
my_h <- 50

af_string_list <- list(my_fun, my_fun,  my_fun, "linear")
q_taylor_vector <- c(my_q_taylor, my_q_taylor, my_q_taylor, 1) # If the output is linear, the last value should be 1 so it doesn't affect the product
h_neurons_vector <- c(my_h, my_h,  1) # If the output is linear, the last value should be 1
my_max_norm <- list("l1_norm", 1)
af_derivatives_list <- nn2poly::obtain_derivatives_list(af_string_list,q_taylor_vector)

# Fixed parameters for the data generation
n_sample <- 400
mean_range <- c(-100, 100)
beta_range <- c(-5, 5)
error_var <- 0.2

# Data generation:
data_generated <- generate_normal_data(n_sample, p, q_original, mean_range, beta_range, error_var)
data <- data_generated$data
original_betas <- data_generated$original_betas

# Scale the data in the desired interval and separate train and test
scale_method <- "-1,1"
data_scaled <- scale_data(data, scale_method)
aux <- divide_train_test(data_scaled, train_proportion = 0.75)
train <- aux$train
test <- aux$test

# NN (keras) hyperparameters
my_loss <- "mse"
my_metrics <- "mse"
my_optimizer <- optimizer_adamax()
my_epochs <- 300
my_validation_split <- 0.3
my_verbose <- 0

# NN training ---------------------------------
  
# Divide again in x and y to train the NN with keras
train_x <- as.matrix(subset(train, select = -c(p+1)))
train_y <- as.matrix(subset(train, select = c(p+1)))

test_x <- as.matrix(subset(test, select = -c(p+1)))
test_y <- as.matrix(subset(test, select = c(p+1)))
  
# Build the nn
nn <- build_keras_model(
  p,
  af_string_list,
  h_neurons_vector,
  my_max_norm
)
  
# Compile the model
compile(nn,
        loss = my_loss,
        optimizer = my_optimizer,
        metrics = my_metrics
)
  
# Fit the model
history <- fit(nn,
               train_x,
               train_y,
               verbose = my_verbose,
               epochs = my_epochs,
               validation_split = my_validation_split,
               batch_size = 500
)
  
# Visualize the training process
plot(history)


# Extract the weights:
  keras_weights <- keras::get_weights(nn)
  
  # En este caso solo hace falta cambiar lo de la ultima capa
  # ARREGLAR ESTO PARA QUE SEA CONSISTENTE
  
  n <- length(keras_weights)
  
  if(my_max_norm[[1]]=="no_constraint"){

    n2 <- n/2

    nn_weights <- vector(mode = "list", length = n2)
    for (i in 1:n2){
      nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]])
   }
    
  } else {
    nn_weights <- keras_weights[1:(n - 2)]
    
    nn_weights[[n - 1]] <- rbind(keras_weights[[n]], keras_weights[[n - 1]])
  }
  
weights_list <- nn_weights





# Store all the data needed for the examples --------------------------
nn2poly_example3 <- vector(mode="list", length= 0)

nn2poly_example3$weights_list <- weights_list
nn2poly_example3$coefficients <- original_betas
nn2poly_example3$af_string_list <- af_string_list
nn2poly_example3$af_derivatives_list <- af_derivatives_list
nn2poly_example3$q_taylor_vector <- q_taylor_vector
nn2poly_example3$h_neurons_vector <- h_neurons_vector
nn2poly_example3$train_x <- train_x
nn2poly_example3$train_y <- train_y
nn2poly_example3$test_y <- test_y
nn2poly_example3$test_x <- test_x

#save(nn2poly_example3, file ="data/nn2poly_example3.rda")

```
