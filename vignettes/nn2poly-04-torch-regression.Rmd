---
title: "04 - Regression example using torch"
author: "Pablo Morala"
output:
  rmarkdown::html_vignette:
    toc: yes
    fig_width: 6
date: "2023-10-30"
vignette: >
  %\VignetteIndexEntry{02 - Regression example using tensorflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



# This vignette's goal

After showing how to use `nn2poly` in its default version in `vignette("nn2poly-01-introduction")`, here we will present how to use specific methods related to `torch` (and its high level interface for R `luz`) that allow for an easier and smoother use of `nn2poly` with that deep learning framework. Furthermore, we will show how to impose the needed weight constraints in `torch` during training to have accurate results and compare those results with an unconstrained neural network.

In this vignette we will focus on a simple regression example. A classification example is covered in `vignette("nn2poly-03-tensorflow-classification")`.


```r
library(nn2poly)
library(torch)
library(luz)

set.seed(42)
```

# Simple regression example

## Simulated data generation

We will simulate polynomial data from the following polynomial: $4x_1 - 3 x_2x_3$. Data needs to be scaled to the $[-1,1]$ interval.


```r
# Define the desired polynomial for the simulated data
polynomial <- list()
polynomial$labels <- list(c(1), c(2,3))
polynomial$values <- c(4,-3)
```


```r
# Define number of variables p and sample n
p <- 3
n_sample <- 500

# Predictor variables
X <- matrix(0,n_sample,p)
for (i in 1:p){
  X[,i] <- rnorm(n = n_sample,0,1)
}

# Response variable + small error term
Y <- as.vector(eval_poly(X,polynomial)) + stats::rnorm(n_sample, 0, 0.1)

# Store all as a data frame
data <- as.data.frame(cbind(X, Y))
head(data)
#>           V1           V2         V3          Y
#> 1  1.3709584  1.029140719  2.3250585 -1.7547416
#> 2 -0.5646982  0.914774868  0.5241222 -3.7107357
#> 3  0.3631284 -0.002456267  0.9707334  1.3609395
#> 4  0.6328626  0.136009552  0.3769734  2.4608270
#> 5  0.4042683 -0.720153545 -0.9959334 -0.6141076
#> 6 -0.1061245 -0.198124330 -0.5974829 -0.7455793
```


```r
# Data scaling
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))

# Divide in train (0.75) and test (0.25)
index <- sample(1:nrow(data), round(0.75 * nrow(data)))
train <- data[index, ]
test <- data[-index, ]

train_x <- as.matrix(train[,-(p+1)])
train_y <- as.matrix(train[,(p+1)])

test_x <- as.matrix(test[,-(p+1)])
test_y <- as.matrix(test[,(p+1)])

plot(data)
```

![](figure/nn2poly-04-reg-data-scaling-split-1.png)


## Original neural networks

We will build and train two different neural networks (NNs), one with unconstrained weights (`nn1`) and another one imposing a constraint on the weights (`nn2`).

Different constraints can be tested, but the suggested constraint based on our theoretical and empirical evaluation is to use the L1 norm equal to 1, constraining each vector of weights + bias arriving to a neuron to satisfy that their L1 norm is equal or less than 1.

### Build the NNs

When using `torch` first of all we must define a torch dataset, divide the data in train and validation and define a data loader:


```r
# torch dataset
nn2poly_dataset <- torch::dataset(
  name = "nn2poly_dataset",
  initialize = function(x, y) {
    self$x <- torch::torch_tensor(x)
    self$y <- torch::torch_tensor(y)
  },
  .getitem = function(i) {
    x <- self$x[i,]
    y <- self$y[i]
    list(x = x,
         y = y)
  },
  .length = function() {
    self$y$size()[[1]]
  }
)

data_full <- nn2poly_dataset(train_x, train_y)

# Divide in train and validation
all_indices   <- 1:length(data_full)
train_indices <- sample(all_indices, size = round(length(data_full)) * 0.8)
val_indices   <- setdiff(all_indices, train_indices)

data_train <- torch::dataset_subset(data_full, train_indices)
data_val   <- torch::dataset_subset(data_full, val_indices)


# Data loader
torch_data <- list(
  train = torch::dataloader(data_train, batch_size = 32, shuffle = TRUE),
  valid = torch::dataloader(data_val, batch_size = 32)
)
```

Then, the NNs will be built using a sequential model with `luz`and `torch`. As we want to compare the behavior of `nn2poly` with constrained and unconstrained weights, we will create 2 identical NNs and then train them with and without constraints.


```r

luz_nn1 <- function() {
  torch::torch_manual_seed(42)
  
  luz_model_sequential(
    torch::nn_linear(p,100),
    torch::nn_tanh(),
    torch::nn_linear(100,100),
    torch::nn_tanh(),
    torch::nn_linear(100,100),
    torch::nn_tanh(),
    torch::nn_linear(100,1)
  )
}

luz_nn2 <- function() {
  torch::torch_manual_seed(42)
  
  luz_model_sequential(
    torch::nn_linear(p,100),
    torch::nn_tanh(),
    torch::nn_linear(100,100),
    torch::nn_tanh(),
    torch::nn_linear(100,100),
    torch::nn_tanh(),
    torch::nn_linear(100,1)
  )
}
```


### NNs training

We will first train the first NN (`luz_nn1`) without constraints:


```r
fitted_1 <- luz_nn1() %>%
    luz::setup(
      loss = torch::nn_mse_loss(),
      optimizer = torch::optim_adam,
      metrics = list(
        luz::luz_metric_mse()
      )
    ) %>%
    luz::fit(torch_data$train, epochs = 50, valid_data = torch_data$valid)
```

In order to implement the desired constraints, we provide the `add_constraints()` function, that allows to set the desired constraints in the torch training setup as callbacks. 

Our constraints implementation is such that the bias on each neuron is included in the weights vector incident on that neuron, meaning that if the previous layer had $h$ neurons, then the considered weight vector including the bias at a given neuron would have dimension $h+1$, having the bias as it first element. Currently, L1 norm and L2 norm equal to 1 are implemented as options.

Note that L1 norm equal to 1 when scaling the input data to the $[-1,1]$ interval is the recommended option.



```r
# Do the constrained training with the l1 constraint
  fitted_2 <- luz_nn2() %>%
    luz::setup(
      loss = torch::nn_mse_loss(),
      optimizer = torch::optim_adam,
    ) %>%
    add_constraints("l1_norm") %>%
    fit(torch_data$train, epochs = 600, valid_data = torch_data$valid)
```




### Visualize both NN predictions

We can visualize the NN predictions vs the original Y values for both neural networks and observe how both of them provide accurate predictions (the values fall near the "perfect" diagonal red line).


```r
# Obtain the predicted values with the NN to compare them
prediction_NN1 <- as.array(predict(fitted_1, test_x))

# Diagonal plot implemented in the package to quickly visualize and compare predictions
plot_diagonal(x_axis =  prediction_NN1, y_axis =  test_y, xlab = "NN 1 prediction", ylab = "Original Y")
```

![](figure/nn2poly-04-reg-comparison-y-nn1-1.png)

```r
fitted_1 %>% plot()
```

![](figure/nn2poly-04-unnamed-chunk-5-1.png)



```r
# Obtain the predicted values with the NN to compare them
prediction_NN2 <- as.array(predict(fitted_2, test_x))

# Diagonal plot implemented in the package to quickly visualize and compare predictions
plot_diagonal(x_axis =  prediction_NN2, y_axis =  test_y, xlab = "NN 2 prediction", ylab = "Original Y")
```

![](figure/nn2poly-04-reg-comparison-y-nn2-1.png)



```r
fitted_2 %>% plot()
```

![](figure/nn2poly-04-unnamed-chunk-6-1.png)


## Using nn2poly to obtain  the polynomial

After the NNs have been trained, we can directly call `nn2poly` on the `luz`/`torch` sequential model. Therefore, we do not need to build an object with weights and activation functions as in the default case covered in `vignette("nn2poly-01-introduction")`, and can benefit from the generic methods implemented for `torch` models.

The only parameters that have to be added for nn2poly to work is the chosen limit for the polynomial order with `forced_max_Q=3`.

We will do this for both neural networks and compare the results:


```r

# Polynomial for nn1
final_poly1 <- nn2poly(object = fitted_1,
                      forced_max_Q = 3)

# Polynomial for nn2
final_poly2 <- nn2poly(object = fitted_2,
                      forced_max_Q = 3)
```



## Obtaining polynomial predictions


```r

# Obtain the predicted values for the test data with our two polynomials

prediction_poly1 <- eval_poly(x = test_x, poly = final_poly1)

prediction_poly2 <- eval_poly(x = test_x, poly = final_poly2)
```


## Visualizing the results

With the polynomial predictions, we can plot them using our diagonal plot to compare them with their respective NN predictions. Please note here that we compare the predictions of the polynomial with the NN predictions and not the original data, as `nn2poly`'s goal is to faithfully represent the NN behavior independently of how well the NN predicts.

We can observe clearly how the polynomial obtained for the constrained network (`nn2`) is predicting almost the same, while the unconstrained network has significant errors.



```r
plot_diagonal(x_axis =  prediction_NN1, y_axis =  prediction_poly1, xlab = "NN prediction", ylab = "Polynomial prediction") + ggplot2::ggtitle("Polynomial for NN1")
```

![](figure/nn2poly-04-reg-comparison-polynomial-nn-1.png)

```r

plot_diagonal(x_axis =  prediction_NN2, y_axis =  prediction_poly2, xlab = "NN prediction", ylab = "Polynomial prediction") + ggplot2::ggtitle("Polynomial for NN2")
```

![](figure/nn2poly-04-reg-comparison-polynomial-nn-2.png)

We can also plot the $n$ most important coefficients in absolute value to compare which variables or interactions are more relevant in the polynomial. Note that, as data should be scaled to the $[-1,1]$ interval, interactions of order 2 or higher would usually need a higher absolute value than the lower order coefficients to be more relevant.

Recall that the original polynomial was $4x_1 - 3x_2x_3$. If we observe the polynomial from `nn2`, precisely interaction `2,3` has a high negative coefficient while variable `1` has a positive one and the rest of variables and the intercept (`0`) are quite close to zero. However, in the polynomial from `nn1`, the obtained coefficients are not correct as Taylor expansion is failing because of the high weights.


```r
plot_n_important_coeffs(final_poly1, n_important_coeffs = 8)
```

![](figure/nn2poly-04-reg-n-important-1.png)

```r
plot_n_important_coeffs(final_poly2, n_important_coeffs = 8)
```

![](figure/nn2poly-04-reg-n-important-2.png)

Finally, the problem with Taylor expansion can be checked with the following plot, where each layer is represented with their activation function, its Taylor expansion, the error and also the density of the activation potentials that the activation functions receives at that layer.

It can be clearly seen with the activation potentials density, in green, that it expands over a wide range in the unconstrained NN while the it is kept closer to zero in the constrained one, thus having a more accurate Taylor expansion  around zero.


```r

q_taylor_vector <- c(8, 8, 8, 1) # Esto debería ser tomado de algun sitio o dejado como default igual que en el uso de nn2poly

plot_taylor_and_activation_potentials(object = fitted_1,
                                      data = train,
                                      q_taylor_vector = q_taylor_vector,
                                      forced_max_Q = 3,
                                      constraints = FALSE)
#> [[1]]
```

![](figure/nn2poly-04-reg-potentials-1.png)

```
#> 
#> [[2]]
```

![](figure/nn2poly-04-reg-potentials-2.png)

```
#> 
#> [[3]]
```

![](figure/nn2poly-04-reg-potentials-3.png)

```
#> 
#> [[4]]
```

![](figure/nn2poly-04-reg-potentials-4.png)

```r

plot_taylor_and_activation_potentials(object = fitted_2,
                                      data = train,
                                      q_taylor_vector = q_taylor_vector,
                                      forced_max_Q = 3,
                                      constraints = TRUE)
#> [[1]]
```

![](figure/nn2poly-04-reg-potentials-5.png)

```
#> 
#> [[2]]
```

![](figure/nn2poly-04-reg-potentials-6.png)

```
#> 
#> [[3]]
```

![](figure/nn2poly-04-reg-potentials-7.png)

```
#> 
#> [[4]]
```

![](figure/nn2poly-04-reg-potentials-8.png)
