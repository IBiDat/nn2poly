[{"path":"https://ibidat.github.io/nn2poly/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021-2023 Pablo Morala, Iñaki Úcar Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"overall-package-goal","dir":"Articles","previous_headings":"","what":"Overall package goal","title":"01 - Introduction to nn2poly","text":"main objective nn2poly obtain representation feed forward artificial neural network (like multilayered perceptron) terms polynomial representation. coefficients polynomials obtained applying first Taylor expansion activation function neural network. expansions given neural network weights joint using combinatorial properties, obtaining final value polynomial coefficients. information theoretical insights underlying mathematical process used build relationship can found following references: * Initial development idea single hidden layer neural network article free access arXiv preprint version. * Extension deeper layers proper formulation NN2Poly method arXiv preprint. Important remark 1: approximations made NN2poly method rely Taylor expansions therefore require constraints imposed training original neural network. implementation constraints depend deep learning framework used train neural networks. Currently, package supports constraints implementation keras/tensorflow, covered vignette(\"nn2poly-02-tensorflow-regression\") vignette(\"nn2poly-03-tensorflow-classification\"). Implementation pytorch networks development. However, nn2poly can work default kind neural network manually feeding neural network weights activation functions algorithm. Therefore, nn2poly limited special deep learning framework.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"this-vignettes-goal","dir":"Articles","previous_headings":"","what":"This vignette’s goal","title":"01 - Introduction to nn2poly","text":"present basic behavior nn2poly used default version, without specifying deep learning framework explained previous remark. matter, showcase example get weights trained neural network manually create object needed information use nn2poly. result polynomial tries approximate neural network behavior. case neural network training constraint imposed. , explained previously, final approximation polynomial may accurate enough. example focused default version, need build NN framework, use keras tensorflow matter. case, needed parameters extracted used default version nn2poly, can extrapolated framework.","code":"library(nn2poly) library(keras)  # This sets all needed seeds tensorflow::set_random_seed(42)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"simple-regression-example","dir":"Articles","previous_headings":"","what":"Simple regression example","title":"01 - Introduction to nn2poly","text":"example solve regression problem using simulated data polynomial, allows control final polynomial coefficients obtained nn2poly similar polynomial originates data.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"simulated-data-generation","dir":"Articles","previous_headings":"Simple regression example","what":"Simulated data generation","title":"01 - Introduction to nn2poly","text":"simulate polynomial data follows. First define polynomial using format needed nn2poly, specifically use function eval_poly, consists list containing: * Labels: list integer vectors denoting combinations variables appear term polynomial. Variables numbered 1 p p dimension problem. example, c(1,1,3) represent term \\(x_1^2x_3\\) * Values: Vector containing numerical values coefficients denoted labels. multiple polynomials terms different coefficients want represented, matrix can employed, row polynomial. create polynomial \\(4x_1 - 3 x_2x_3\\): said polynomial, can now generate desired data train NN example. employ normal distribution generate variables \\(x_1, x_2, x_3\\) also error term \\(\\epsilon\\). Therefore, response variable \\(y\\) generated : \\(y = 4x_1 - 3 x_2x_3 + \\epsilon\\) scale data everything \\([-1,1]\\) interval divide train test datasets.","code":"polynomial <- list() polynomial$labels <- list(c(1), c(2,3)) polynomial$values <- c(4,-3) # Define number of variables p and sample n p <- 3 n_sample <- 500  # Predictor variables X <- matrix(0,n_sample,p) for (i in 1:p){   X[,i] <- rnorm(n = n_sample,0,1) }  # Response variable + small error term Y <- as.vector(eval_poly(X,polynomial)) + stats::rnorm(n_sample, 0, 0.1)  # Store all as a data frame data <- as.data.frame(cbind(X, Y)) head(data) #>           V1           V2         V3          Y #> 1  1.3709584  1.029140719  2.3250585 -1.7547416 #> 2 -0.5646982  0.914774868  0.5241222 -3.7107357 #> 3  0.3631284 -0.002456267  0.9707334  1.3609395 #> 4  0.6328626  0.136009552  0.3769734  2.4608270 #> 5  0.4042683 -0.720153545 -0.9959334 -0.6141076 #> 6 -0.1061245 -0.198124330 -0.5974829 -0.7455793 # Data scaling maxs <- apply(data, 2, max) mins <- apply(data, 2, min) data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"original-neural-network","dir":"Articles","previous_headings":"Simple regression example","what":"Original neural network","title":"01 - Introduction to nn2poly","text":"simulated data ready, can train neural network. method expected applied given trained densely connected feed forward neural network (NN now ), also referred multilayer perceptron (MLP). Therefore, explained , method used train NN can used. use kerasto train , manually build needed object weights fed nn2poly algorithm trained framework. information specific keras methods implemented package, please refer vignette(\"nn2poly-02-tensorflow-regression\") vignette(\"nn2poly-02-tensorflow-classification\"). Note: , note , order avoid asymptotic behavior method, important impose kind constraints training neural network weights. Details depend chosen deep learning framework covered next vignettes. First, build model. Compile model: train : can visualize training process:  can also visualize NN predictions vs original Y values.  Note: Recall NN performance addressed nn2poly, meaning performance either good bad nn2poly still represent NN behavior.","code":"nn <- keras_model_sequential()  nn %>% layer_dense(units = 10,                   activation = \"tanh\",                   input_shape = p)  nn %>% layer_dense(units = 10,                   activation = \"tanh\")  nn %>% layer_dense(units = 1,                   activation = \"linear\")  nn #> Model: \"sequential_9\" #> ________________________________________________________________________________________________________________________ #>  Layer (type)                                         Output Shape                                    Param #            #> ======================================================================================================================== #>  dense_12 (Dense)                                     (None, 10)                                      40                 #>  dense_13 (Dense)                                     (None, 10)                                      110                #>  dense_14 (Dense)                                     (None, 1)                                       11                 #> ======================================================================================================================== #> Total params: 161 #> Trainable params: 161 #> Non-trainable params: 0 #> ________________________________________________________________________________________________________________________ compile(nn,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\") history <- fit(nn,                train_x,                train_y,                verbose = 0,                epochs = 300,                validation_split = 0.3 ) plot(history) # Obtain the predicted values with the NN to compare them prediction_NN <- predict(nn, test_x)  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN, y_axis =  test_y, xlab = \"NN prediction\", ylab = \"Original Y\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"using-nn2poly-to-obtain-the-polynomial","dir":"Articles","previous_headings":"Simple regression example","what":"Using nn2poly to obtain the polynomial","title":"01 - Introduction to nn2poly","text":"NN trained, using chosen method user, parameters extracted reshaped, needed, match expected input function nn2poly_algorithm(). input object formed list matrices weight matrix layer. weights matrices dimension ((1+input) * output) first row corresponds bias vector, rest rows correspond ordered vector weights associated input. list, name element activation function names layer. Currently supported activation functions \"tanh\", \"sigmoid\", \"softplus\", \"linear\". Particularly, keras framework default separates kernel weights matrices dimension (input * output) bias vectors (1 * output), need add bias first row matrix ((1+input) * output). Additionally, parameters affecting properties algorithm: taylor_orders: vector integers containing order Taylor expansion performed layer. output layer linear activation function, last value 1. max_order: (optional value) integer value denoting maximum order terms computed polynomial. Usually 2 3 enough practice. Note higher orders suppose explosion possible combinations. user provide value, polynomial order grows multiplicatively Taylor order hidden layer, therefore better start low values. input desired shape, nn2poly method can applied: can glimpse coefficients polynomial stored. Note structure explained polynomial generated data, list labels values. case, obtained polynomial order 3. Note output nn2poly class.","code":"keras_weights <- keras::get_weights(nn)  # Due to keras giving weights separated from the bias, we have twice the # elements that we want: n <- length(keras_weights)/2 nn_weights <- vector(mode = \"list\", length = n) for (i in 1:n){   nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]]) }  # The activation functions stored as strings: af_string_names <- c(\"tanh\",\"tanh\", \"linear\")  weights_object <- nn_weights names(weights_object) <- af_string_names  weights_object #> $tanh #>            [,1]       [,2]      [,3]        [,4]        [,5]       [,6]       [,7]        [,8]        [,9]       [,10] #> [1,]  0.2637966 -0.1632335 0.4649245 -0.20589803  0.04730094 -0.3763387 -0.2124885 -0.08747434 -0.07051360 -0.37202930 #> [2,]  0.2385524  0.1342811 0.2513933  0.48018309 -0.47788176  0.1631715  0.6607828 -0.31218216 -0.01426336 -0.01626668 #> [3,] -0.7742428 -0.1097567 0.8174284  0.08418242  0.06163564  0.7477616 -0.8533844 -0.48116657  0.04514860 -0.70131820 #> [4,] -0.6955228  0.5266028 0.5572206 -0.46307948  0.48515409 -0.7743985  0.1162070  0.35274017  0.04112899 -0.56946415 #>  #> $tanh #>              [,1]        [,2]        [,3]        [,4]        [,5]       [,6]        [,7]        [,8]        [,9] #>  [1,] -0.06261473  0.01255261  0.42954829  0.06141516  0.08550595  0.2265693  0.08071607  0.16983804  0.08445328 #>  [2,]  0.61023968 -0.15460493  0.32420215  0.16633785  0.25600886 -0.2739864  0.06122208 -0.04379337 -0.57018125 #>  [3,] -0.25070983 -0.05766810 -0.50344008  0.13009298 -0.03173310 -0.4047118  0.28472036  0.23372132 -0.47447795 #>  [4,]  0.40872586  0.46675542  0.11527456  0.43078154  0.15864335  0.6346628  0.01778316 -0.29318872  0.07777876 #>  [5,] -0.24814591  0.01059229  0.40011716  0.27607894 -0.46360433 -0.3692199 -0.53746992 -0.42425549 -0.48620722 #>  [6,] -0.26658243  0.51526904 -0.09131107  0.37373766  0.26082405  0.4299564 -0.41601449 -0.25669649 -0.28299403 #>  [7,]  0.51186675  0.12881947  0.67951459 -0.17605777  0.15393135  0.4613117 -0.22281407  0.40243641 -0.12623918 #>  [8,]  0.40394908 -0.42659599 -0.98550946 -0.15839998  0.02221864  0.3011585 -0.04859665 -0.38288346  0.25072047 #>  [9,]  0.50324774  0.11122702 -0.84318024 -0.02784178  0.31848624  0.2643846  0.48137835 -0.40863025  0.01616014 #> [10,]  0.27406499 -0.11159007  0.02285146  0.35646623  0.25075704 -0.1359648 -0.32286894  0.09449039 -0.06011314 #> [11,] -0.52268207  0.45537508 -0.56030786  0.49129054 -0.03669148 -0.7891714  0.14410253 -0.21469988  0.24569559 #>               [,10] #>  [1,]  0.0732324421 #>  [2,] -0.3144004941 #>  [3,]  0.0003889628 #>  [4,] -0.0302867368 #>  [5,] -0.1922566146 #>  [6,] -0.2540818155 #>  [7,] -0.4014825821 #>  [8,] -0.3935797811 #>  [9,] -0.5411312580 #> [10,] -0.4011713266 #> [11,]  0.3822859824 #>  #> $linear #>              [,1] #>  [1,] -0.07251097 #>  [2,]  0.95829737 #>  [3,] -0.07316440 #>  [4,] -0.35980642 #>  [5,] -0.43944517 #>  [6,] -0.07012334 #>  [7,]  0.60509491 #>  [8,] -0.66404337 #>  [9,] -0.04713812 #> [10,] -0.79189116 #> [11,] -0.91330308 taylor_orders <- c(8, 8,  1) final_poly <- nn2poly(object = weights_object,                       taylor_orders = taylor_orders,                       max_order = 3) final_poly #> $labels #> $labels[[1]] #> [1] 0 #>  #> $labels[[2]] #> [1] 1 #>  #> $labels[[3]] #> [1] 2 #>  #> $labels[[4]] #> [1] 3 #>  #> $labels[[5]] #> [1] 1 1 #>  #> $labels[[6]] #> [1] 1 2 #>  #> $labels[[7]] #> [1] 1 3 #>  #> $labels[[8]] #> [1] 2 2 #>  #> $labels[[9]] #> [1] 2 3 #>  #> $labels[[10]] #> [1] 3 3 #>  #> $labels[[11]] #> [1] 1 1 1 #>  #> $labels[[12]] #> [1] 1 1 2 #>  #> $labels[[13]] #> [1] 1 1 3 #>  #> $labels[[14]] #> [1] 1 2 2 #>  #> $labels[[15]] #> [1] 1 2 3 #>  #> $labels[[16]] #> [1] 1 3 3 #>  #> $labels[[17]] #> [1] 2 2 2 #>  #> $labels[[18]] #> [1] 2 2 3 #>  #> $labels[[19]] #> [1] 2 3 3 #>  #> $labels[[20]] #> [1] 3 3 3 #>  #>  #> $values #>            [,1]      [,2]       [,3]        [,4]       [,5]       [,6]        [,7]      [,8]      [,9]      [,10] #> [1,] -0.1617715 0.7475098 0.09908831 -0.08050417 0.07277049 0.03545956 0.009334022 0.3035036 -2.559723 0.02124509 #>            [,11]     [,12]      [,13]     [,14]       [,15]      [,16]     [,17]     [,18]      [,19]       [,20] #> [1,] -0.08924709 0.2939184 0.01080101 0.6197476 -0.04956815 0.07802355 0.5294107 -1.012448 -0.7079898 -0.05140071 #>  #> attr(,\"class\") #> [1] \"nn2poly\""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"Simple regression example","what":"Obtaining polynomial predictions","title":"01 - Introduction to nn2poly","text":"obtaining polynomial coefficients, can use predict response variable \\(Y\\). done employing function predciton nn2poly object providing new data predict.","code":"# Obtain the predicted values for the test data with our polynomial prediction_poly <- predict(object = final_poly,                            newdata = test_x)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Simple regression example","what":"Visualizing the results","title":"01 - Introduction to nn2poly","text":"advisable always check predictions obtained new polynomial differ much original neural network predictions (case differ, can also try find checking Taylor expansions). help , couple functions included allow us plot results. simple plot comparing polynomial NN predictions can obtained plot_diagonal(), red diagonal line represents perfect relationship NN polynomial predictions obtained. example, theoretical weights constraints imposed, can observe approximation perfect.  can also plot \\(n\\) important coefficients absolute value compare variables interactions relevant polynomial. Note , data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant. case can see coefficients differ original polynomial \\(4x_1 - 3 x_2x_3\\), constraints neural network weights training.  Another convenient plot show algorithm affected layer can obtained plot_taylor_and_activation_potentials(), activation potentials neuron computed presented Taylor expansion approximation activation function layer. case, used constraints NN training, activation potentials strictly centered around zero.","code":"plot_diagonal(x_axis =  prediction_NN, y_axis =  prediction_poly, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") plot_n_important_coeffs(final_poly, 8) plot_taylor_and_activation_potentials(object = nn,                                       data = train,                                     taylor_orders = taylor_orders,                                     max_order = 3,                                     constraints = FALSE) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"this-vignettes-goal","dir":"Articles","previous_headings":"","what":"This vignette’s goal","title":"02 - Regression example using tensorflow","text":"showing use nn2poly default version vignette(\"nn2poly-01-introduction\"), present use specific methods related keras tensorflow allow easier smoother use nn2poly deep learning framework. Furthermore, sow impose needed weight constraints tensorflow training accurate results compare results unconstrained neural network. vignette focus simple regression example classification one covered vignette(\"nn2poly-03-tensorflow-classification\").","code":"library(nn2poly) library(keras)  # This sets all needed seeds tensorflow::set_random_seed(1)"},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"simulated-data-generation","dir":"Articles","previous_headings":"Simple regression example","what":"Simulated data generation","title":"02 - Regression example using tensorflow","text":"simulate polynomial data following polynomial: \\(4x_1 - 3 x_2x_3\\). Data needs scaled \\([-1,1]\\) interval.","code":"# Define the desired polynomial for the simulated data polynomial <- list() polynomial$labels <- list(c(1), c(2,3)) polynomial$values <- c(4,-3) # Define number of variables p and sample n p <- 3 n_sample <- 500  # Predictor variables X <- matrix(0,n_sample,p) for (i in 1:p){   X[,i] <- rnorm(n = n_sample,0,1) }  # Response variable + small error term Y <- as.vector(eval_poly(X,polynomial)) + stats::rnorm(n_sample, 0, 0.1)  # Store all as a data frame data <- as.data.frame(cbind(X, Y)) head(data) #>           V1          V2          V3         Y #> 1 -0.6264538  0.07730312  1.13496509 -2.684020 #> 2  0.1836433 -0.29686864  1.11193185  1.632335 #> 3 -0.8356286 -1.18324224 -0.87077763 -6.344179 #> 4  1.5952808  0.01129269  0.21073159  6.279883 #> 5  0.3295078  0.99160104  0.06939565  1.165488 #> 6 -0.8204684  1.59396745 -1.66264885  4.650553 # Data scaling maxs <- apply(data, 2, max) mins <- apply(data, 2, min) data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])  plot(data)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"original-neural-networks","dir":"Articles","previous_headings":"Simple regression example","what":"Original neural networks","title":"02 - Regression example using tensorflow","text":"build train two different neural networks (NNs), one unconstrained weights (nn1) another one imposing constraint weights (nn2). Different constraints can tested, suggested constraint based theoretical empirical evaluation use L1 norm equal 1, constraining vector weights + bias arriving neuron satisfy L1 norm equal less 1.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"build-nn-1-unconstrained","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Build NN 1, unconstrained","title":"02 - Regression example using tensorflow","text":"NN built using standard tensorflow keras practices, case sequential keras model without constraint weights.","code":"nn1 <- keras_model_sequential()  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\",                   input_shape = p)  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\")  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\")  nn1 %>% layer_dense(units = 1,                   activation = \"linear\")  nn1 #> Model: \"sequential_10\" #> ________________________________________________________________________________________________________________________ #>  Layer (type)                                         Output Shape                                    Param #            #> ======================================================================================================================== #>  dense_15 (Dense)                                     (None, 100)                                     400                #>  dense_16 (Dense)                                     (None, 100)                                     10100              #>  dense_17 (Dense)                                     (None, 100)                                     10100              #>  dense_18 (Dense)                                     (None, 1)                                       101                #> ======================================================================================================================== #> Total params: 20,701 #> Trainable params: 20,701 #> Non-trainable params: 0 #> ________________________________________________________________________________________________________________________"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"build-nn-2-constrained","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Build NN 2, constrained","title":"02 - Regression example using tensorflow","text":"order implement desired constraints, provide add_constraints() function, takes structure given NN (feed forward dense NN) modifies layers include constraints. needed default constraints implemented keras support impose constraint time weights bias combined custom layer. implementation bias neuron included weights vector incident neuron, meaning previous layer \\(h\\) neurons, considered weight vector including bias given neuron dimension \\(h+1\\), bias first element. Currently, L1 norm L2 norm equal 1 implemented options. Note L1 norm equal 1 scaling input data \\([-1,1]\\) interval recommended option. Note parameters structure , layer type modified.","code":"nn2 <- add_constraints(nn1, constraint_type = \"l1_norm\") nn2 #> Model: \"sequential_11\" #> ________________________________________________________________________________________________________________________ #>  Layer (type)                                         Output Shape                                    Param #            #> ======================================================================================================================== #>  layer__combined_l1_5 (Layer_Combined_L1)             (None, 100)                                     400                #>  layer__combined_l1_6 (Layer_Combined_L1)             (None, 100)                                     10100              #>  layer__combined_l1_7 (Layer_Combined_L1)             (None, 100)                                     10100              #>  dense_19 (Dense)                                     (None, 1)                                       101                #> ======================================================================================================================== #> Total params: 20,701 #> Trainable params: 20,701 #> Non-trainable params: 0 #> ________________________________________________________________________________________________________________________"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"compile-and-train-both-nns","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Compile and train both NNs","title":"02 - Regression example using tensorflow","text":"building NNs, compile train . Note , constraining weights trade-learning speed NN, nn2 needs higher number epochs properly learn data. Compile train nn1 model, visualize training history:  Compile train nn2 model, visualize training history:","code":"compile(nn1,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\")  history1 <- fit(nn1,                train_x,                train_y,                verbose = 0,                epochs = 300,                batch_size = 50,                validation_split = 0.2 )  plot(history1) compile(nn2,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\")  history2 <- fit(nn2,                train_x,                train_y,                verbose = 0,                epochs = 2000,                batch_size = 50,                validation_split = 0.2 )  plot(history2)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"visualize-both-nn-predictions","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Visualize both NN predictions","title":"02 - Regression example using tensorflow","text":"can visualize NN predictions vs original Y values neural networks observe provide accurate predictions (values fall near “perfect” diagonal red line).","code":"# Obtain the predicted values with the NN to compare them prediction_NN1 <- predict(nn1, test_x)  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN1, y_axis =  test_y, xlab = \"NN 1 prediction\", ylab = \"Original Y\") # Obtain the predicted values with the NN to compare them prediction_NN2 <- predict(nn2, test_x)  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN2, y_axis =  test_y, xlab = \"NN 2 prediction\", ylab = \"Original Y\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"using-nn2poly-to-obtain-the-polynomial","dir":"Articles","previous_headings":"Simple regression example","what":"Using nn2poly to obtain the polynomial","title":"02 - Regression example using tensorflow","text":"NNs trained, can directly call nn2poly keras model. Therefore, need build object weights activation functions default case covered vignette(\"nn2poly-01-introduction\"), can benefit generic methods implemented keras models. parameters added nn2poly work Taylor order expansion layer (taylor_orders), choose 8 default non linear layers 1 last linear layer Taylor used . (final polynomial order limited max_order=3) neural networks compare results:","code":"taylor_orders <- c(8, 8, 8, 1)  # Polynomial for nn1 final_poly1 <- nn2poly(object = nn1,                       taylor_orders = taylor_orders,                       max_order = 3)  # Polynomial for nn2 final_poly2 <- nn2poly(object = nn2,                       taylor_orders = taylor_orders,                       max_order = 3)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"Simple regression example","what":"Obtaining polynomial predictions","title":"02 - Regression example using tensorflow","text":"","code":"# Obtain the predicted values for the test data with our two polynomials  prediction_poly1 <- eval_poly(x = test_x, poly = final_poly1)  prediction_poly2 <- eval_poly(x = test_x, poly = final_poly2)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Simple regression example","what":"Visualizing the results","title":"02 - Regression example using tensorflow","text":"polynomial predictions, can plot using diagonal plot compare respective NN predictions. Please note compare predictions polynomial NN predictions original data, nn2poly’s goal faithfully represent NN behavior independently well NN predicts. can observe clearly polynomial obtained constrained network (nn2) predicting almost , unconstrained network significant errors.   can also plot \\(n\\) important coefficients absolute value compare variables interactions relevant polynomial. Note , data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant. Recall original polynomial \\(4x_1 - 3x_2x_3\\). observe polynomial nn2, precisely interaction 2,3 high negative coefficient variable 1 positive one rest variables intercept (0) quite close zero. However, polynomial nn1, obtained coefficients correct Taylor expansion failing high weights.   Finally, problem Taylor expansion can checked following plot, layer represented activation function, Taylor expansion, error also density activation potentials activation functions receives layer. can clearly seen activation potentials density, green, expands wide range unconstrained NN kept closer zero constrained one, thus accurate Taylor expansion around zero.","code":"plot_diagonal(x_axis =  prediction_NN1, y_axis =  prediction_poly1, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for NN1\") plot_diagonal(x_axis =  prediction_NN2, y_axis =  prediction_poly2, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for NN2\") plot_n_important_coeffs(final_poly1, n_important_coeffs = 8) plot_n_important_coeffs(final_poly2, n_important_coeffs = 8) plot_taylor_and_activation_potentials(object = nn1,                                       data = train,                                       taylor_orders = taylor_orders,                                       max_order = 3,                                       constraints = FALSE) #> [[1]] #>  #> [[2]] #>  #> [[3]] #>  #> [[4]] plot_taylor_and_activation_potentials(object = nn2,                                       data = train,                                       taylor_orders = taylor_orders,                                       max_order = 3,                                       constraints = TRUE) #> [[1]] #>  #> [[2]] #>  #> [[3]] #>  #> [[4]]"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"this-vignettes-goal","dir":"Articles","previous_headings":"","what":"This vignette’s goal","title":"03 - Classification example using tensorflow","text":"showing use nn2poly default version vignette(\"nn2poly-01-introduction\"), present use specific methods related keras tensorflow allow easier smoother use nn2poly deep learning framework. Furthermore, sow impose needed weight constraints tensorflow training accurate results compare results unconstrained neural network. vignette focus simple classification example using iris dataset. regression one covered vignette(\"nn2poly-02-tensorflow-regression\").","code":"library(nn2poly) library(keras)  # This sets all needed seeds tensorflow::set_random_seed(1)"},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"data-preparation","dir":"Articles","previous_headings":"Simple classification example","what":"Data preparation","title":"03 - Classification example using tensorflow","text":"First load iris dataset scale data \\([-1,1]\\) interval:","code":"# Load the data data(iris)  # Change response to numeric. In this case, Species was already numeric, # but this step is needed if it is a factor variable. iris$Species <- as.numeric(iris$Species)  # Define dimension p (number of predictor variables) p <- dim(iris)[2] - 1  # Define objective classes n_class <- max(iris[,(p+1)])  # Move objective classes from (1:3) to (0:2), needed for tensorflow iris[,(p+1)] <- iris[,(p+1)] - 1 # Scale the data in the [-1,1] interval and separate train and test # Only the predictor variables are scaled, not the response as those will be # the different classes. iris_x <- iris[,-(p+1)] maxs <- apply(iris_x, 2, max) mins <- apply(iris_x, 2, min) data_x_scaled <- as.data.frame(scale(iris_x, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2)) data <- cbind(data_x_scaled, iris[,(p+1)])  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"original-neural-networks","dir":"Articles","previous_headings":"Simple classification example","what":"Original neural networks","title":"03 - Classification example using tensorflow","text":"build train two different neural networks (NNs), one unconstrained weights (nn1) another one imposing constraint weights (nn2). Different constraints can tested, suggested constraint based theoretical empirical evaluation use L1 norm equal 1, constraining vector weights + bias arriving neuron satisfy L1 norm equal less 1.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"build-nn-1-unconstrained","dir":"Articles","previous_headings":"Simple classification example > Original neural networks","what":"Build NN 1, unconstrained","title":"03 - Classification example using tensorflow","text":"First, build model. Note case NN linear output number neurons number classes predict (3 species, n_class). , linear output transformed probability find probable class step done training. Therefore, nn2poly used obtain polynomial approximates nn linear outputs results also transformed probabilities predict highest probability class. NN built using standard tensorflow keras practices, case sequential keras model without constraint weights.","code":"nn1 <- keras_model_sequential()  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\",                   input_shape = p)  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\")  nn1 %>% layer_dense(units = n_class)  nn1 #> Model: \"sequential_12\" #> ________________________________________________________________________________________________________________________ #>  Layer (type)                                         Output Shape                                    Param #            #> ======================================================================================================================== #>  dense_20 (Dense)                                     (None, 100)                                     500                #>  dense_21 (Dense)                                     (None, 100)                                     10100              #>  dense_22 (Dense)                                     (None, 3)                                       303                #> ======================================================================================================================== #> Total params: 10,903 #> Trainable params: 10,903 #> Non-trainable params: 0 #> ________________________________________________________________________________________________________________________"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"build-nn-2-constrained","dir":"Articles","previous_headings":"Simple classification example > Original neural networks","what":"Build NN 2, constrained","title":"03 - Classification example using tensorflow","text":"order implement desired constraints, provide add_constraints() function, takes structure given NN (feed forward dense NN) modifies layers include constraints. needed default constraints implemented keras support impose constraint time weights bias combined custom layer. implementation bias neuron included weights vector incident neuron, meaning previous layer \\(h\\) neurons, considered weight vector including bias given neuron dimension \\(h+1\\), bias first element. Currently, L1 norm L2 norm equal 1 implemented options. Note L1 norm equal 1 scaling input data \\([-1,1]\\) interval recommended option. Note parameters structure , layer type modified.","code":"nn2 <- add_constraints(nn1, constraint_type = \"l1_norm\") nn2 #> Model: \"sequential_13\" #> ________________________________________________________________________________________________________________________ #>  Layer (type)                                         Output Shape                                    Param #            #> ======================================================================================================================== #>  layer__combined_l1_8 (Layer_Combined_L1)             (None, 100)                                     500                #>  layer__combined_l1_9 (Layer_Combined_L1)             (None, 100)                                     10100              #>  dense_23 (Dense)                                     (None, 3)                                       303                #> ======================================================================================================================== #> Total params: 10,903 #> Trainable params: 10,903 #> Non-trainable params: 0 #> ________________________________________________________________________________________________________________________"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"compile-and-train-both-nns","dir":"Articles","previous_headings":"Simple classification example > Original neural networks","what":"Compile and train both NNs","title":"03 - Classification example using tensorflow","text":"building NNs, compile train . Note , constraining weights trade-learning speed NN, nn2 needs higher number epochs properly learn data. case, need define categorical crossentropy loss use accuracy chosen metric. Compile train nn1 model, visualize :  Compile train nn2 model, visualize :","code":"compile(nn1,         loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),         optimizer = optimizer_adam(),         metrics = \"accuracy\")  history1 <- fit(nn1,                train_x,                train_y,                verbose = 0,                epochs = 200,                validation_split = 0.3 )  plot(history1) compile(nn2,         loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),         optimizer = optimizer_adam(),         metrics = \"accuracy\")  history2 <- fit(nn2,                train_x,                train_y,                verbose = 0,                epochs = 300,                validation_split = 0.3 )  plot(history2)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"obtain-the-predicionts","dir":"Articles","previous_headings":"Simple classification example > Original neural networks","what":"Obtain the predicionts","title":"03 - Classification example using tensorflow","text":"case, asses NNs accuracy transform output probability: predict results test data neural network. (also predict store linear response NNs compared later polynomial output)","code":"probability_model1 <- keras_model_sequential() %>%   nn1() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax) probability_model2 <- keras_model_sequential() %>%   nn2() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax) # Obtain the predicted classes with the NN to compare them prediction_NN_class1 <- predict(probability_model1, test_x)  # Also, the linear output can be predicted before the probability model prediction_NN1 <- predict(nn1, test_x) # Obtain the predicted classes with the NN to compare them prediction_NN_class2 <- predict(probability_model2, test_x)  # Also, the linear output can be predicted before the probability model prediction_NN2 <- predict(nn2, test_x)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"visualize-the-nns-results","dir":"Articles","previous_headings":"Simple classification example > Original neural networks","what":"Visualize the NNs results:","title":"03 - Classification example using tensorflow","text":"can use confusion matrix visualize results, can see NNs correctly predicts almost classes test data:","code":"# Create a confusion matrix cm1 <- caret::confusionMatrix(as.factor(prediction_NN_class1), as.factor(test_y)) cm1 #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 13  0  0 #>          1  0 14  2 #>          2  0  0  9 #>  #> Overall Statistics #>                                            #>                Accuracy : 0.9474           #>                  95% CI : (0.8225, 0.9936) #>     No Information Rate : 0.3684           #>     P-Value [Acc > NIR] : 7.078e-14        #>                                            #>                   Kappa : 0.9202           #>                                            #>  Mcnemar's Test P-Value : NA               #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   0.8182 #> Specificity            1.0000   0.9167   1.0000 #> Pos Pred Value         1.0000   0.8750   1.0000 #> Neg Pred Value         1.0000   1.0000   0.9310 #> Prevalence             0.3421   0.3684   0.2895 #> Detection Rate         0.3421   0.3684   0.2368 #> Detection Prevalence   0.3421   0.4211   0.2368 #> Balanced Accuracy      1.0000   0.9583   0.9091 # Create a confusion matrix cm2 <- caret::confusionMatrix(as.factor(prediction_NN_class2), as.factor(test_y)) cm2 #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 13  0  0 #>          1  0 14  3 #>          2  0  0  8 #>  #> Overall Statistics #>                                            #>                Accuracy : 0.9211           #>                  95% CI : (0.7862, 0.9834) #>     No Information Rate : 0.3684           #>     P-Value [Acc > NIR] : 1.482e-12        #>                                            #>                   Kappa : 0.8799           #>                                            #>  Mcnemar's Test P-Value : NA               #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   0.7273 #> Specificity            1.0000   0.8750   1.0000 #> Pos Pred Value         1.0000   0.8235   1.0000 #> Neg Pred Value         1.0000   1.0000   0.9000 #> Prevalence             0.3421   0.3684   0.2895 #> Detection Rate         0.3421   0.3684   0.2105 #> Detection Prevalence   0.3421   0.4474   0.2105 #> Balanced Accuracy      1.0000   0.9375   0.8636"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"using-nn2poly-to-obtain-the-polynomial","dir":"Articles","previous_headings":"Simple classification example","what":"Using nn2poly to obtain the polynomial","title":"03 - Classification example using tensorflow","text":"NNs trained, can directly call nn2poly keras model. Therefore, need build object weights activation functions default case covered vignette(\"nn2poly-01-introduction\"), can benefit generic methods implemented keras models. parameters added nn2poly work Taylor order expansion layer (taylor_orders), choose 8 default non linear layers 1 last linear layer Taylor used . (final polynomial order limited max_order=3) Note case, 3 output neurons, 3 output polynomials. polynomials stored way regression case, list labels values, case values matrix instead vector, row polynomial obtained output neuron. neural networks compare results:","code":"taylor_orders <- c(8, 8, 8, 1)  # Polynomial for nn1 final_poly1 <- nn2poly(object = nn1,                       taylor_orders = taylor_orders,                       max_order = 3)  # Polynomial for nn2 final_poly2 <- nn2poly(object = nn2,                       taylor_orders = taylor_orders,                       max_order = 3)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"Simple classification example","what":"Obtaining polynomial predictions","title":"03 - Classification example using tensorflow","text":"said , obtained polynomial represents neural network including softmax function computing class assigned observation. , need define keras sequential model includes class computation polynomial output. polynomial output obtained eval_poly(), case matrix form, 3 polynomials evaluated time:","code":"# Obtain the predicted values for the test data with our Polynomial Regression prediction_poly_matrix1 <- eval_poly(x = test_x, poly = final_poly1)  # Define probability model with keras fro the polynomial outputs probability_poly1 <- keras_model_sequential() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax)  # Class prediction with the polynomial outputs prediction_poly_class1 <- predict(probability_poly1,t(prediction_poly_matrix1)) # Obtain the predicted values for the test data with our Polynomial Regression prediction_poly_matrix2 <- eval_poly(x = test_x, poly = final_poly2)  # Define probability model with keras fro the polynomial outputs probability_poly2 <- keras_model_sequential() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax)  # Class prediction with the polynomial outputs prediction_poly_class2 <- predict(probability_poly2,t(prediction_poly_matrix2))"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Simple classification example","what":"Visualizing the results","title":"03 - Classification example using tensorflow","text":"polynomial predictions, two options. can represent diagonal line linear outputs obtained directly polynomial NN predictions, compare assigned classes employing probability models. Please note compare predictions (linear classes) polynomials NN predictions original data, nn2poly’s goal faithfully represent NN behavior independently well NN predicts. First, let’s observe confusion matrix NNs: , can extract diagonal plot polynomials obtained NN, total \\(3\\times 2=6\\) diagonal plots.   can observe polynomials obtain quite similar predictions equivalent NN predictions, specially second polynomial 100% accuracy. However, comparing linear outputs, unconstrained NN presents problems constrained one quite accurate. can also plot \\(n\\) important coefficients absolute value compare variables interactions relevant polynomial. Note , data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant. case, 3 plots NN , one per polynomial output neuron. case, obtained coefficients represent important variables assigning probability class. can see coefficients share characteristics like positive negative nn1and nn2 interpretations, expected predictions differ much.   Finally, problem Taylor expansion can checked following plot, layer represented activation function, Taylor expansion, error also density activation potentials activation functions receives layer. can clearly seen activation potentials density, green, expands wide range unconstrained NN kept closer zero constrained one, thus accurate Taylor expansion around zero.","code":"# Confussion matrix between NN class prediction and polynomial class prediction cm_poly1 <- caret::confusionMatrix(as.factor(prediction_NN_class1), as.factor(prediction_poly_class1)) cm_poly1 #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 13  0  0 #>          1  0 14  2 #>          2  0  0  9 #>  #> Overall Statistics #>                                            #>                Accuracy : 0.9474           #>                  95% CI : (0.8225, 0.9936) #>     No Information Rate : 0.3684           #>     P-Value [Acc > NIR] : 7.078e-14        #>                                            #>                   Kappa : 0.9202           #>                                            #>  Mcnemar's Test P-Value : NA               #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   0.8182 #> Specificity            1.0000   0.9167   1.0000 #> Pos Pred Value         1.0000   0.8750   1.0000 #> Neg Pred Value         1.0000   1.0000   0.9310 #> Prevalence             0.3421   0.3684   0.2895 #> Detection Rate         0.3421   0.3684   0.2368 #> Detection Prevalence   0.3421   0.4211   0.2368 #> Balanced Accuracy      1.0000   0.9583   0.9091 # Confussion matrix between NN class prediction and polynomial class prediction cm_poly2 <- caret::confusionMatrix(as.factor(prediction_NN_class2), as.factor(prediction_poly_class2)) cm_poly2 #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 13  0  0 #>          1  0 17  0 #>          2  0  0  8 #>  #> Overall Statistics #>                                       #>                Accuracy : 1           #>                  95% CI : (0.9075, 1) #>     No Information Rate : 0.4474      #>     P-Value [Acc > NIR] : 5.312e-14   #>                                       #>                   Kappa : 1           #>                                       #>  Mcnemar's Test P-Value : NA          #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   1.0000 #> Specificity            1.0000   1.0000   1.0000 #> Pos Pred Value         1.0000   1.0000   1.0000 #> Neg Pred Value         1.0000   1.0000   1.0000 #> Prevalence             0.3421   0.4474   0.2105 #> Detection Rate         0.3421   0.4474   0.2105 #> Detection Prevalence   0.3421   0.4474   0.2105 #> Balanced Accuracy      1.0000   1.0000   1.0000 for (i in 1:3){   print(     plot_diagonal(x_axis =  prediction_NN1[,i],                   y_axis =  prediction_poly_matrix1[i,],                   xlab = \"NN prediction\",                   ylab = \"Polynomial prediction\")         ) } for (i in 1:3){   print(     plot_diagonal(x_axis =  prediction_NN2[,i],                   y_axis =  prediction_poly_matrix2[i,],                   xlab = \"NN prediction\",                   ylab = \"Polynomial prediction\")         ) } plot_n_important_coeffs(final_poly1, n_important_coeffs = 8) plot_n_important_coeffs(final_poly2, n_important_coeffs = 8) plot_taylor_and_activation_potentials(object = nn1,                                       data = train,                                       taylor_orders = taylor_orders,                                       max_order = 3,                                       constraints = FALSE) #> [[1]] #>  #> [[2]] #>  #> [[3]] plot_taylor_and_activation_potentials(object = nn2,                                       data = train,                                       taylor_orders = taylor_orders,                                       max_order = 3,                                       constraints = TRUE) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-04-torch-regression.html","id":"this-vignettes-goal","dir":"Articles","previous_headings":"","what":"This vignette’s goal","title":"04 - Regression example using torch","text":"showing use nn2poly default version vignette(\"nn2poly-01-introduction\"), present use specific methods related torch (high level interface R luz) allow easier smoother use nn2poly deep learning framework. Furthermore, show impose needed weight constraints torch training accurate results compare results unconstrained neural network. vignette focus simple regression example. classification example covered vignette(\"nn2poly-03-tensorflow-classification\").","code":"library(nn2poly) library(torch) library(luz)  set.seed(42)"},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-04-torch-regression.html","id":"simulated-data-generation","dir":"Articles","previous_headings":"Simple regression example","what":"Simulated data generation","title":"04 - Regression example using torch","text":"simulate polynomial data following polynomial: \\(4x_1 - 3 x_2x_3\\). Data needs scaled \\([-1,1]\\) interval.","code":"# Define the desired polynomial for the simulated data polynomial <- list() polynomial$labels <- list(c(1), c(2,3)) polynomial$values <- c(4,-3) # Define number of variables p and sample n p <- 3 n_sample <- 500  # Predictor variables X <- matrix(0,n_sample,p) for (i in 1:p){   X[,i] <- rnorm(n = n_sample,0,1) }  # Response variable + small error term Y <- as.vector(eval_poly(X,polynomial)) + stats::rnorm(n_sample, 0, 0.1)  # Store all as a data frame data <- as.data.frame(cbind(X, Y)) head(data) #>           V1           V2         V3          Y #> 1  1.3709584  1.029140719  2.3250585 -1.7547416 #> 2 -0.5646982  0.914774868  0.5241222 -3.7107357 #> 3  0.3631284 -0.002456267  0.9707334  1.3609395 #> 4  0.6328626  0.136009552  0.3769734  2.4608270 #> 5  0.4042683 -0.720153545 -0.9959334 -0.6141076 #> 6 -0.1061245 -0.198124330 -0.5974829 -0.7455793 # Data scaling maxs <- apply(data, 2, max) mins <- apply(data, 2, min) data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])  plot(data)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-04-torch-regression.html","id":"original-neural-networks","dir":"Articles","previous_headings":"Simple regression example","what":"Original neural networks","title":"04 - Regression example using torch","text":"build train two different neural networks (NNs), one unconstrained weights (nn1) another one imposing constraint weights (nn2). Different constraints can tested, suggested constraint based theoretical empirical evaluation use L1 norm equal 1, constraining vector weights + bias arriving neuron satisfy L1 norm equal less 1.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-04-torch-regression.html","id":"build-the-nns","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Build the NNs","title":"04 - Regression example using torch","text":"using torch first must define torch dataset, divide data train validation define data loader: , NNs built using sequential model luzand torch. want compare behavior nn2poly constrained unconstrained weights, create 2 identical NNs train without constraints.","code":"# torch dataset nn2poly_dataset <- torch::dataset(   name = \"nn2poly_dataset\",   initialize = function(x, y) {     self$x <- torch::torch_tensor(x)     self$y <- torch::torch_tensor(y)   },   .getitem = function(i) {     x <- self$x[i,]     y <- self$y[i]     list(x = x,          y = y)   },   .length = function() {     self$y$size()[[1]]   } )  data_full <- nn2poly_dataset(train_x, train_y)  # Divide in train and validation all_indices   <- 1:length(data_full) train_indices <- sample(all_indices, size = round(length(data_full)) * 0.8) val_indices   <- setdiff(all_indices, train_indices)  data_train <- torch::dataset_subset(data_full, train_indices) data_val   <- torch::dataset_subset(data_full, val_indices)   # Data loader torch_data <- list(   train = torch::dataloader(data_train, batch_size = 32, shuffle = TRUE),   valid = torch::dataloader(data_val, batch_size = 32) ) luz_nn1 <- function() {   torch::torch_manual_seed(42)      luz_model_sequential(     torch::nn_linear(p,100),     torch::nn_tanh(),     torch::nn_linear(100,100),     torch::nn_tanh(),     torch::nn_linear(100,100),     torch::nn_tanh(),     torch::nn_linear(100,1)   ) }  luz_nn2 <- function() {   torch::torch_manual_seed(42)      luz_model_sequential(     torch::nn_linear(p,100),     torch::nn_tanh(),     torch::nn_linear(100,100),     torch::nn_tanh(),     torch::nn_linear(100,100),     torch::nn_tanh(),     torch::nn_linear(100,1)   ) }"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-04-torch-regression.html","id":"nns-training","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"NNs training","title":"04 - Regression example using torch","text":"first train first NN (luz_nn1) without constraints: order implement desired constraints, provide add_constraints() function, allows set desired constraints torch training setup callbacks. constraints implementation bias neuron included weights vector incident neuron, meaning previous layer \\(h\\) neurons, considered weight vector including bias given neuron dimension \\(h+1\\), bias first element. Currently, L1 norm L2 norm equal 1 implemented options. Note L1 norm equal 1 scaling input data \\([-1,1]\\) interval recommended option.","code":"fitted_1 <- luz_nn1() %>%     luz::setup(       loss = torch::nn_mse_loss(),       optimizer = torch::optim_adam,       metrics = list(         luz::luz_metric_mse()       )     ) %>%     luz::fit(torch_data$train, epochs = 50, valid_data = torch_data$valid) # Do the constrained training with the l1 constraint   fitted_2 <- luz_nn2() %>%     luz::setup(       loss = torch::nn_mse_loss(),       optimizer = torch::optim_adam,     ) %>%     add_constraints(\"l1_norm\") %>%     fit(torch_data$train, epochs = 600, valid_data = torch_data$valid)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-04-torch-regression.html","id":"visualize-both-nn-predictions","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Visualize both NN predictions","title":"04 - Regression example using torch","text":"can visualize NN predictions vs original Y values neural networks observe provide accurate predictions (values fall near “perfect” diagonal red line).","code":"# Obtain the predicted values with the NN to compare them prediction_NN1 <- as.array(predict(fitted_1, test_x))  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN1, y_axis =  test_y, xlab = \"NN 1 prediction\", ylab = \"Original Y\") fitted_1 %>% plot() # Obtain the predicted values with the NN to compare them prediction_NN2 <- as.array(predict(fitted_2, test_x))  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN2, y_axis =  test_y, xlab = \"NN 2 prediction\", ylab = \"Original Y\") fitted_2 %>% plot()"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-04-torch-regression.html","id":"using-nn2poly-to-obtain-the-polynomial","dir":"Articles","previous_headings":"Simple regression example","what":"Using nn2poly to obtain the polynomial","title":"04 - Regression example using torch","text":"NNs trained, can directly call nn2poly luz/torch sequential model. Therefore, need build object weights activation functions default case covered vignette(\"nn2poly-01-introduction\"), can benefit generic methods implemented torch models. parameters added nn2poly work chosen limit polynomial order max_order=3. neural networks compare results:","code":"# Polynomial for nn1 final_poly1 <- nn2poly(object = fitted_1,                       max_order = 3)  # Polynomial for nn2 final_poly2 <- nn2poly(object = fitted_2,                       max_order = 3)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-04-torch-regression.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"Simple regression example","what":"Obtaining polynomial predictions","title":"04 - Regression example using torch","text":"","code":"# Obtain the predicted values for the test data with our two polynomials  prediction_poly1 <- eval_poly(x = test_x, poly = final_poly1)  prediction_poly2 <- eval_poly(x = test_x, poly = final_poly2)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-04-torch-regression.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Simple regression example","what":"Visualizing the results","title":"04 - Regression example using torch","text":"polynomial predictions, can plot using diagonal plot compare respective NN predictions. Please note compare predictions polynomial NN predictions original data, nn2poly’s goal faithfully represent NN behavior independently well NN predicts. can observe clearly polynomial obtained constrained network (nn2) predicting almost , unconstrained network significant errors.   can also plot \\(n\\) important coefficients absolute value compare variables interactions relevant polynomial. Note , data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant. Recall original polynomial \\(4x_1 - 3x_2x_3\\). observe polynomial nn2, precisely interaction 2,3 high negative coefficient variable 1 positive one rest variables intercept (0) quite close zero. However, polynomial nn1, obtained coefficients correct Taylor expansion failing high weights.   Finally, problem Taylor expansion can checked following plot, layer represented activation function, Taylor expansion, error also density activation potentials activation functions receives layer. can clearly seen activation potentials density, green, expands wide range unconstrained NN kept closer zero constrained one, thus accurate Taylor expansion around zero.","code":"plot_diagonal(x_axis =  prediction_NN1, y_axis =  prediction_poly1, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for NN1\") plot_diagonal(x_axis =  prediction_NN2, y_axis =  prediction_poly2, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for NN2\") plot_n_important_coeffs(final_poly1, n_important_coeffs = 8) plot_n_important_coeffs(final_poly2, n_important_coeffs = 8) taylor_orders <- c(8, 8, 8, 1) # Esto debería ser tomado de algun sitio o dejado como default igual que en el uso de nn2poly  plot_taylor_and_activation_potentials(object = fitted_1,                                       data = train,                                       taylor_orders = taylor_orders,                                       max_order = 3,                                       constraints = FALSE) #> [[1]] #>  #> [[2]] #>  #> [[3]] #>  #> [[4]] plot_taylor_and_activation_potentials(object = fitted_2,                                       data = train,                                       taylor_orders = taylor_orders,                                       max_order = 3,                                       constraints = TRUE) #> [[1]] #>  #> [[2]] #>  #> [[3]] #>  #> [[4]]"},{"path":"https://ibidat.github.io/nn2poly/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Pablo Morala. Author, maintainer. Iñaki Ucar. Author. Jose Ignacio Diez. Contractor.","code":""},{"path":"https://ibidat.github.io/nn2poly/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Morala P, Ucar (2023). nn2poly: Neural Network Weights Transformation Polynomial Coefficients. R package version 0.0.0.9000, https://ibidat.github.io/nn2poly/.","code":"@Manual{,   title = {nn2poly: Neural Network Weights Transformation into Polynomial Coefficients},   author = {Pablo Morala and Iñaki Ucar},   year = {2023},   note = {R package version 0.0.0.9000},   url = {https://ibidat.github.io/nn2poly/}, }"},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"nn2poly-transforming-neural-networks-into-polynomials","dir":"","previous_headings":"","what":"Neural Network Weights Transformation into Polynomial Coefficients","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"nn2poly package implements NN2Poly method allows transform already trained deep feed-forward fully connected neural network polynomial representation predicts similar possible original neural network.","code":""},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"related-papers","dir":"","previous_headings":"","what":"Related Papers:","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo, Iñaki Ucar (2021). “Towards mathematical framework inform neural network modelling via polynomial regression.” Neural Networks, 142, 57-72. doi:[10.1016/j.neunet.2021.04.036](https://doi.org/10.1016/j.neunet.2021.04.036) Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo, Iñaki Ucar (2023). “NNN2Poly: Polynomial Representation Deep Feed-Forward Artificial Neural Networks.” IEEE Transactions Neural Networks Learning Systems, (Early Access). doi:[10.1109/TNNLS.2023.3330328](https://doi.org/10.1109/TNNLS.2023.3330328)","code":""},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"installation GitHub requires remotes package.","code":"# install.packages(\"remotes\") remotes::install_github(\"IBiDat/nn2poly\")"},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":null,"dir":"Reference","previous_headings":"","what":"Add constraints to a neural network — add_constraints","title":"Add constraints to a neural network — add_constraints","text":"function sets neural network object constraints required nn2poly algorithm.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add constraints to a neural network — add_constraints","text":"","code":"add_constraints(object, type = c(\"l1_norm\", \"l2_norm\"), ...)"},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add constraints to a neural network — add_constraints","text":"object neural network object. type Constraint type. Currently, l1_norm l2_norm supported. ... Additional arguments (unused).","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add constraints to a neural network — add_constraints","text":"nn2poly neural network object.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"Evaluates one several polynomials one data points desired variables.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"","code":"eval_poly(x, poly)"},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"x Input data matrix, vector dataframe. number columns number variables polynomial (dimension p). Response variable predicted included. poly List containing 2 items: labels values. labels List integer vectors length (number rows) values, integer vector denotes combination variables associated coefficient value stored position values. Note variables numbered 1 p. values Matrix (also vector single polynomial), row represents polynomial, number columns length labels, containing column value coefficient given equivalent label position. Example: labels contains integer vector c(1,1,3) position 5, value stored values position 5 coefficient associated term x_1^2*x_3.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"Vector containing evaluation single polynomial matrix containing evaluation polynomials. row corresponds polynomial used column observation, meaning row vector corresponds results evaluating given data polynomial.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"","code":"# Create the polynomial 1 + (-1)·x_1 + 1·x_2 + 0.5·(x_1)^2 as a list poly <- list() poly$values <- c(1,-1,1,0.5) poly$labels <- list(c(0),c(1),c(2),c(1,1)) # Create two observations, (x_1,x_2) = (1,2) and (x_1,x_2) = (3,1) x <- rbind(c(1,2), c(3,1)) # Evaluate the polynomial on both observations eval_poly(x,poly) #> [1] 2.5 3.5"},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":null,"dir":"Reference","previous_headings":"","what":"Luz Model composed of a linear stack of layers — luz_model_sequential","title":"Luz Model composed of a linear stack of layers — luz_model_sequential","text":"Luz Model composed linear stack layers","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Luz Model composed of a linear stack of layers — luz_model_sequential","text":"","code":"luz_model_sequential(...)"},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Luz Model composed of a linear stack of layers — luz_model_sequential","text":"... Sequence modules added.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Luz Model composed of a linear stack of layers — luz_model_sequential","text":"nn_sequential module.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain a polynomial representation from a trained neural network — nn2poly","title":"Obtain a polynomial representation from a trained neural network — nn2poly","text":"Implements main NN2Poly algorithm obtain polynomial representation trained neural network using weights Taylor expansion activation functions.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain a polynomial representation from a trained neural network — nn2poly","text":"","code":"nn2poly(   object,   max_order = 2,   keep_layers = FALSE,   taylor_orders = 8,   ...,   all_partitions = NULL )"},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain a polynomial representation from a trained neural network — nn2poly","text":"object object computation NN2Poly algorithm desired. Currently supports models following deep learning frameworks: tensorflow/keras models built sequential model. torch/luz models built sequential model. also supports named list input allows introduce hand model source. list length L (number hidden layers + 1) containing weights matrix layer. element list names activation function used layer. layer \\(l\\), expected shape matrices form \\((h_{(l-1)} + 1)*(h_l)\\), , number rows number neurons previous layer plus bias vector, number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer. bias vector first row. max_order integer determines maximum order forced final polynomial, discarding terms higher order naturally arise considering Taylor expansions allowed taylor_orders. keep_layers Boolean determines polynomials computed internal layers stored given output (TRUE), polynomials last layer needed (FALSE). taylor_orders integer vector length L sets degree Taylor expansion truncated layer. single value used, value set non linear layer 1 linear layer activation function. Default set 8. ... Ignored. all_partitions Optional argument containing needed multipartitions list lists lists. set NULL, nn2poly compute said multipartitions. step can computationally expensive chosen polynomial order dimension high. cases, encouraged multipartitions stored reused possible.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain a polynomial representation from a trained neural network — nn2poly","text":"object class nn2poly. keep_layers = FALSE (default case), returns list two items: item named labels list integer vectors. vectors represent monomial polynomial, integer vector represents time one original variables appears term. example, vector c(1,1,2) represents term \\(x_1^2x_2\\). item named values contains matrix row contains coefficients polynomial associated output neuron. , neural network single output unit, matrix values single row multiple output units, matrix values several rows. keep_layers = TRUE, returns list length L layer contains item explained . last element list element keep_layers = FALSE. polynomials obtained hidden layers needed represent NN can used explore insights NN.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a comparison between two sets of points. — plot_diagonal","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"points come predictions NN PM line (plot.line = TRUE) displayed, case method exhibit asymptotic behavior, points fall line.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"","code":"plot_diagonal(   x_axis,   y_axis,   xlab = NULL,   ylab = NULL,   title = NULL,   plot.line = TRUE )"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"x_axis Values plot x axis. y_axis Values plot y axis. xlab Lab x axis ylab Lab y axis. title Title plot. plot.line red line slope = 1 intercept = 0 plotted.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"Plot (ggplot object).","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_n_important_coeffs.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot n most important coefficients. — plot_n_important_coeffs","title":"Plot n most important coefficients. — plot_n_important_coeffs","text":"function takes polynomial (several ones) given nn2poly algorithm, plots absolute magnitude barplots able compare important coefficients. number plotted coefficients controlled n_important_coeffs.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_n_important_coeffs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot n most important coefficients. — plot_n_important_coeffs","text":"","code":"plot_n_important_coeffs(poly, n_important_coeffs)"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_n_important_coeffs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot n most important coefficients. — plot_n_important_coeffs","text":"poly polynomial represented list \"labels\" \"values\", manner returned nn2poly algorithm. n_important_coeffs integer denoting number coefficients plotted, ordering absolute magnitude.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_n_important_coeffs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot n most important coefficients. — plot_n_important_coeffs","text":"plot showing n important coefficients.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"Function allows take NN data input values plot distribution data activation potentials (sum input values * weights) neurons together layer Taylor expansion used activation functions. layer 'linear' (usually output), layer approximation Taylor expansion needed.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"","code":"plot_taylor_and_activation_potentials(   object,   data,   taylor_orders,   max_order,   constraints,   taylor_interval = 1.5,   ... )"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"object object computation NN2Poly algorithm desired. Currently supports models following deep learning frameworks: tensorflow/keras models built sequential model. torch/luz models built sequential model. also supports named list input allows introduce hand model source. list length L (number hidden layers + 1) containing weights matrix layer. element list names activation function used layer. layer \\(l\\), expected shape matrices form \\((h_{(l-1)} + 1)*(h_l)\\), , number rows number neurons previous layer plus bias vector, number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer. bias vector first row. data Matrix data frame containing predictor variables (X) used input compute activation potentials. response variable column included. taylor_orders integer vector length L sets degree Taylor expansion truncated layer. single value used, value set non linear layer 1 linear layer activation function. Default set 8. max_order integer determines maximum order forced final polynomial, discarding terms higher order naturally arise considering Taylor expansions allowed taylor_orders. constraints Boolean parameter determining NN constrained (TRUE) (FALSE). modifies de plots title show \"constrained\" \"unconstrained\" respectively. taylor_interval optional parameter determining interval Taylor expansion represented. Default 1.5. ... Additional parameters.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"list plots.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 method for class 'nn2poly' — predict.nn2poly","title":"S3 method for class 'nn2poly' — predict.nn2poly","text":"S3 method class 'nn2poly'","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 method for class 'nn2poly' — predict.nn2poly","text":"","code":"# S3 method for nn2poly predict(object, newdata, ...)"},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 method for class 'nn2poly' — predict.nn2poly","text":"object object class inheriting 'nn2poly'. newdata Matrix predictions made. ... Additional arguments.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"S3 method for class 'nn2poly' — predict.nn2poly","text":"matrix containing predictions. one prediction row newdata.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics fit","code":""}]
