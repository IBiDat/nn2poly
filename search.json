[{"path":"https://ibidat.github.io/nn2poly/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 Pablo Morala Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"overall-package-goal","dir":"Articles","previous_headings":"","what":"Overall package goal","title":"01 - Introduction to nn2poly","text":"main objective nn2poly obtain representation feed forward artificial neural network (like multilayered perceptron) terms polynomial representation. coefficients polynomials obtained applying first Taylor expansion activation function neural network. expansions given neural network weights joint using combinatorial properties, obtaining final value polynomial coefficients. information theoretical insights underlying mathematical process used build relationship can found following references: * Initial development idea single hidden layer neural network article free access arXiv preprint version. * Extension deeper layers proper formulation NN2Poly method arXiv preprint.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"this-vignettes-goal","dir":"Articles","previous_headings":"","what":"This vignette’s goal","title":"01 - Introduction to nn2poly","text":"aim present simple use cases nn2poly. matter, showcase use regression problem simulated data classification problem iris dataset. cases, neural network trained first solve problem, nn2poly generate one several polynomials approximate neural network behavior. case neural network training constraint imposed. , explained previously, final approximation polynomial may accurate enough. information impose constraints obtain good approximations, please see vignette(\"nn2poly-02-constraints\"). initial setup follows, besides nn2poly package, load keras package, also loads tensorflow, used example build train neural networks.","code":"library(nn2poly) library(keras)  # For reproducibility set.seed(1) tensorflow::tf$random$set_seed(1)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"simple-regression-example","dir":"Articles","previous_headings":"This vignette’s goal","what":"Simple regression example","title":"01 - Introduction to nn2poly","text":"example solve regression problem using simulated data polynomial, allows control final polynomial coefficients obtained nn2poly similar polynomial originates data.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"simulated-data-generation","dir":"Articles","previous_headings":"This vignette’s goal > Simple regression example","what":"Simulated data generation","title":"01 - Introduction to nn2poly","text":"simulate polynomial data follows. First define polynomial using format needed nn2poly, specifically use function eval_poly, consists list containing: * Labels: list integer vectors denoting combinations variables appear term polynomial. Variables numbered 1 p p dimension problem. example, c(1,1,3) represent term \\(x_1^2x_3\\) * Values: Vector containing numerical values coefficients denoted labels. multiple polynomials terms different coefficients want represented, matrix can employed, row polynomial. create polynomial : \\(4x_1 - 3 x_2x_3\\): said polynomial, can now generate desired data train NN example. employ normal distribution generate variables \\(x_1, x_2, x_3\\) also error term \\(\\epsilon\\). Therefore, response variable \\(y\\) generated : \\(y = 4x_1 - 3 x_2x_3 + \\epsilon\\) scale data everything \\([-1,1]\\) interval divide train test datasets.","code":"polynomial <- list() polynomial$labels <- list(c(1), c(2,3)) polynomial$values <- c(4,-3) # Define number of variables p and sample n p <- 3 n_sample <- 500  # Predictor variables X <- matrix(0,n_sample,p) for (i in 1:p){   X[,i] <- rnorm(n = n_sample,0,1) }  # Response variable + small error term Y <- as.vector(eval_poly(X,polynomial)) + stats::rnorm(n_sample, 0, 0.1)  # Store all as a data frame data <- as.data.frame(cbind(X, Y)) head(data) #>           V1          V2          V3         Y #> 1 -0.6264538  0.07730312  1.13496509 -2.684020 #> 2  0.1836433 -0.29686864  1.11193185  1.632335 #> 3 -0.8356286 -1.18324224 -0.87077763 -6.344179 #> 4  1.5952808  0.01129269  0.21073159  6.279883 #> 5  0.3295078  0.99160104  0.06939565  1.165488 #> 6 -0.8204684  1.59396745 -1.66264885  4.650553 # Data scaling maxs <- apply(data, 2, max) mins <- apply(data, 2, min) data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"original-neural-network","dir":"Articles","previous_headings":"This vignette’s goal > Simple regression example","what":"Original neural network","title":"01 - Introduction to nn2poly","text":"simulated data ready, can train feed forward dense neural network. method expected applied given trained densely connected feed forward neural network (NN now ), also referred multilayer perceptron (MLP). Therefore, step completely optional can skipped preferred method used train NN already given NN weights. order present example, create train NN. choice use keras framework build train . First, build model. Compile model: train : can visualize training process:  can also visualize NN predictions vs original Y values.","code":"nn <- keras_model_sequential()  nn %>% layer_dense(units = 10,                   activation = \"tanh\",                   input_shape = p)  nn %>% layer_dense(units = 10,                   activation = \"tanh\")  nn %>% layer_dense(units = 1,                   activation = \"linear\")  nn #> Model: \"sequential_12\" #> __________________________________________________________________________________________________________________________ #>  Layer (type)                                         Output Shape                                     Param #             #> ========================================================================================================================== #>  dense_14 (Dense)                                     (None, 10)                                       40                  #>  dense_15 (Dense)                                     (None, 10)                                       110                 #>  dense_16 (Dense)                                     (None, 1)                                        11                  #> ========================================================================================================================== #> Total params: 161 #> Trainable params: 161 #> Non-trainable params: 0 #> __________________________________________________________________________________________________________________________ compile(nn,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\") history <- fit(nn,                train_x,                train_y,                verbose = 0,                epochs = 300,                validation_split = 0.3 ) plot(history) # Obtain the predicted values with the NN to compare them prediction_NN <- predict(nn, test_x)  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN, y_axis =  test_y, xlab = \"NN prediction\", ylab = \"Original Y\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"using-nn2poly-to-obtain-the-polynomial","dir":"Articles","previous_headings":"This vignette’s goal > Simple regression example","what":"Using nn2poly to obtain the polynomial","title":"01 - Introduction to nn2poly","text":"NN trained, using chosen method user, parameters extracted reshaped, needed, match expected input function nn2poly_algorithm(). input consists following objects: weights_list: list matrices weight matrix layer. weights matrices dimension ((1+input) * output) first row corresponds bias vector, rest rows correspond ordered vector weights associated input. af_string_list: list strings names activation functions layer. q_taylor_vector: vector integers containing order Taylor expansion performed layer. output layer linear activation function, last value 1. forced_max_Q: (optional value) integer value denoting maximum order terms computed polynomial. Usually 2 3 enough practice. Note higher orders suppose explosion possible combinations. user provide value, polynomial order grows multiplicatively Taylor order hidden layer, therefore better start low values. Following example NN created previously, need extract weights biases reshape . Particularly, keras framework default separates kernel weights matrices dimension (input * output) bias vectors (1 * output), need add bias first row matrix ((1+input) * output). activation functions used can stored : finally order Taylor approximation going choose 8 hidden layer. (final polynomial order limited forced_max_Q=3) input desired shape, nn2poly method can applied: can glimpse coefficients polynomial stored. Note structure explained polynomial generated data, list labels values. case, obtained polynomial order 3.","code":"keras_weights <- keras::get_weights(nn)  # Due to keras giving weights separated from the bias, we have twice the # elements that we want: n <- length(keras_weights)/2 nn_weights <- vector(mode = \"list\", length = n) for (i in 1:n){   nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]]) } af_string_list <- list(\"tanh\",\"tanh\", \"linear\") q_taylor_vector <- c(8, 8,  1) final_poly <- nn2poly_algorithm(   weights_list = nn_weights,   af_string_list = af_string_list,   q_taylor_vector = q_taylor_vector,   store_coeffs = FALSE,   forced_max_Q = 3 ) final_poly #> $labels #> $labels[[1]] #> [1] 0 #>  #> $labels[[2]] #> [1] 1 #>  #> $labels[[3]] #> [1] 2 #>  #> $labels[[4]] #> [1] 3 #>  #> $labels[[5]] #> [1] 1 1 #>  #> $labels[[6]] #> [1] 1 2 #>  #> $labels[[7]] #> [1] 1 3 #>  #> $labels[[8]] #> [1] 2 2 #>  #> $labels[[9]] #> [1] 2 3 #>  #> $labels[[10]] #> [1] 3 3 #>  #> $labels[[11]] #> [1] 1 1 1 #>  #> $labels[[12]] #> [1] 1 1 2 #>  #> $labels[[13]] #> [1] 1 1 3 #>  #> $labels[[14]] #> [1] 1 2 2 #>  #> $labels[[15]] #> [1] 1 2 3 #>  #> $labels[[16]] #> [1] 1 3 3 #>  #> $labels[[17]] #> [1] 2 2 2 #>  #> $labels[[18]] #> [1] 2 2 3 #>  #> $labels[[19]] #> [1] 2 3 3 #>  #> $labels[[20]] #> [1] 3 3 3 #>  #>  #> $values #>           [,1]      [,2]        [,3]        [,4]          [,5]        [,6]        [,7]       [,8]      [,9]      [,10] #> [1,] 0.1563016 0.7812154 -0.03797179 -0.05108133 -0.0002868382 -0.04386124 -0.01815984 -0.1081569 -2.742122 -0.1109731 #>            [,11]      [,12]      [,13]       [,14]     [,15]     [,16]     [,17]     [,18]      [,19]     [,20] #> [1,] -0.05425595 -0.1401266 0.08083027 -0.08008732 0.1254146 0.2044834 0.0233742 0.7296176 0.01581843 0.3277675"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"This vignette’s goal > Simple regression example","what":"Obtaining polynomial predictions","title":"01 - Introduction to nn2poly","text":"obtaining polynomial coefficients, can use predict response variable \\(Y\\), can done using function eval_poly():","code":"# Obtain the predicted values for the test data with our polynomial prediction_poly <- as.vector(eval_poly(x = test_x, poly = final_poly))"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"This vignette’s goal > Simple regression example","what":"Visualizing the results","title":"01 - Introduction to nn2poly","text":"advisable always check predictions obtained new polynomial differ much original neural network predictions (case differ, can also try find checking Taylor expansions). help , couple functions included allow us plot results. simple plot comparing polynomial NN predictions can obtained plot_diagonal(), red diagonal line represents perfect relationship NN polynomial predictions obtained. example, theoretical weights constraints imposed, can observe approximation perfect.  Another convenient plot show algorithm affected layer can obtained plot_taylor_and_activation_potentials(), activation potentials neuron computed presented Taylor expansion approximation activation function layer. case, used constraints NN training, activation potentials strictly centered around zero.","code":"plot_diagonal(x_axis =  prediction_NN, y_axis =  prediction_poly, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") plot_taylor_and_activation_potentials(data = train,                                     weights_list = nn_weights,                                     af_string_list = af_string_list,                                     q_taylor_vector = q_taylor_vector,                                     forced_max_Q = 3,                                     my_max_norm = list(\"unconstrained\",1)) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"simple-classification-example","dir":"Articles","previous_headings":"This vignette’s goal","what":"Simple classification example","title":"01 - Introduction to nn2poly","text":"example, instead regression problem show classification example, NN trained classify species iris dataset, nn2poly employed obtain polynomial species.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"data-preparation","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Data preparation","title":"01 - Introduction to nn2poly","text":"load iris dataset R scale everything \\([-1,1]\\) interval divide train test datasets, manner previous regression example.","code":"# Load the data data(iris)  # Change response to numeric. In this case, Species was already numeric, # but this step is needed if it is a factor variable. iris$Species <- as.numeric(iris$Species)  # Define dimension p (number of predictor variables) p <- dim(iris)[2] - 1  # Define objective classes n_class <- max(iris[,(p+1)])  # Move objective classes from (1:3) to (0:2), needed for tensorflow iris[,(p+1)] <- iris[,(p+1)] - 1 # Scale the data in the [-1,1] interval and separate train and test # Only the predictor variables are scaled, not the response as those will be # the different classes. iris_x <- iris[,-(p+1)] maxs <- apply(iris_x, 2, max) mins <- apply(iris_x, 2, min) data_x_scaled <- as.data.frame(scale(iris_x, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2)) data <- cbind(data_x_scaled, iris[,(p+1)])  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"original-neural-network-1","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Original neural network","title":"01 - Introduction to nn2poly","text":"can now train NN, following procedure regression problem. First, build model. Note case NN linear output number neurons number classes predict (3 species). , linear output transformed probability find probable class step done training. Therefore, nn2poly used obtain polynomial approximates nn linear outputs results also transformed ito predict highest probability class. Compile model: train : can visualize training process:  case, asses NN accuracy transform nn output probability: predict results test data: can use confusion matrix visualize results, can see NN correctly predicts classes observation:","code":"nn <- keras_model_sequential()  nn %>% layer_dense(units = 100,                   activation = \"tanh\",                   input_shape = p)  nn %>% layer_dense(units = 100,                   activation = \"tanh\")  nn %>% layer_dense(units = n_class)  nn #> Model: \"sequential_13\" #> __________________________________________________________________________________________________________________________ #>  Layer (type)                                         Output Shape                                     Param #             #> ========================================================================================================================== #>  dense_17 (Dense)                                     (None, 100)                                      500                 #>  dense_18 (Dense)                                     (None, 100)                                      10100               #>  dense_19 (Dense)                                     (None, 3)                                        303                 #> ========================================================================================================================== #> Total params: 10,903 #> Trainable params: 10,903 #> Non-trainable params: 0 #> __________________________________________________________________________________________________________________________ compile(nn,         loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),         optimizer = optimizer_adam(),         metrics = \"accuracy\") history <- fit(nn,                train_x,                train_y,                verbose = 0,                epochs = 200,                validation_split = 0.3 ) plot(history) probability_model <- keras_model_sequential() %>%   nn() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax) # Obtain the predicted classes with the NN to compare them prediction_NN_class <- predict(probability_model, test_x)  # Also, the linear output can be predicted before the probability model prediction_NN <- predict(nn, test_x) # Create a confusion matrix cm <- caret::confusionMatrix(as.factor(prediction_NN_class), as.factor(test_y)) cm #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 15  0  0 #>          1  0 11  0 #>          2  0  0 12 #>  #> Overall Statistics #>                                       #>                Accuracy : 1           #>                  95% CI : (0.9075, 1) #>     No Information Rate : 0.3947      #>     P-Value [Acc > NIR] : 4.568e-16   #>                                       #>                   Kappa : 1           #>                                       #>  Mcnemar's Test P-Value : NA          #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   1.0000 #> Specificity            1.0000   1.0000   1.0000 #> Pos Pred Value         1.0000   1.0000   1.0000 #> Neg Pred Value         1.0000   1.0000   1.0000 #> Prevalence             0.3947   0.2895   0.3158 #> Detection Rate         0.3947   0.2895   0.3158 #> Detection Prevalence   0.3947   0.2895   0.3158 #> Balanced Accuracy      1.0000   1.0000   1.0000"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"using-nn2poly-to-obtain-the-polynomial-1","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Using nn2poly to obtain the polynomial","title":"01 - Introduction to nn2poly","text":"NN trained, need extract reshape parameters explained regression case: activation functions used can stored : finally order Taylor approximation going choose 8 hidden layer. (final polynomial order limited forced_max_Q=3) input desired shape, nn2poly method can applied: can glimpse coefficients polynomial stored. explained , structure list labels values. case, 3 output polynomials (one per output neuron), values stored matrix 3 rows, one corresponding final polynomial neuron.","code":"keras_weights <- keras::get_weights(nn)  # Due to keras giving weights separated from the bias, we have twice the # elements that we want: n <- length(keras_weights)/2 nn_weights <- vector(mode = \"list\", length = n) for (i in 1:n){   nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]]) } af_string_list <- list(\"tanh\",\"tanh\", \"linear\") q_taylor_vector <- c(8, 8,  1) final_poly <- nn2poly_algorithm(   weights_list = nn_weights,   af_string_list = af_string_list,   q_taylor_vector = q_taylor_vector,   store_coeffs = FALSE,   forced_max_Q = 3 ) final_poly #> $labels #> $labels[[1]] #> [1] 0 #>  #> $labels[[2]] #> [1] 1 #>  #> $labels[[3]] #> [1] 2 #>  #> $labels[[4]] #> [1] 3 #>  #> $labels[[5]] #> [1] 4 #>  #> $labels[[6]] #> [1] 1 1 #>  #> $labels[[7]] #> [1] 1 2 #>  #> $labels[[8]] #> [1] 1 3 #>  #> $labels[[9]] #> [1] 1 4 #>  #> $labels[[10]] #> [1] 2 2 #>  #> $labels[[11]] #> [1] 2 3 #>  #> $labels[[12]] #> [1] 2 4 #>  #> $labels[[13]] #> [1] 3 3 #>  #> $labels[[14]] #> [1] 3 4 #>  #> $labels[[15]] #> [1] 4 4 #>  #> $labels[[16]] #> [1] 1 1 1 #>  #> $labels[[17]] #> [1] 1 1 2 #>  #> $labels[[18]] #> [1] 1 1 3 #>  #> $labels[[19]] #> [1] 1 1 4 #>  #> $labels[[20]] #> [1] 1 2 2 #>  #> $labels[[21]] #> [1] 1 2 3 #>  #> $labels[[22]] #> [1] 1 2 4 #>  #> $labels[[23]] #> [1] 1 3 3 #>  #> $labels[[24]] #> [1] 1 3 4 #>  #> $labels[[25]] #> [1] 1 4 4 #>  #> $labels[[26]] #> [1] 2 2 2 #>  #> $labels[[27]] #> [1] 2 2 3 #>  #> $labels[[28]] #> [1] 2 2 4 #>  #> $labels[[29]] #> [1] 2 3 3 #>  #> $labels[[30]] #> [1] 2 3 4 #>  #> $labels[[31]] #> [1] 2 4 4 #>  #> $labels[[32]] #> [1] 3 3 3 #>  #> $labels[[33]] #> [1] 3 3 4 #>  #> $labels[[34]] #> [1] 3 4 4 #>  #> $labels[[35]] #> [1] 4 4 4 #>  #>  #> $values #>            [,1]       [,2]      [,3]       [,4]      [,5]       [,6]        [,7]       [,8]       [,9]      [,10] #> [1,]  0.8657658 -1.7300284  8.113782 -10.717649 -7.525810  0.1469271 -0.70277557  0.8939304  0.5885493  0.1100270 #> [2,]  5.4942742  1.4966233 -1.085263  -1.835088 -3.184985 -0.2746923  0.04573656  0.6527696  0.8150438 -0.8027244 #> [3,] -5.7575579 -0.5725639 -4.417216   8.945973  8.171859  0.2144595  0.33243746 -1.0557353 -1.0806295  0.6099637 #>           [,11]     [,12]     [,13]     [,14]     [,15]         [,16]      [,17]       [,18]       [,19]      [,20] #> [1,]  0.4881656  1.068435 -1.166918 -2.792616 -1.501065  0.1063938930 -0.4377667  0.51825850  0.34801827 -0.4742372 #> [2,]  2.7916732  2.479559 -3.668994 -7.058366 -3.785811 -0.0600402544  0.1777157 -0.06871239  0.02029091  0.1880001 #> [3,] -2.6275450 -2.659728  3.825817  7.535975  4.007151  0.0008082067  0.1119611 -0.25966474 -0.25119710  0.1195640 #>           [,21]      [,22]       [,23]      [,24]      [,25]      [,26]      [,27]      [,28]      [,29]     [,30] #> [1,] -1.0910061 -0.5866069  0.57747843  0.3071512  0.4076099 -1.0644389  3.6230229  2.3926840  3.7525210 -6.062567 #> [2,]  0.7459916  0.5918715  0.06005741 -1.4353904  0.1645449  0.2718333 -0.6268611 -0.1363726 -0.4302278 -1.263415 #> [3,] -0.3287014 -0.3277693 -0.41444497  1.1755901 -0.4089281  0.4415258 -1.7749913 -1.4421963 -2.0169330  4.834294 #>            [,31]      [,32]     [,33]     [,34]     [,35] #> [1,]  2.50018436  2.2998056  4.735468  4.741073  1.159419 #> [2,]  0.07348067  0.8013098  3.284242  3.329447  1.498424 #> [3,] -1.67965325 -2.0488358 -5.474633 -5.517897 -1.817963"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"obtaining-polynomial-predictions-1","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Obtaining polynomial predictions","title":"01 - Introduction to nn2poly","text":"said , obtained polynomial represents neural network including softmax function computing class assigned observation. , need define keras sequential model includes class computation polynomial output. polynomial output obtained eval_poly(), case matrix form, 3 polynomials evaluated time:","code":"# Obtain the predicted values for the test data with our Polynomial Regression prediction_poly_matrix <- eval_poly(x = test_x, poly = final_poly)  # Define probability model with keras fro the polynomial outputs probability_poly <- keras_model_sequential() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax)  # Class prediction with the polynomial outputs prediction_poly_class <- predict(probability_poly,t(prediction_poly_matrix))"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"visualising-the-results","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Visualising the results","title":"01 - Introduction to nn2poly","text":", confusion matrix useful visualize predictions, case comparing polynomial predictions NN predictions. can also compare linear output NN polynomial output, computing assigned class. case can see even classes correctly assigned, linear output well approximated polynomial. due example computed without kind constraints NN, problem solved vignette(\"nn2poly-02-constraints\"):  order try identify problem arising, can use synaptic potential plots. case easy see activation potentials widely spread x axis. Therefore, constraints needed keep close zero:","code":"# Confussion matrix between NN class prediction and polynomial class prediction cm <- caret::confusionMatrix(as.factor(prediction_NN_class), as.factor(prediction_poly_class)) cm #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 14  0  1 #>          1  0 11  0 #>          2  0  0 12 #>  #> Overall Statistics #>                                            #>                Accuracy : 0.9737           #>                  95% CI : (0.8619, 0.9993) #>     No Information Rate : 0.3684           #>     P-Value [Acc > NIR] : 2.196e-15        #>                                            #>                   Kappa : 0.9603           #>                                            #>  Mcnemar's Test P-Value : NA               #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   0.9231 #> Specificity            0.9583   1.0000   1.0000 #> Pos Pred Value         0.9333   1.0000   1.0000 #> Neg Pred Value         1.0000   1.0000   0.9615 #> Prevalence             0.3684   0.2895   0.3421 #> Detection Rate         0.3684   0.2895   0.3158 #> Detection Prevalence   0.3947   0.2895   0.3158 #> Balanced Accuracy      0.9792   1.0000   0.9615 for (i in 1:3){   print(     plot_diagonal(x_axis =  prediction_NN[,i],                   y_axis =  prediction_poly_matrix[i,],                   xlab = \"NN prediction\",                   ylab = \"Polynomial prediction\")         ) } plot_taylor_and_activation_potentials(data = train,                                     weights_list = nn_weights,                                     af_string_list = af_string_list,                                     q_taylor_vector = q_taylor_vector,                                     forced_max_Q = 3,                                     my_max_norm = list(\"unconstrained\",1)) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"this-vignettes-goal","dir":"Articles","previous_headings":"","what":"This vignette’s goal","title":"02 - Constraining the weights in the NN","text":"goal vignette present impose needed constraints NN training nn2poly can give accurate approximations NN. Therefore, replication vignette(\"nn2poly-01-introduction\") steps, imposing constraints. Please refer introduction vignette understand nn2poly basics expand details omitted . initial setup , using kerasand tensorflow create train NN. Additionally, now use auxiliary package nn2poly.tools implements helper functions impose desired constraints NN. solve two simple examples, regression classification.","code":"library(nn2poly.tools) library(nn2poly) library(keras) library(tensorflow)  # For reproducibility set.seed(1) tensorflow::tf$random$set_seed(1)"},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"simulated-data-generation","dir":"Articles","previous_headings":"This vignette’s goal > Simple regression example","what":"Simulated data generation","title":"02 - Constraining the weights in the NN","text":"simulate polynomial data following polynomial: \\(4x_1 - 3 x_2x_3\\). Data needs scaled \\([-1,1]\\) interval.","code":"# Define the desired polynomial for the simulated data polynomial <- list() polynomial$labels <- list(c(1), c(2,3)) polynomial$values <- c(4,-3) # Define number of variables p and sample n p <- 3 n_sample <- 500  # Predictor variables X <- matrix(0,n_sample,p) for (i in 1:p){   X[,i] <- rnorm(n = n_sample,0,1) }  # Response variable + small error term Y <- as.vector(eval_poly(X,polynomial)) + stats::rnorm(n_sample, 0, 0.1)  # Store all as a data frame data <- as.data.frame(cbind(X, Y)) head(data) #>           V1          V2          V3         Y #> 1 -0.6264538  0.07730312  1.13496509 -2.684020 #> 2  0.1836433 -0.29686864  1.11193185  1.632335 #> 3 -0.8356286 -1.18324224 -0.87077763 -6.344179 #> 4  1.5952808  0.01129269  0.21073159  6.279883 #> 5  0.3295078  0.99160104  0.06939565  1.165488 #> 6 -0.8204684  1.59396745 -1.66264885  4.650553 # Data scaling maxs <- apply(data, 2, max) mins <- apply(data, 2, min) data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"original-neural-network-with-constraints","dir":"Articles","previous_headings":"This vignette’s goal","what":"Original neural network (with constraints)","title":"02 - Constraining the weights in the NN","text":"case, need use custom layers nn2poly.tools include constraints. constraints different usual constraints keras case restriction applied weight vector bias neuron time, usual keras constraints can employed either bias kernel weights. use L1 norm, checking vector weights + bias arriving neuron satisfy L1 norm equal less 1. created using function build_keras_model() requires: - input dimension p. - list activation functions layer af_string_list. - list number neurons layer h_neurons_vector. - list my_max_norm consisting two elements: type norm used (l1_norm case) maximum value (1 case). can define parameters well ones used keras: Build model custom constraints, compile fit model: can visualize training process:  can also visualize NN predictions vs original Y values.","code":"# keras hyperparameters my_loss <- \"mse\" my_metrics <- \"mse\" my_optimizer <- optimizer_adam() my_epochs <- 2000 my_validation_split <- 0.2 my_verbose <- 0  # Parameters: af_string_list <- list(\"tanh\", \"tanh\", \"linear\") h_neurons_vector <- c(50, 50, 1) # If the output is linear, the last value should be 1 my_max_norm <- list(\"l1_norm\",1) nn <- build_keras_model(p,     af_string_list,     h_neurons_vector,     my_max_norm)  # Compile the model compile(nn,                 loss = my_loss,                 optimizer = my_optimizer,                 metrics = my_metrics )  # Fit the model history <- fit(nn,                              train_x,                              train_y,                              verbose = my_verbose,                              epochs = my_epochs,                              validation_split = my_validation_split,                              batch_size = 50 )  nn #> Model: \"sequential_16\" #> __________________________________________________________________________________________________________________________ #>  Layer (type)                                         Output Shape                                     Param #             #> ========================================================================================================================== #>  layer__combined_l1_4 (Layer_Combined_L1)             (None, 50)                                       200                 #>  activation_4 (Activation)                            (None, 50)                                       0                   #>  layer__combined_l1_5 (Layer_Combined_L1)             (None, 50)                                       2550                #>  activation_5 (Activation)                            (None, 50)                                       0                   #>  dense_20 (Dense)                                     (None, 1)                                        51                  #> ========================================================================================================================== #> Total params: 2,801 #> Trainable params: 2,801 #> Non-trainable params: 0 #> __________________________________________________________________________________________________________________________ plot(history) # Obtain the predicted values with the NN to compare them prediction_NN <- predict(nn, test_x)  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN, y_axis =  test_y, xlab = \"NN prediction\", ylab = \"Original Y\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"using-nn2poly-to-obtain-the-polynomial","dir":"Articles","previous_headings":"This vignette’s goal > Original neural network (with constraints)","what":"Using nn2poly to obtain the polynomial","title":"02 - Constraining the weights in the NN","text":"NN trained (case constraints) extract reshape weights NN parameters explained vignette(\"nn2poly-01-introduction\"), difference keras weights custom layers slightly different behavior already include bias weights matrix: activation functions used can stored : finally order Taylor approximation going choose 8 hidden layer. (final polynomial order limited forced_max_Q=3) input desired shape, nn2poly method can applied:","code":"keras_weights <- keras::get_weights(nn)  n <- length(keras_weights)  nn_weights <- keras_weights[1:(n-2)]  # Add last layer as this one has the bias separated, it is not a custom layer nn_weights[[n-1]] <- rbind(keras_weights[[n]], keras_weights[[n-1]]) af_string_list <- list(\"tanh\",\"tanh\", \"linear\") q_taylor_vector <- c(8, 8,  1) final_poly <- nn2poly_algorithm(   weights_list = nn_weights,   af_string_list = af_string_list,   q_taylor_vector = q_taylor_vector,   store_coeffs = FALSE,   forced_max_Q = 3 )"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"This vignette’s goal > Original neural network (with constraints)","what":"Obtaining polynomial predictions","title":"02 - Constraining the weights in the NN","text":"","code":"# Obtain the predicted values for the test data with our polynomial prediction_poly <- as.vector(eval_poly(x = test_x, poly = final_poly))"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"This vignette’s goal > Original neural network (with constraints)","what":"Visualizing the results","title":"02 - Constraining the weights in the NN","text":"Diagonal plot. example, imposed weight constraints approximation better.  Representing activation potentials, can seen now centered around zero, nn2poly works best.","code":"plot_diagonal(x_axis =  prediction_NN, y_axis =  prediction_poly, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") plot_taylor_and_activation_potentials(data = train,                                     weights_list = nn_weights,                                     af_string_list = af_string_list,                                     q_taylor_vector = q_taylor_vector,                                     forced_max_Q = 3,                                     my_max_norm = my_max_norm) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"simple-classification-example","dir":"Articles","previous_headings":"This vignette’s goal","what":"Simple classification example","title":"02 - Constraining the weights in the NN","text":"example, instead regression problem show classification example, NN trained classify species iris dataset, nn2poly employed obtain polynomial species.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"data-preparation","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Data preparation","title":"02 - Constraining the weights in the NN","text":"load iris scale data:","code":"# Load the data data(iris)  # Change response to numeric. In this case, Species was already numeric, # but this step is needed if it is a factor variable. iris$Species <- as.numeric(iris$Species)  # Define dimension p (number of predictor variables) p <- dim(iris)[2] - 1  # Define objective classes n_class <- max(iris[,(p+1)])  # Move objective classes from (1:3) to (0:2), needed for tensorflow iris[,(p+1)] <- iris[,(p+1)] - 1 # Scale the data in the [-1,1] interval and separate train and test # Only the predictor variables are scaled, not the response as those will be # the different classes. iris_x <- iris[,-(p+1)] maxs <- apply(iris_x, 2, max) mins <- apply(iris_x, 2, min) data_x_scaled <- as.data.frame(scale(iris_x, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2)) data <- cbind(data_x_scaled, iris[,(p+1)])  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"original-neural-network","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Original neural network","title":"02 - Constraining the weights in the NN","text":"can now train NN, following procedure regression problem using build_keras_model function includes layers custom constrains. First define parameters keras hyperparameters: Build model custom constraints, compile fit model: can visualize training process:  case, asses NN accuracy transform nn output probability: predict results test data: can use confusion matrix visualize results, can see NN correctly predicts classes observation:","code":"# keras hyperparameters my_loss <- loss_sparse_categorical_crossentropy(from_logits = TRUE) my_metrics <- \"accuracy\" my_optimizer <- optimizer_adam() my_epochs <- 500 my_validation_split <- 0.2 my_verbose <- 0  # Parameters: af_string_list <- list(\"tanh\", \"tanh\", \"linear\") h_neurons_vector <- c(50, 50, n_class) my_max_norm <- list(\"l1_norm\",1) nn <- build_keras_model(p,     af_string_list,     h_neurons_vector,     my_max_norm)  # Compile the model compile(nn,                 loss = my_loss,                 optimizer = my_optimizer,                 metrics = my_metrics )  # Fit the model history <- fit(nn,                              train_x,                              train_y,                              verbose = my_verbose,                              epochs = my_epochs,                              validation_split = my_validation_split,                              batch_size = 50 )  nn #> Model: \"sequential_17\" #> __________________________________________________________________________________________________________________________ #>  Layer (type)                                         Output Shape                                     Param #             #> ========================================================================================================================== #>  layer__combined_l1_6 (Layer_Combined_L1)             (None, 50)                                       250                 #>  activation_6 (Activation)                            (None, 50)                                       0                   #>  layer__combined_l1_7 (Layer_Combined_L1)             (None, 50)                                       2550                #>  activation_7 (Activation)                            (None, 50)                                       0                   #>  dense_21 (Dense)                                     (None, 3)                                        153                 #> ========================================================================================================================== #> Total params: 2,953 #> Trainable params: 2,953 #> Non-trainable params: 0 #> __________________________________________________________________________________________________________________________ plot(history) probability_model <- keras_model_sequential() %>%   nn() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax) # Obtain the predicted classes with the NN to compare them prediction_NN_class <- predict(probability_model, test_x)  # Also, the linear output can be predicted before the probability model prediction_NN <- predict(nn, test_x) # Create a confusion matrix cm <- caret::confusionMatrix(as.factor(prediction_NN_class), as.factor(test_y)) cm #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 15  0  0 #>          1  0 11  0 #>          2  0  0 12 #>  #> Overall Statistics #>                                       #>                Accuracy : 1           #>                  95% CI : (0.9075, 1) #>     No Information Rate : 0.3947      #>     P-Value [Acc > NIR] : 4.568e-16   #>                                       #>                   Kappa : 1           #>                                       #>  Mcnemar's Test P-Value : NA          #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   1.0000 #> Specificity            1.0000   1.0000   1.0000 #> Pos Pred Value         1.0000   1.0000   1.0000 #> Neg Pred Value         1.0000   1.0000   1.0000 #> Prevalence             0.3947   0.2895   0.3158 #> Detection Rate         0.3947   0.2895   0.3158 #> Detection Prevalence   0.3947   0.2895   0.3158 #> Balanced Accuracy      1.0000   1.0000   1.0000"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"using-nn2poly-to-obtain-the-polynomial-1","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Using nn2poly to obtain the polynomial","title":"02 - Constraining the weights in the NN","text":"NN trained, need extract reshape parameters explained regression case: activation functions used can stored : finally order Taylor approximation going choose 8 hidden layer. (final polynomial order limited forced_max_Q=3) input desired shape, nn2poly method can applied:","code":"keras_weights <- keras::get_weights(nn)  n <- length(keras_weights)  nn_weights <- keras_weights[1:(n-2)]  # Add last layer as this one has the bias separated, it is not a custom layer nn_weights[[n-1]] <- rbind(keras_weights[[n]], keras_weights[[n-1]]) af_string_list <- list(\"tanh\",\"tanh\", \"linear\") q_taylor_vector <- c(8, 8,  1) final_poly <- nn2poly_algorithm(   weights_list = nn_weights,   af_string_list = af_string_list,   q_taylor_vector = q_taylor_vector,   store_coeffs = FALSE,   forced_max_Q = 3 )"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"obtaining-polynomial-predictions-1","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Obtaining polynomial predictions","title":"02 - Constraining the weights in the NN","text":"","code":"# Obtain the predicted values for the test data with our Polynomial Regression prediction_poly_matrix <- eval_poly(x = test_x, poly = final_poly)  # Define probability model with keras fro the polynomial outputs probability_poly <- keras_model_sequential() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax)  # Class prediction with the polynomial outputs (one row for each polynomial) prediction_poly_class <- predict(probability_poly,t(prediction_poly_matrix))"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-constraints.html","id":"visualising-the-results","dir":"Articles","previous_headings":"This vignette’s goal > Simple classification example","what":"Visualising the results","title":"02 - Constraining the weights in the NN","text":"can compare, polynomial, output linear output NN. case, constraints used training, results much closer happened vignette(\"nn2poly-01-introduction\")  Finally, activation potentials can visualized follows, can seen constraints made activations closer zero unrestricted case. However, still dispersion creates slight deviations seen diagonal plots. However, ","code":"# Confussion matrix between NN class prediction and polynomial class prediction cm <- caret::confusionMatrix(as.factor(prediction_NN_class), as.factor(prediction_poly_class)) cm #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 15  0  0 #>          1  0 11  0 #>          2  0  0 12 #>  #> Overall Statistics #>                                       #>                Accuracy : 1           #>                  95% CI : (0.9075, 1) #>     No Information Rate : 0.3947      #>     P-Value [Acc > NIR] : 4.568e-16   #>                                       #>                   Kappa : 1           #>                                       #>  Mcnemar's Test P-Value : NA          #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   1.0000 #> Specificity            1.0000   1.0000   1.0000 #> Pos Pred Value         1.0000   1.0000   1.0000 #> Neg Pred Value         1.0000   1.0000   1.0000 #> Prevalence             0.3947   0.2895   0.3158 #> Detection Rate         0.3947   0.2895   0.3158 #> Detection Prevalence   0.3947   0.2895   0.3158 #> Balanced Accuracy      1.0000   1.0000   1.0000 for (i in 1:3){   print(     plot_diagonal(x_axis =  prediction_NN[,i],                   y_axis =  prediction_poly_matrix[i,],                   xlab = \"NN prediction\",                   ylab = \"Polynomial prediction\")         ) } plot_taylor_and_activation_potentials(data = train,                                     weights_list = nn_weights,                                     af_string_list = af_string_list,                                     q_taylor_vector = q_taylor_vector,                                     forced_max_Q = 3,                                     my_max_norm = my_max_norm) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Pablo Morala. Author, maintainer. Iñaki Ucar. Author.","code":""},{"path":"https://ibidat.github.io/nn2poly/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Morala P, Ucar (2023). nn2poly: Neural Network Weights Transformation Polynomial Coefficients. R package version 0.0.0.9000, https://ibidat.github.io/nn2poly/.","code":"@Manual{,   title = {nn2poly: Neural Network Weights Transformation into Polynomial Coefficients},   author = {Pablo Morala and Iñaki Ucar},   year = {2023},   note = {R package version 0.0.0.9000},   url = {https://ibidat.github.io/nn2poly/}, }"},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"nn2poly-transforming-neural-networks-into-polynomials","dir":"","previous_headings":"","what":"Neural Network Weights Transformation into Polynomial Coefficients","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"nn2poly package implements NN2Poly method allows transform already trained deep feed-forward fully connected neural network polynomial representation predicts similar possible original neural network.","code":""},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"related-papers","dir":"","previous_headings":"","what":"Related Papers:","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo Iñaki Ucar (2021). “Towards mathematical framework inform neural network modelling via polynomial regression.” Neural Networks (ISSN 0893-6080), vol. 142, 57-72. DOI: 10.1016/j.neunet.2021.04.036 Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo Iñaki Ucar (2021). “NN2Poly: polynomial representation deep feed-forward artificial neural networks”. Arxiv preprint: arXiv:2112.11397","code":""},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"installation GitHub requires remotes package.","code":"# install.packages(\"remotes\") remotes::install_github(paste(\"IBiDat\", c(\"nn2poly\", \"nn2poly.tools\"), sep=\"/\"))"},{"path":"https://ibidat.github.io/nn2poly/reference/change_string_to_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","title":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","text":"Auxiliar function used transform used activation functions neural network string name R function.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/change_string_to_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","text":"","code":"change_string_to_function(af_string_list)"},{"path":"https://ibidat.github.io/nn2poly/reference/change_string_to_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","text":"af_string_list List strings containing predefined possible activation functions, .e., \"softplus\", \"tanh\", \"sigmoid\" \"linear\".","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/change_string_to_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","text":"List R functions associated string names provided.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/check_weight_constraints.html","id":null,"dir":"Reference","previous_headings":"","what":"Check weight's vector constraints (including bias) — check_weight_constraints","title":"Check weight's vector constraints (including bias) — check_weight_constraints","text":"Check weight's vector constraints (including bias)","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/check_weight_constraints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check weight's vector constraints (including bias) — check_weight_constraints","text":"","code":"check_weight_constraints(weights, maxnorm)"},{"path":"https://ibidat.github.io/nn2poly/reference/check_weight_constraints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check weight's vector constraints (including bias) — check_weight_constraints","text":"weights List matrices weights layer. maxnorm List 2 elements: name used norm max value.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/check_weight_constraints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check weight's vector constraints (including bias) — check_weight_constraints","text":"List across layers vector containing norms weight vector.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/datasets.html","id":null,"dir":"Reference","previous_headings":"","what":"Datasets — datasets","title":"Datasets — datasets","text":"Datasets","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/datasets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Datasets — datasets","text":"","code":"nn2poly_example0  nn2poly_example1  nn2poly_example2  nn2poly_example3"},{"path":"https://ibidat.github.io/nn2poly/reference/datasets.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Datasets — datasets","text":"object class list length 10. object class list length 10. object class list length 10. object class list length 10.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"Evaluates one several polynomials one data points desired variables.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"","code":"eval_poly(x, poly)"},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"x Input data matrix, vector dataframe. number columns number variables polynomial (dimension p). Response variable predicted included. poly List containing 2 items: labels values. labels List integer vectors length (number rows) values, integer vector denotes combination variables associated coefficient value stored position values. Note variables numbered 1 p. values Matrix (also vector single polynomial), row represents polynomial, number columns length labels, containing column value coefficient given equivalent label position. Example: labels contains integer vector c(1,1,3) position 5, value stored values position 5 coefficient associated term x_1^2*x_3.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"Matrix containing evaluation polynomials. row corresponds polynomial used column observation, meaning row vector corresponds results evaluating given data polynomial.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"","code":"# Create the polynomial 1 + (-1)·x_1 + 1·x_2 + 0.5·(x_1)^2 as a list poly <- list() poly$values <- c(1,-1,1,0.5) poly$labels <- list(c(0),c(1),c(2),c(1,1)) # Create two observations, (x_1,x_2) = (1,2) and (x_1,x_2) = (3,1) x <- rbind(c(1,2), c(3,1)) # Evaluate the polynomial on both observations eval_poly(x,poly) #>      [,1] [,2] #> [1,]  2.5  3.5"},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly_algorithm.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","title":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","text":"Performs full NN2Poly algorithm obtains polynomial coefficients model performs closely given already trained neural network using weights Taylor approximation activation functions.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly_algorithm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","text":"","code":"nn2poly_algorithm(   weights_list,   af_string_list,   q_taylor_vector,   all_partitions,   store_coeffs = FALSE,   forced_max_Q )"},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly_algorithm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","text":"weights_list list length L ( number hidden layers + 1) containing weights matrix layer. expected shape matrices layer L form $(h_(l-1) + 1)*(h_l)$, , number rows number neurons previous layer plus bias vector, number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer. af_string_list list length L containing character strings names activation function used layer. q_taylor_vector vector length L containing degree (numeric) Taylor expansion performed layer. all_partitions Optional argument containing needed multipartitions list lists lists. missing, function computes first. step can computationally expensive encouraged multipartitions stored reused possible. store_coeffs Boolean determines polynomials computed internal layers stored given output (TRUE), last layer needed (FALSE). Default FALSE. forced_max_Q Optional argument: integer determines maximum order force final polynomial, discarding terms higher order naturally arise using orders q_taylor_vector.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly_algorithm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","text":"store_coeffs = FALSE (default case), returns list item named labels list integer vectors variables index associated polynomial term, item named values contains matrix row coefficients polynomial associated output neuron. store_coeffs = TRUE, returns list length L layer contains item explained . polynomials obtained hidden layers needed represent NN can used explore method works.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_derivatives_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Title — obtain_derivatives_list","title":"Title — obtain_derivatives_list","text":"Title","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_derivatives_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Title — obtain_derivatives_list","text":"","code":"obtain_derivatives_list(af_string_list, q_taylor_vector)"},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_derivatives_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Title — obtain_derivatives_list","text":"af_string_list List names activation function used layer string. Currently accepted values: \"softplus\", \"linear\", \"tanh\" \"sigmoid. q_taylor_vector List containing degree Taylor expansion performed layer.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_derivatives_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Title — obtain_derivatives_list","text":"list vectors derivatives","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_partitions_with_labels.html","id":null,"dir":"Reference","previous_headings":"","what":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","title":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","text":"functions generate partitions obtained Knuth's algorithm, compute labels store things list length 2.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_partitions_with_labels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","text":"","code":"obtain_partitions_with_labels(p, q_max)"},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_partitions_with_labels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","text":"p Number variables. q_max Maximum degree final polynomial.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_partitions_with_labels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","text":"List length 2 first element list labels second element list partitions.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_partitions_with_labels.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","text":"","code":"obtain_partitions_with_labels(2, 3) #> $labels #> $labels[[1]] #> [1] 1 #>  #> $labels[[2]] #> [1] 1 1 #>  #> $labels[[3]] #> [1] 1 2 #>  #> $labels[[4]] #> [1] 1 1 1 #>  #> $labels[[5]] #> [1] 1 1 2 #>  #>  #> $partitions #> $partitions[[1]] #> $partitions[[1]][[1]] #> $partitions[[1]][[1]][[1]] #> [1] 1 #>  #>  #>  #> $partitions[[2]] #> $partitions[[2]][[1]] #> $partitions[[2]][[1]][[1]] #> [1] 1 1 #>  #>  #> $partitions[[2]][[2]] #> $partitions[[2]][[2]][[1]] #> [1] 1 #>  #> $partitions[[2]][[2]][[2]] #> [1] 1 #>  #>  #>  #> $partitions[[3]] #> $partitions[[3]][[1]] #> $partitions[[3]][[1]][[1]] #> [1] 1 2 #>  #>  #> $partitions[[3]][[2]] #> $partitions[[3]][[2]][[1]] #> [1] 1 #>  #> $partitions[[3]][[2]][[2]] #> [1] 2 #>  #>  #>  #> $partitions[[4]] #> $partitions[[4]][[1]] #> $partitions[[4]][[1]][[1]] #> [1] 1 1 1 #>  #>  #> $partitions[[4]][[2]] #> $partitions[[4]][[2]][[1]] #> [1] 1 1 #>  #> $partitions[[4]][[2]][[2]] #> [1] 1 #>  #>  #> $partitions[[4]][[3]] #> $partitions[[4]][[3]][[1]] #> [1] 1 #>  #> $partitions[[4]][[3]][[2]] #> [1] 1 #>  #> $partitions[[4]][[3]][[3]] #> [1] 1 #>  #>  #>  #> $partitions[[5]] #> $partitions[[5]][[1]] #> $partitions[[5]][[1]][[1]] #> [1] 1 1 2 #>  #>  #> $partitions[[5]][[2]] #> $partitions[[5]][[2]][[1]] #> [1] 1 1 #>  #> $partitions[[5]][[2]][[2]] #> [1] 2 #>  #>  #> $partitions[[5]][[3]] #> $partitions[[5]][[3]][[1]] #> [1] 1 2 #>  #> $partitions[[5]][[3]][[2]] #> [1] 1 #>  #>  #> $partitions[[5]][[4]] #> $partitions[[5]][[4]][[1]] #> [1] 1 #>  #> $partitions[[5]][[4]][[2]] #> [1] 1 #>  #> $partitions[[5]][[4]][[3]] #> [1] 2 #>  #>  #>  #>"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a comparison between two sets of points. — plot_diagonal","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"points come predictions NN PM line (plot.line = TRUE) displayed, case method exhibit asymptotic behavior, points fall line.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"","code":"plot_diagonal(   x_axis,   y_axis,   xlab = NULL,   ylab = NULL,   title = NULL,   plot.line = TRUE )"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"x_axis Values plot x axis. y_axis Values plot y axis. xlab Lab x axis ylab Lab y axis. title Title plot. plot.line red line slope = 1 intercept = 0 plotted.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"Plot (ggplot object).","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"Function allows take NN data input values plot distribution data activation potentials (sum input values * weights) neurons together layer Taylor expansion used activation functions. layer 'linear' (usually output), layer approximation Taylor expansion needed.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"","code":"plot_taylor_and_activation_potentials(   data,   weights_list,   af_string_list,   q_taylor_vector,   forced_max_Q,   my_max_norm,   taylor_interval = 1.5 )"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"data Matrix data frame containing predictor variables (X) used input compute activation potentials. response variable column included. weights_list list length L ( number hidden layers + 1) containing weights matrix layer. expected shape matrices layer L form $(h_(l-1) + 1)*(h_l)$, , number rows number neurons previous layer plus bias vector, number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer. af_string_list list length L containing character strings names activation function used layer, used nn2poly_algorithm. q_taylor_vector vector length L containing degree (numeric) Taylor expansion performed layer, used nn2poly_algorithm. forced_max_Q Integer determines maximum order force final polynomial, discarding terms higher order naturally arise using orders q_taylor_vector, used nn2poly_algorithm. my_max_norm List containing type norm maximum value. See documentation constrain NN weights. taylor_interval optional parameter determining interval Taylor expansion represented. Default 1.5.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"list plots.","code":""}]
