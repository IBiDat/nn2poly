[{"path":"https://ibidat.github.io/nn2poly/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021-2023 Pablo Morala, Iñaki Úcar Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"overall-package-goal","dir":"Articles","previous_headings":"","what":"Overall package goal","title":"01 - Introduction to nn2poly","text":"main objective nn2poly obtain representation feed forward artificial neural network (like multilayered perceptron) terms polynomial representation. coefficients polynomials obtained applying first Taylor expansion activation function neural network. expansions given neural network weights joint using combinatorial properties, obtaining final value polynomial coefficients. information theoretical insights underlying mathematical process used build relationship can found following references: * Initial development idea single hidden layer neural network article free access arXiv preprint version. * Extension deeper layers proper formulation NN2Poly method arXiv preprint. Important remark 1: approximations made NN2poly method rely Taylor expansions therefore require constraints imposed training original neural network. implementation constraints depend deep learning framework used train neural networks. Currently, package supports constraints implementation keras/tensorflow, covered vignette(\"nn2poly-02-tensorflow-regression\") vignette(\"nn2poly-03-tensorflow-classification\"). Implementation pytorch networks development. However, nn2poly can work default kind neural network manually feeding neural network weights activation functions algorithm. Therefore, nn2poly limited special deep learning framework.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"this-vignettes-goal","dir":"Articles","previous_headings":"","what":"This vignette’s goal","title":"01 - Introduction to nn2poly","text":"present basic behavior nn2poly used default version, without specifying deep learning framework explained previous remark. matter, showcase example get weights trained neural network manually create object needed information use nn2poly. result polynomial tries approximate neural network behavior. case neural network training constraint imposed. , explained previously, final approximation polynomial may accurate enough. example focused default version, need build NN framework, use keras tensorflow matter. case, needed parameters extracted used default version nn2poly, can extrapolated framework.","code":"library(nn2poly) library(keras)  # This sets all needed seeds tensorflow::set_random_seed(42)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"simple-regression-example","dir":"Articles","previous_headings":"","what":"Simple regression example","title":"01 - Introduction to nn2poly","text":"example solve regression problem using simulated data polynomial, allows control final polynomial coefficients obtained nn2poly similar polynomial originates data.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"simulated-data-generation","dir":"Articles","previous_headings":"Simple regression example","what":"Simulated data generation","title":"01 - Introduction to nn2poly","text":"simulate polynomial data follows. First define polynomial using format needed nn2poly, specifically use function eval_poly, consists list containing: * Labels: list integer vectors denoting combinations variables appear term polynomial. Variables numbered 1 p p dimension problem. example, c(1,1,3) represent term \\(x_1^2x_3\\) * Values: Vector containing numerical values coefficients denoted labels. multiple polynomials terms different coefficients want represented, matrix can employed, row polynomial. create polynomial \\(4x_1 - 3 x_2x_3\\): said polynomial, can now generate desired data train NN example. employ normal distribution generate variables \\(x_1, x_2, x_3\\) also error term \\(\\epsilon\\). Therefore, response variable \\(y\\) generated : \\(y = 4x_1 - 3 x_2x_3 + \\epsilon\\) scale data everything \\([-1,1]\\) interval divide train test datasets.","code":"polynomial <- list() polynomial$labels <- list(c(1), c(2,3)) polynomial$values <- c(4,-3) # Define number of variables p and sample n p <- 3 n_sample <- 500  # Predictor variables X <- matrix(0,n_sample,p) for (i in 1:p){   X[,i] <- rnorm(n = n_sample,0,1) }  # Response variable + small error term Y <- as.vector(eval_poly(X,polynomial)) + stats::rnorm(n_sample, 0, 0.1)  # Store all as a data frame data <- as.data.frame(cbind(X, Y)) head(data) #>           V1           V2         V3          Y #> 1  1.3709584  1.029140719  2.3250585 -1.7547416 #> 2 -0.5646982  0.914774868  0.5241222 -3.7107357 #> 3  0.3631284 -0.002456267  0.9707334  1.3609395 #> 4  0.6328626  0.136009552  0.3769734  2.4608270 #> 5  0.4042683 -0.720153545 -0.9959334 -0.6141076 #> 6 -0.1061245 -0.198124330 -0.5974829 -0.7455793 # Data scaling maxs <- apply(data, 2, max) mins <- apply(data, 2, min) data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"original-neural-network","dir":"Articles","previous_headings":"Simple regression example","what":"Original neural network","title":"01 - Introduction to nn2poly","text":"simulated data ready, can train neural network. method expected applied given trained densely connected feed forward neural network (NN now ), also referred multilayer perceptron (MLP). Therefore, explained , method used train NN can used. use kerasto train , manually build needed object weights fed nn2poly algorithm trained framework. information specific keras methods implemented package, please refer vignette(\"nn2poly-02-tensorflow-regression\") vignette(\"nn2poly-02-tensorflow-classification\"). Note: , note , order avoid asymptotic behavior method, important impose kind constraints training neural network weights. Details depend chosen deep learning framework covered next vignettes. First, build model. Compile model: train : can visualize training process:  can also visualize NN predictions vs original Y values.  Note: Recall NN performance addressed nn2poly, meaning performance either good bad nn2poly still represent NN behavior.","code":"nn <- keras_model_sequential()  nn %>% layer_dense(units = 10,                   activation = \"tanh\",                   input_shape = p)  nn %>% layer_dense(units = 10,                   activation = \"tanh\")  nn %>% layer_dense(units = 1,                   activation = \"linear\")  nn #> Model: \"sequential_15\" #> ________________________________________________________________________________ #>  Layer (type)                       Output Shape                    Param #      #> ================================================================================ #>  dense_18 (Dense)                   (None, 10)                      40           #>  dense_19 (Dense)                   (None, 10)                      110          #>  dense_20 (Dense)                   (None, 1)                       11           #> ================================================================================ #> Total params: 161 #> Trainable params: 161 #> Non-trainable params: 0 #> ________________________________________________________________________________ compile(nn,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\") history <- fit(nn,                train_x,                train_y,                verbose = 0,                epochs = 300,                validation_split = 0.3 ) plot(history) # Obtain the predicted values with the NN to compare them prediction_NN <- predict(nn, test_x)  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN, y_axis =  test_y, xlab = \"NN prediction\", ylab = \"Original Y\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"using-nn2poly-to-obtain-the-polynomial","dir":"Articles","previous_headings":"Simple regression example","what":"Using nn2poly to obtain the polynomial","title":"01 - Introduction to nn2poly","text":"NN trained, using chosen method user, parameters extracted reshaped, needed, match expected input function nn2poly_algorithm(). input object formed list matrices weight matrix layer. weights matrices dimension ((1+input) * output) first row corresponds bias vector, rest rows correspond ordered vector weights associated input. list, name element activation function names layer. Currently supported activation functions \"tanh\", \"sigmoid\", \"softplus\", \"linear\". Particularly, keras framework default separates kernel weights matrices dimension (input * output) bias vectors (1 * output), need add bias first row matrix ((1+input) * output). Additionally, parameters affecting properties algorithm: q_taylor_vector: vector integers containing order Taylor expansion performed layer. output layer linear activation function, last value 1. forced_max_Q: (optional value) integer value denoting maximum order terms computed polynomial. Usually 2 3 enough practice. Note higher orders suppose explosion possible combinations. user provide value, polynomial order grows multiplicatively Taylor order hidden layer, therefore better start low values. input desired shape, nn2poly method can applied: can glimpse coefficients polynomial stored. Note structure explained polynomial generated data, list labels values. case, obtained polynomial order 3. Note output nn2poly class.","code":"keras_weights <- keras::get_weights(nn)  # Due to keras giving weights separated from the bias, we have twice the # elements that we want: n <- length(keras_weights)/2 nn_weights <- vector(mode = \"list\", length = n) for (i in 1:n){   nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]]) }  # The activation functions stored as strings: af_string_names <- c(\"tanh\",\"tanh\", \"linear\")  weights_object <- nn_weights names(weights_object) <- af_string_names  weights_object #> $tanh #>            [,1]       [,2]      [,3]        [,4]        [,5]       [,6] #> [1,]  0.2637966 -0.1632335 0.4649245 -0.20589803  0.04730094 -0.3763387 #> [2,]  0.2385524  0.1342811 0.2513933  0.48018309 -0.47788176  0.1631715 #> [3,] -0.7742428 -0.1097567 0.8174284  0.08418242  0.06163564  0.7477616 #> [4,] -0.6955228  0.5266028 0.5572206 -0.46307948  0.48515409 -0.7743985 #>            [,7]        [,8]        [,9]       [,10] #> [1,] -0.2124885 -0.08747434 -0.07051360 -0.37202930 #> [2,]  0.6607828 -0.31218216 -0.01426336 -0.01626668 #> [3,] -0.8533844 -0.48116657  0.04514860 -0.70131820 #> [4,]  0.1162070  0.35274017  0.04112899 -0.56946415 #>  #> $tanh #>              [,1]        [,2]        [,3]        [,4]        [,5]       [,6] #>  [1,] -0.06261473  0.01255261  0.42954829  0.06141516  0.08550595  0.2265693 #>  [2,]  0.61023968 -0.15460493  0.32420215  0.16633785  0.25600886 -0.2739864 #>  [3,] -0.25070983 -0.05766810 -0.50344008  0.13009298 -0.03173310 -0.4047118 #>  [4,]  0.40872586  0.46675542  0.11527456  0.43078154  0.15864335  0.6346628 #>  [5,] -0.24814591  0.01059229  0.40011716  0.27607894 -0.46360433 -0.3692199 #>  [6,] -0.26658243  0.51526904 -0.09131107  0.37373766  0.26082405  0.4299564 #>  [7,]  0.51186675  0.12881947  0.67951459 -0.17605777  0.15393135  0.4613117 #>  [8,]  0.40394908 -0.42659599 -0.98550946 -0.15839998  0.02221864  0.3011585 #>  [9,]  0.50324774  0.11122702 -0.84318024 -0.02784178  0.31848624  0.2643846 #> [10,]  0.27406499 -0.11159007  0.02285146  0.35646623  0.25075704 -0.1359648 #> [11,] -0.52268207  0.45537508 -0.56030786  0.49129054 -0.03669148 -0.7891714 #>              [,7]        [,8]        [,9]         [,10] #>  [1,]  0.08071607  0.16983804  0.08445328  0.0732324421 #>  [2,]  0.06122208 -0.04379337 -0.57018125 -0.3144004941 #>  [3,]  0.28472036  0.23372132 -0.47447795  0.0003889628 #>  [4,]  0.01778316 -0.29318872  0.07777876 -0.0302867368 #>  [5,] -0.53746992 -0.42425549 -0.48620722 -0.1922566146 #>  [6,] -0.41601449 -0.25669649 -0.28299403 -0.2540818155 #>  [7,] -0.22281407  0.40243641 -0.12623918 -0.4014825821 #>  [8,] -0.04859665 -0.38288346  0.25072047 -0.3935797811 #>  [9,]  0.48137835 -0.40863025  0.01616014 -0.5411312580 #> [10,] -0.32286894  0.09449039 -0.06011314 -0.4011713266 #> [11,]  0.14410253 -0.21469988  0.24569559  0.3822859824 #>  #> $linear #>              [,1] #>  [1,] -0.07251097 #>  [2,]  0.95829737 #>  [3,] -0.07316440 #>  [4,] -0.35980642 #>  [5,] -0.43944517 #>  [6,] -0.07012334 #>  [7,]  0.60509491 #>  [8,] -0.66404337 #>  [9,] -0.04713812 #> [10,] -0.79189116 #> [11,] -0.91330308 q_taylor_vector <- c(8, 8,  1) final_poly <- nn2poly(object = weights_object,                       q_taylor_vector = q_taylor_vector,                       forced_max_Q = 3) final_poly #> $labels #> $labels[[1]] #> [1] 0 #>  #> $labels[[2]] #> [1] 1 #>  #> $labels[[3]] #> [1] 2 #>  #> $labels[[4]] #> [1] 3 #>  #> $labels[[5]] #> [1] 1 1 #>  #> $labels[[6]] #> [1] 1 2 #>  #> $labels[[7]] #> [1] 1 3 #>  #> $labels[[8]] #> [1] 2 2 #>  #> $labels[[9]] #> [1] 2 3 #>  #> $labels[[10]] #> [1] 3 3 #>  #> $labels[[11]] #> [1] 1 1 1 #>  #> $labels[[12]] #> [1] 1 1 2 #>  #> $labels[[13]] #> [1] 1 1 3 #>  #> $labels[[14]] #> [1] 1 2 2 #>  #> $labels[[15]] #> [1] 1 2 3 #>  #> $labels[[16]] #> [1] 1 3 3 #>  #> $labels[[17]] #> [1] 2 2 2 #>  #> $labels[[18]] #> [1] 2 2 3 #>  #> $labels[[19]] #> [1] 2 3 3 #>  #> $labels[[20]] #> [1] 3 3 3 #>  #>  #> $values #>            [,1]      [,2]       [,3]        [,4]       [,5]       [,6] #> [1,] -0.1617715 0.7475098 0.09908831 -0.08050417 0.07277049 0.03545956 #>             [,7]      [,8]      [,9]      [,10]       [,11]     [,12] #> [1,] 0.009334022 0.3035036 -2.559723 0.02124509 -0.08924709 0.2939184 #>           [,13]     [,14]       [,15]      [,16]     [,17]     [,18]      [,19] #> [1,] 0.01080101 0.6197476 -0.04956815 0.07802355 0.5294107 -1.012448 -0.7079898 #>            [,20] #> [1,] -0.05140071 #>  #> attr(,\"class\") #> [1] \"nn2poly\""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"Simple regression example","what":"Obtaining polynomial predictions","title":"01 - Introduction to nn2poly","text":"obtaining polynomial coefficients, can use predict response variable \\(Y\\). done employing function predciton nn2poly object providing new data predict.","code":"# Obtain the predicted values for the test data with our polynomial prediction_poly <- predict(object = final_poly,                            newdata = test_x)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Simple regression example","what":"Visualizing the results","title":"01 - Introduction to nn2poly","text":"advisable always check predictions obtained new polynomial differ much original neural network predictions (case differ, can also try find checking Taylor expansions). help , couple functions included allow us plot results. simple plot comparing polynomial NN predictions can obtained plot_diagonal(), red diagonal line represents perfect relationship NN polynomial predictions obtained. example, theoretical weights constraints imposed, can observe approximation perfect.  can also plot \\(n\\) important coefficients absolute value compare variables interactions relevant polynomial. Note , data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant. case can see coefficients differ original polynomial \\(4x_1 - 3 x_2x_3\\), constraints neural network weights training.  Another convenient plot show algorithm affected layer can obtained plot_taylor_and_activation_potentials(), activation potentials neuron computed presented Taylor expansion approximation activation function layer. case, used constraints NN training, activation potentials strictly centered around zero.","code":"plot_diagonal(x_axis =  prediction_NN, y_axis =  prediction_poly, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") plot_n_important_coeffs(final_poly, 8) plot_taylor_and_activation_potentials(object = nn,                                       data = train,                                     q_taylor_vector = q_taylor_vector,                                     forced_max_Q = 3,                                     my_max_norm = list(\"unconstrained\",1)) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"this-vignettes-goal","dir":"Articles","previous_headings":"","what":"This vignette’s goal","title":"02 - Regression example using tensorflow","text":"showing use nn2poly default version vignette(\"nn2poly-01-introduction\"), present use specific methods related keras tensorflow allow easier smoother use nn2poly deep learning framework. Furthermore, sow impose needed weight constraints tensorflow training accurate results compare results unconstrained neural network. vignette focus simple regression example classification one covered vignette(\"nn2poly-03-tensorflow-classification\").","code":"library(nn2poly) library(keras)  # This sets all needed seeds tensorflow::set_random_seed(1)"},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"simulated-data-generation","dir":"Articles","previous_headings":"Simple regression example","what":"Simulated data generation","title":"02 - Regression example using tensorflow","text":"simulate polynomial data following polynomial: \\(4x_1 - 3 x_2x_3\\). Data needs scaled \\([-1,1]\\) interval.","code":"# Define the desired polynomial for the simulated data polynomial <- list() polynomial$labels <- list(c(1), c(2,3)) polynomial$values <- c(4,-3) # Define number of variables p and sample n p <- 3 n_sample <- 500  # Predictor variables X <- matrix(0,n_sample,p) for (i in 1:p){   X[,i] <- rnorm(n = n_sample,0,1) }  # Response variable + small error term Y <- as.vector(eval_poly(X,polynomial)) + stats::rnorm(n_sample, 0, 0.1)  # Store all as a data frame data <- as.data.frame(cbind(X, Y)) head(data) #>           V1          V2          V3         Y #> 1 -0.6264538  0.07730312  1.13496509 -2.684020 #> 2  0.1836433 -0.29686864  1.11193185  1.632335 #> 3 -0.8356286 -1.18324224 -0.87077763 -6.344179 #> 4  1.5952808  0.01129269  0.21073159  6.279883 #> 5  0.3295078  0.99160104  0.06939565  1.165488 #> 6 -0.8204684  1.59396745 -1.66264885  4.650553 # Data scaling maxs <- apply(data, 2, max) mins <- apply(data, 2, min) data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])  plot(data)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"original-neural-networks","dir":"Articles","previous_headings":"Simple regression example","what":"Original neural networks","title":"02 - Regression example using tensorflow","text":"build train two different neural networks (NNs), one unconstrained weights (nn1) another one imposing constraint weights (nn2). Different constraints can tested, suggested constraint based theoretical empirical evaluation use L1 norm equal 1, constraining vector weights + bias arriving neuron satisfy L1 norm equal less 1.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"build-nn-1-unconstrained","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Build NN 1, unconstrained","title":"02 - Regression example using tensorflow","text":"NN built using standard tensorflow keras practices, case sequential keras model without constraint weights.","code":"nn1 <- keras_model_sequential()  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\",                   input_shape = p)  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\")  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\")  nn1 %>% layer_dense(units = 1,                   activation = \"linear\")  nn1 #> Model: \"sequential_16\" #> ________________________________________________________________________________ #>  Layer (type)                       Output Shape                    Param #      #> ================================================================================ #>  dense_21 (Dense)                   (None, 100)                     400          #>  dense_22 (Dense)                   (None, 100)                     10100        #>  dense_23 (Dense)                   (None, 100)                     10100        #>  dense_24 (Dense)                   (None, 1)                       101          #> ================================================================================ #> Total params: 20,701 #> Trainable params: 20,701 #> Non-trainable params: 0 #> ________________________________________________________________________________"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"build-nn-2-constrained","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Build NN 2, constrained","title":"02 - Regression example using tensorflow","text":"order implement desired constraints, provide add_constraints() function, takes structure given NN (feed forward dense NN) modifies layers include constraints. needed default constraints implemented keras support impose constraint time weights bias combined custom layer. implementation bias neuron included weights vector incident neuron, meaning previous layer \\(h\\) neurons, considered weight vector including bias given neuron dimension \\(h+1\\), bias first element. Currently, L1 norm L2 norm equal 1 implemented options. Note L1 norm equal 1 scaling input data \\([-1,1]\\) interval recommended option. Note parameters structure , layer type modified.","code":"nn2 <- add_constraints(nn1, constraint_type = \"l1_norm\") nn2 #> Model: \"sequential_17\" #> ________________________________________________________________________________ #>  Layer (type)                       Output Shape                    Param #      #> ================================================================================ #>  layer__combined_l1_12 (Layer_Combi  (None, 100)                    400          #>  ned_L1)                                                                         #>  layer__combined_l1_13 (Layer_Combi  (None, 100)                    10100        #>  ned_L1)                                                                         #>  layer__combined_l1_14 (Layer_Combi  (None, 100)                    10100        #>  ned_L1)                                                                         #>  dense_25 (Dense)                   (None, 1)                       101          #> ================================================================================ #> Total params: 20,701 #> Trainable params: 20,701 #> Non-trainable params: 0 #> ________________________________________________________________________________"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"compile-and-train-both-nns","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Compile and train both NNs","title":"02 - Regression example using tensorflow","text":"building NNs, compile train . Note , constraining weights trade-learning speed NN, nn2 needs higher number epochs properly learn data. Compile train nn1 model, visualize training history:  Compile train nn2 model, visualize training history:","code":"compile(nn1,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\")  history1 <- fit(nn1,                train_x,                train_y,                verbose = 0,                epochs = 300,                batch_size = 50,                validation_split = 0.2 )  plot(history1) compile(nn2,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\")  history2 <- fit(nn2,                train_x,                train_y,                verbose = 0,                epochs = 2000,                batch_size = 50,                validation_split = 0.2 )  plot(history2)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"visualize-both-nn-predictions","dir":"Articles","previous_headings":"Simple regression example > Original neural networks","what":"Visualize both NN predictions","title":"02 - Regression example using tensorflow","text":"can visualize NN predictions vs original Y values neural networks observe provide accurate predictions (values fall near “perfect” diagonal red line).","code":"# Obtain the predicted values with the NN to compare them prediction_NN1 <- predict(nn1, test_x)  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN1, y_axis =  test_y, xlab = \"NN 1 prediction\", ylab = \"Original Y\") # Obtain the predicted values with the NN to compare them prediction_NN2 <- predict(nn2, test_x)  # Diagonal plot implemented in the package to quickly visualize and compare predictions plot_diagonal(x_axis =  prediction_NN2, y_axis =  test_y, xlab = \"NN 2 prediction\", ylab = \"Original Y\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"using-nn2poly-to-obtain-the-polynomial","dir":"Articles","previous_headings":"Simple regression example","what":"Using nn2poly to obtain the polynomial","title":"02 - Regression example using tensorflow","text":"NNs trained, can directly call nn2poly keras model. Therefore, need build object weights activation functions default case covered vignette(\"nn2poly-01-introduction\"), can benefit generic methods implemented keras models. parameters added nn2poly work Taylor order expansion layer (q_taylor_vector), choose 8 default non linear layers 1 last linear layer Taylor used . (final polynomial order limited forced_max_Q=3) neural networks compare results:","code":"q_taylor_vector <- c(8, 8, 8, 1)  # Polynomial for nn1 final_poly1 <- nn2poly(object = nn1,                       q_taylor_vector = q_taylor_vector,                       forced_max_Q = 3)  # Polynomial for nn2 final_poly2 <- nn2poly(object = nn2,                       q_taylor_vector = q_taylor_vector,                       forced_max_Q = 3)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"Simple regression example","what":"Obtaining polynomial predictions","title":"02 - Regression example using tensorflow","text":"","code":"# Obtain the predicted values for the test data with our two polynomials  prediction_poly1 <- eval_poly(x = test_x, poly = final_poly1)  prediction_poly2 <- eval_poly(x = test_x, poly = final_poly2)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-tensorflow-regression.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Simple regression example","what":"Visualizing the results","title":"02 - Regression example using tensorflow","text":"polynomial predictions, can plot using diagonal plot compare respective NN predictions. Please note compare predictions polynomial NN predictions original data, nn2poly’s goal faithfully represent NN behavior independently well NN predicts. can observe clearly polynomial obtained constrained network (nn2) predicting almost , unconstrained network significant errors.   can also plot \\(n\\) important coefficients absolute value compare variables interactions relevant polynomial. Note , data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant. Recall original polynomial \\(4x_1 - 3x_2x_3\\). observe polynomial nn2, precisely interaction 2,3 high negative coefficient variable 1 positive one rest variables intercept (0) quite close zero. However, polynomial nn1, obtained coefficients correct Taylor expansion failing high weights.   Finally, problem Taylor expansion can checked following plot, layer represented activation function, Taylor expansion, error also density activation potentials activation functions receives layer. can clearly seen activation potentials density, green, expands wide range unconstrained NN kept closer zero constrained one, thus accurate Taylor expansion around zero.","code":"plot_diagonal(x_axis =  prediction_NN1, y_axis =  prediction_poly1, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for NN1\") plot_diagonal(x_axis =  prediction_NN2, y_axis =  prediction_poly2, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for NN2\") plot_n_important_coeffs(final_poly1, n_important_coeffs = 8) plot_n_important_coeffs(final_poly2, n_important_coeffs = 8) #Temporary parameter, should be removed or detected from object my_max_norm1 <- list(\"no_constraints\",1) my_max_norm2 <- list(\"l1_norm\",1)  plot_taylor_and_activation_potentials(object = nn1,                                       data = train,                                       q_taylor_vector = q_taylor_vector,                                       forced_max_Q = 3,                                       my_max_norm = my_max_norm1) #> [[1]] #>  #> [[2]] #>  #> [[3]] #>  #> [[4]] plot_taylor_and_activation_potentials(object = nn2,                                       data = train,                                       q_taylor_vector = q_taylor_vector,                                       forced_max_Q = 3,                                       my_max_norm = my_max_norm2) #> [[1]] #>  #> [[2]] #>  #> [[3]] #>  #> [[4]]"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"this-vignettes-goal","dir":"Articles","previous_headings":"","what":"This vignette’s goal","title":"03 - Classification example using tensorflow","text":"showing use nn2poly default version vignette(\"nn2poly-01-introduction\"), present use specific methods related keras tensorflow allow easier smoother use nn2poly deep learning framework. Furthermore, sow impose needed weight constraints tensorflow training accurate results compare results unconstrained neural network. vignette focus simple classification example using iris dataset. regression one covered vignette(\"nn2poly-02-tensorflow-regression\").","code":"library(nn2poly) library(keras)  # This sets all needed seeds tensorflow::set_random_seed(1)"},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"data-preparation","dir":"Articles","previous_headings":"Simple classification example","what":"Data preparation","title":"03 - Classification example using tensorflow","text":"First load iris dataset scale data \\([-1,1]\\) interval:","code":"# Load the data data(iris)  # Change response to numeric. In this case, Species was already numeric, # but this step is needed if it is a factor variable. iris$Species <- as.numeric(iris$Species)  # Define dimension p (number of predictor variables) p <- dim(iris)[2] - 1  # Define objective classes n_class <- max(iris[,(p+1)])  # Move objective classes from (1:3) to (0:2), needed for tensorflow iris[,(p+1)] <- iris[,(p+1)] - 1 # Scale the data in the [-1,1] interval and separate train and test # Only the predictor variables are scaled, not the response as those will be # the different classes. iris_x <- iris[,-(p+1)] maxs <- apply(iris_x, 2, max) mins <- apply(iris_x, 2, min) data_x_scaled <- as.data.frame(scale(iris_x, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2)) data <- cbind(data_x_scaled, iris[,(p+1)])  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"original-neural-networks","dir":"Articles","previous_headings":"Simple classification example","what":"Original neural networks","title":"03 - Classification example using tensorflow","text":"build train two different neural networks (NNs), one unconstrained weights (nn1) another one imposing constraint weights (nn2). Different constraints can tested, suggested constraint based theoretical empirical evaluation use L1 norm equal 1, constraining vector weights + bias arriving neuron satisfy L1 norm equal less 1.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"build-nn-1-unconstrained","dir":"Articles","previous_headings":"Simple classification example > Original neural networks","what":"Build NN 1, unconstrained","title":"03 - Classification example using tensorflow","text":"First, build model. Note case NN linear output number neurons number classes predict (3 species, n_class). , linear output transformed probability find probable class step done training. Therefore, nn2poly used obtain polynomial approximates nn linear outputs results also transformed probabilities predict highest probability class. NN built using standard tensorflow keras practices, case sequential keras model without constraint weights.","code":"nn1 <- keras_model_sequential()  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\",                   input_shape = p)  nn1 %>% layer_dense(units = 100,                   activation = \"tanh\")  nn1 %>% layer_dense(units = n_class)  nn1 #> Model: \"sequential_18\" #> ________________________________________________________________________________ #>  Layer (type)                       Output Shape                    Param #      #> ================================================================================ #>  dense_26 (Dense)                   (None, 100)                     500          #>  dense_27 (Dense)                   (None, 100)                     10100        #>  dense_28 (Dense)                   (None, 3)                       303          #> ================================================================================ #> Total params: 10,903 #> Trainable params: 10,903 #> Non-trainable params: 0 #> ________________________________________________________________________________"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"build-nn-2-constrained","dir":"Articles","previous_headings":"Simple classification example > Original neural networks","what":"Build NN 2, constrained","title":"03 - Classification example using tensorflow","text":"order implement desired constraints, provide add_constraints() function, takes structure given NN (feed forward dense NN) modifies layers include constraints. needed default constraints implemented keras support impose constraint time weights bias combined custom layer. implementation bias neuron included weights vector incident neuron, meaning previous layer \\(h\\) neurons, considered weight vector including bias given neuron dimension \\(h+1\\), bias first element. Currently, L1 norm L2 norm equal 1 implemented options. Note L1 norm equal 1 scaling input data \\([-1,1]\\) interval recommended option. Note parameters structure , layer type modified.","code":"nn2 <- add_constraints(nn1, constraint_type = \"l1_norm\") nn2 #> Model: \"sequential_19\" #> ________________________________________________________________________________ #>  Layer (type)                       Output Shape                    Param #      #> ================================================================================ #>  layer__combined_l1_15 (Layer_Combi  (None, 100)                    500          #>  ned_L1)                                                                         #>  layer__combined_l1_16 (Layer_Combi  (None, 100)                    10100        #>  ned_L1)                                                                         #>  dense_29 (Dense)                   (None, 3)                       303          #> ================================================================================ #> Total params: 10,903 #> Trainable params: 10,903 #> Non-trainable params: 0 #> ________________________________________________________________________________"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"compile-and-train-both-nns","dir":"Articles","previous_headings":"Simple classification example > Original neural networks","what":"Compile and train both NNs","title":"03 - Classification example using tensorflow","text":"building NNs, compile train . Note , constraining weights trade-learning speed NN, nn2 needs higher number epochs properly learn data. case, need define categorical crossentropy loss use accuracy chosen metric. Compile train nn1 model, visualize :  Compile train nn2 model, visualize :","code":"compile(nn1,         loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),         optimizer = optimizer_adam(),         metrics = \"accuracy\")  history1 <- fit(nn1,                train_x,                train_y,                verbose = 0,                epochs = 200,                validation_split = 0.3 )  plot(history1) compile(nn2,         loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),         optimizer = optimizer_adam(),         metrics = \"accuracy\")  history2 <- fit(nn2,                train_x,                train_y,                verbose = 0,                epochs = 300,                validation_split = 0.3 )  plot(history2)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"obtain-the-predicionts","dir":"Articles","previous_headings":"Simple classification example > Original neural networks","what":"Obtain the predicionts","title":"03 - Classification example using tensorflow","text":"case, asses NNs accuracy transform output probability: predict results test data neural network. (also predict store linear response NNs compared later polynomial output)","code":"probability_model1 <- keras_model_sequential() %>%   nn1() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax) probability_model2 <- keras_model_sequential() %>%   nn2() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax) # Obtain the predicted classes with the NN to compare them prediction_NN_class1 <- predict(probability_model1, test_x)  # Also, the linear output can be predicted before the probability model prediction_NN1 <- predict(nn1, test_x) # Obtain the predicted classes with the NN to compare them prediction_NN_class2 <- predict(probability_model2, test_x)  # Also, the linear output can be predicted before the probability model prediction_NN2 <- predict(nn2, test_x)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"visualize-the-nns-results","dir":"Articles","previous_headings":"","what":"Visualize the NNs results:","title":"03 - Classification example using tensorflow","text":"can use confusion matrix visualize results, can see NNs correctly predicts almost classes test data:","code":"# Create a confusion matrix cm1 <- caret::confusionMatrix(as.factor(prediction_NN_class1), as.factor(test_y)) cm1 #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 13  0  0 #>          1  0 14  2 #>          2  0  0  9 #>  #> Overall Statistics #>                                            #>                Accuracy : 0.9474           #>                  95% CI : (0.8225, 0.9936) #>     No Information Rate : 0.3684           #>     P-Value [Acc > NIR] : 7.078e-14        #>                                            #>                   Kappa : 0.9202           #>                                            #>  Mcnemar's Test P-Value : NA               #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   0.8182 #> Specificity            1.0000   0.9167   1.0000 #> Pos Pred Value         1.0000   0.8750   1.0000 #> Neg Pred Value         1.0000   1.0000   0.9310 #> Prevalence             0.3421   0.3684   0.2895 #> Detection Rate         0.3421   0.3684   0.2368 #> Detection Prevalence   0.3421   0.4211   0.2368 #> Balanced Accuracy      1.0000   0.9583   0.9091 # Create a confusion matrix cm2 <- caret::confusionMatrix(as.factor(prediction_NN_class2), as.factor(test_y)) cm2 #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 13  0  0 #>          1  0 14  3 #>          2  0  0  8 #>  #> Overall Statistics #>                                            #>                Accuracy : 0.9211           #>                  95% CI : (0.7862, 0.9834) #>     No Information Rate : 0.3684           #>     P-Value [Acc > NIR] : 1.482e-12        #>                                            #>                   Kappa : 0.8799           #>                                            #>  Mcnemar's Test P-Value : NA               #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   0.7273 #> Specificity            1.0000   0.8750   1.0000 #> Pos Pred Value         1.0000   0.8235   1.0000 #> Neg Pred Value         1.0000   1.0000   0.9000 #> Prevalence             0.3421   0.3684   0.2895 #> Detection Rate         0.3421   0.3684   0.2105 #> Detection Prevalence   0.3421   0.4474   0.2105 #> Balanced Accuracy      1.0000   0.9375   0.8636"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"using-nn2poly-to-obtain-the-polynomial","dir":"Articles","previous_headings":"Visualize the NNs results:","what":"Using nn2poly to obtain the polynomial","title":"03 - Classification example using tensorflow","text":"NNs trained, can directly call nn2poly keras model. Therefore, need build object weights activation functions default case covered vignette(\"nn2poly-01-introduction\"), can benefit generic methods implemented keras models. parameters added nn2poly work Taylor order expansion layer (q_taylor_vector), choose 8 default non linear layers 1 last linear layer Taylor used . (final polynomial order limited forced_max_Q=3) Note case, 3 output neurons, 3 output polynomials. polynomials stored way regression case, list labels values, case values matrix instead vector, row polynomial obtained output neuron. neural networks compare results:","code":"q_taylor_vector <- c(8, 8, 8, 1)  # Polynomial for nn1 final_poly1 <- nn2poly(object = nn1,                       q_taylor_vector = q_taylor_vector,                       forced_max_Q = 3)  # Polynomial for nn2 final_poly2 <- nn2poly(object = nn2,                       q_taylor_vector = q_taylor_vector,                       forced_max_Q = 3)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"Visualize the NNs results:","what":"Obtaining polynomial predictions","title":"03 - Classification example using tensorflow","text":"said , obtained polynomial represents neural network including softmax function computing class assigned observation. , need define keras sequential model includes class computation polynomial output. polynomial output obtained eval_poly(), case matrix form, 3 polynomials evaluated time:","code":"# Obtain the predicted values for the test data with our Polynomial Regression prediction_poly_matrix1 <- eval_poly(x = test_x, poly = final_poly1)  # Define probability model with keras fro the polynomial outputs probability_poly1 <- keras_model_sequential() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax)  # Class prediction with the polynomial outputs prediction_poly_class1 <- predict(probability_poly1,t(prediction_poly_matrix1)) # Obtain the predicted values for the test data with our Polynomial Regression prediction_poly_matrix2 <- eval_poly(x = test_x, poly = final_poly2)  # Define probability model with keras fro the polynomial outputs probability_poly2 <- keras_model_sequential() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax)  # Class prediction with the polynomial outputs prediction_poly_class2 <- predict(probability_poly2,t(prediction_poly_matrix2))"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-tensorflow-classification.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Visualize the NNs results:","what":"Visualizing the results","title":"03 - Classification example using tensorflow","text":"polynomial predictions, two options. can represent diagonal line linear outputs obtained directly polynomial NN predictions, compare assigned classes employing probability models. Please note compare predictions (linear classes) polynomials NN predictions original data, nn2poly’s goal faithfully represent NN behavior independently well NN predicts. First, let’s observe confusion matrix NNs: , can extract diagonal plot polynomials obtained NN, total \\(3\\times 2=6\\) diagonal plots.   can observe polynomials obtain quite similar predictions equivalent NN predictions, specially second polynomial 100% accuracy. However, comparing linear outputs, unconstrained NN presents problems constrained one quite accurate. can also plot \\(n\\) important coefficients absolute value compare variables interactions relevant polynomial. Note , data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant. case, 3 plots NN , one per polynomial output neuron. case, obtained coefficients represent important variables assigning probability class. can see coefficients share characteristics like positive negative nn1and nn2 interpretations, expected predictions differ much.   Finally, problem Taylor expansion can checked following plot, layer represented activation function, Taylor expansion, error also density activation potentials activation functions receives layer. can clearly seen activation potentials density, green, expands wide range unconstrained NN kept closer zero constrained one, thus accurate Taylor expansion around zero.","code":"# Confussion matrix between NN class prediction and polynomial class prediction cm_poly1 <- caret::confusionMatrix(as.factor(prediction_NN_class1), as.factor(prediction_poly_class1)) cm_poly1 #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 13  0  0 #>          1  0 14  2 #>          2  0  0  9 #>  #> Overall Statistics #>                                            #>                Accuracy : 0.9474           #>                  95% CI : (0.8225, 0.9936) #>     No Information Rate : 0.3684           #>     P-Value [Acc > NIR] : 7.078e-14        #>                                            #>                   Kappa : 0.9202           #>                                            #>  Mcnemar's Test P-Value : NA               #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   0.8182 #> Specificity            1.0000   0.9167   1.0000 #> Pos Pred Value         1.0000   0.8750   1.0000 #> Neg Pred Value         1.0000   1.0000   0.9310 #> Prevalence             0.3421   0.3684   0.2895 #> Detection Rate         0.3421   0.3684   0.2368 #> Detection Prevalence   0.3421   0.4211   0.2368 #> Balanced Accuracy      1.0000   0.9583   0.9091 # Confussion matrix between NN class prediction and polynomial class prediction cm_poly2 <- caret::confusionMatrix(as.factor(prediction_NN_class2), as.factor(prediction_poly_class2)) cm_poly2 #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 13  0  0 #>          1  0 17  0 #>          2  0  0  8 #>  #> Overall Statistics #>                                       #>                Accuracy : 1           #>                  95% CI : (0.9075, 1) #>     No Information Rate : 0.4474      #>     P-Value [Acc > NIR] : 5.312e-14   #>                                       #>                   Kappa : 1           #>                                       #>  Mcnemar's Test P-Value : NA          #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   1.0000 #> Specificity            1.0000   1.0000   1.0000 #> Pos Pred Value         1.0000   1.0000   1.0000 #> Neg Pred Value         1.0000   1.0000   1.0000 #> Prevalence             0.3421   0.4474   0.2105 #> Detection Rate         0.3421   0.4474   0.2105 #> Detection Prevalence   0.3421   0.4474   0.2105 #> Balanced Accuracy      1.0000   1.0000   1.0000 for (i in 1:3){   print(     plot_diagonal(x_axis =  prediction_NN1[,i],                   y_axis =  prediction_poly_matrix1[i,],                   xlab = \"NN prediction\",                   ylab = \"Polynomial prediction\")         ) } for (i in 1:3){   print(     plot_diagonal(x_axis =  prediction_NN2[,i],                   y_axis =  prediction_poly_matrix2[i,],                   xlab = \"NN prediction\",                   ylab = \"Polynomial prediction\")         ) } plot_n_important_coeffs(final_poly1, n_important_coeffs = 8) plot_n_important_coeffs(final_poly2, n_important_coeffs = 8) #Temporary parameter, should be removed or detected from object my_max_norm1 <- list(\"no_constraints\",1) my_max_norm2 <- list(\"l1_norm\",1)  plot_taylor_and_activation_potentials(object = nn1,                                       data = train,                                       q_taylor_vector = q_taylor_vector,                                       forced_max_Q = 3,                                       my_max_norm = my_max_norm1) #> [[1]] #>  #> [[2]] #>  #> [[3]] plot_taylor_and_activation_potentials(object = nn2,                                       data = train,                                       q_taylor_vector = q_taylor_vector,                                       forced_max_Q = 3,                                       my_max_norm = my_max_norm2) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Pablo Morala. Author, maintainer. Iñaki Ucar. Author. Jose Ignacio Diez. Contractor.","code":""},{"path":"https://ibidat.github.io/nn2poly/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Morala P, Ucar (2023). nn2poly: Neural Network Weights Transformation Polynomial Coefficients. R package version 0.0.0.9000, https://ibidat.github.io/nn2poly/.","code":"@Manual{,   title = {nn2poly: Neural Network Weights Transformation into Polynomial Coefficients},   author = {Pablo Morala and Iñaki Ucar},   year = {2023},   note = {R package version 0.0.0.9000},   url = {https://ibidat.github.io/nn2poly/}, }"},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"nn2poly-transforming-neural-networks-into-polynomials","dir":"","previous_headings":"","what":"Neural Network Weights Transformation into Polynomial Coefficients","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"nn2poly package implements NN2Poly method allows transform already trained deep feed-forward fully connected neural network polynomial representation predicts similar possible original neural network.","code":""},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"related-papers","dir":"","previous_headings":"","what":"Related Papers:","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo Iñaki Ucar (2021). “Towards mathematical framework inform neural network modelling via polynomial regression.” Neural Networks (ISSN 0893-6080), vol. 142, 57-72. DOI: 10.1016/j.neunet.2021.04.036 Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo Iñaki Ucar (2021). “NN2Poly: polynomial representation deep feed-forward artificial neural networks”. Arxiv preprint: arXiv:2112.11397","code":""},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"installation GitHub requires remotes package.","code":"# install.packages(\"remotes\") remotes::install_github(paste(\"IBiDat\", c(\"nn2poly\", \"nn2poly.tools\"), sep=\"/\"))"},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":null,"dir":"Reference","previous_headings":"","what":"Add custom constraints to the weights of a keras neural network. — add_constraints","title":"Add custom constraints to the weights of a keras neural network. — add_constraints","text":"function designed transform given keras NN (feed forward, dense), NN structure, parameters weights custom layers impose L1 L2 norm constraints","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add custom constraints to the weights of a keras neural network. — add_constraints","text":"","code":"add_constraints(model, constraint_type = \"l1_norm\", keep_old_weights = FALSE)"},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add custom constraints to the weights of a keras neural network. — add_constraints","text":"model keras model restrictions weights applied. constraint_type type constraint want apply model. Currently, 'l1_norm' 'l2_norm' can applied. keep_old_weights Binary parameters controls weights model kept new constrained network.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add custom constraints to the weights of a keras neural network. — add_constraints","text":"keras model custom constraints applied.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/change_string_to_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","title":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","text":"Auxiliar function used transform used activation functions neural network string name R function.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/change_string_to_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","text":"","code":"change_string_to_function(af_string_list)"},{"path":"https://ibidat.github.io/nn2poly/reference/change_string_to_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","text":"af_string_list List strings containing predefined possible activation functions, .e., \"softplus\", \"tanh\", \"sigmoid\" \"linear\".","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/change_string_to_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Auxiliar function used to transform some of the used activation functions\nin the neural network from their string name into an R function. — change_string_to_function","text":"List R functions associated string names provided.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/check_weight_constraints.html","id":null,"dir":"Reference","previous_headings":"","what":"Check weight's vector constraints (including bias) — check_weight_constraints","title":"Check weight's vector constraints (including bias) — check_weight_constraints","text":"Check weight's vector constraints (including bias)","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/check_weight_constraints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check weight's vector constraints (including bias) — check_weight_constraints","text":"","code":"check_weight_constraints(weights, maxnorm)"},{"path":"https://ibidat.github.io/nn2poly/reference/check_weight_constraints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check weight's vector constraints (including bias) — check_weight_constraints","text":"weights List matrices weights layer. maxnorm List 2 elements: name used norm max value.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/check_weight_constraints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check weight's vector constraints (including bias) — check_weight_constraints","text":"List across layers vector containing norms weight vector.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/constraint_l1_norm.html","id":null,"dir":"Reference","previous_headings":"","what":"Custom keras L1 constraint. — constraint_l1_norm","title":"Custom keras L1 constraint. — constraint_l1_norm","text":"function implements L1 constraint weights NN keras. follows similar structure already implemented L2 norm keras. Note currently supports maximum norm bound equal 1.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/constraint_l1_norm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Custom keras L1 constraint. — constraint_l1_norm","text":"","code":"constraint_l1_norm(w)"},{"path":"https://ibidat.github.io/nn2poly/reference/constraint_l1_norm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Custom keras L1 constraint. — constraint_l1_norm","text":"w weights passed constraint. usual keras layers kernel weights bias weights, also works custom layers implemented , bias weights joined together.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/constraint_l1_norm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Custom keras L1 constraint. — constraint_l1_norm","text":"Internal function, exported.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/datasets.html","id":null,"dir":"Reference","previous_headings":"","what":"Datasets — datasets","title":"Datasets — datasets","text":"Datasets","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/datasets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Datasets — datasets","text":"","code":"nn2poly_example0  nn2poly_example1  nn2poly_example2  nn2poly_example3"},{"path":"https://ibidat.github.io/nn2poly/reference/datasets.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Datasets — datasets","text":"object class list length 10. object class list length 10. object class list length 10. object class list length 10.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"Evaluates one several polynomials one data points desired variables.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"","code":"eval_poly(x, poly)"},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"x Input data matrix, vector dataframe. number columns number variables polynomial (dimension p). Response variable predicted included. poly List containing 2 items: labels values. labels List integer vectors length (number rows) values, integer vector denotes combination variables associated coefficient value stored position values. Note variables numbered 1 p. values Matrix (also vector single polynomial), row represents polynomial, number columns length labels, containing column value coefficient given equivalent label position. Example: labels contains integer vector c(1,1,3) position 5, value stored values position 5 coefficient associated term x_1^2*x_3.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"Vector containing evaluation single polynomial matrix containing evaluation polynomials. row corresponds polynomial used column observation, meaning row vector corresponds results evaluating given data polynomial.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluates one or several polynomials over one or more data points in the\ndesired variables. — eval_poly","text":"","code":"# Create the polynomial 1 + (-1)·x_1 + 1·x_2 + 0.5·(x_1)^2 as a list poly <- list() poly$values <- c(1,-1,1,0.5) poly$labels <- list(c(0),c(1),c(2),c(1,1)) # Create two observations, (x_1,x_2) = (1,2) and (x_1,x_2) = (3,1) x <- rbind(c(1,2), c(3,1)) # Evaluate the polynomial on both observations eval_poly(x,poly) #> [1] 2.5 3.5"},{"path":"https://ibidat.github.io/nn2poly/reference/get_model_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Auxiliary function. It obtains some parameters of a keras model. — get_model_parameters","title":"Auxiliary function. It obtains some parameters of a keras model. — get_model_parameters","text":"Auxiliary function. obtains parameters keras model.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/get_model_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Auxiliary function. It obtains some parameters of a keras model. — get_model_parameters","text":"","code":"get_model_parameters(model)"},{"path":"https://ibidat.github.io/nn2poly/reference/get_model_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Auxiliary function. It obtains some parameters of a keras model. — get_model_parameters","text":"model Keras nn model.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/get_model_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Auxiliary function. It obtains some parameters of a keras model. — get_model_parameters","text":"list length 3 following items: weights_list  list weights. af_string_list: list activation functions strings n_nurons: number neurons layer. p: dimension problem, .e., number predictor variables.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":null,"dir":"Reference","previous_headings":"","what":"nn2poly generic — nn2poly","title":"nn2poly generic — nn2poly","text":"nn2poly generic","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"nn2poly generic — nn2poly","text":"","code":"nn2poly(   object,   q_taylor_vector,   all_partitions = NULL,   store_coeffs = FALSE,   forced_max_Q = NULL,   ... )"},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"nn2poly generic — nn2poly","text":"object object computation nn2poly algorithm desired. default case, list length L ( number hidden layers + 1) containing weights matrix layer. name every element list name activation function used layer. expected shape matrices layer L form $(h_(l-1) + 1)*(h_l)$, , number rows number neurons previous layer plus bias vector, number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer. also keras.engine.training.Model model. q_taylor_vector vector length L containing degree (numeric) Taylor expansion performed layer. all_partitions Optional argument containing needed multipartitions list lists lists. NULL, function computes first. step can computationally expensive encouraged multipartitions stored reused possible. store_coeffs Boolean determines polynomials computed internal layers stored given output (TRUE), last layer needed (FALSE). forced_max_Q Optional argument: integer determines maximum order force final polynomial, discarding terms higher order naturally arise using orders q_taylor_vector. ... Ignored.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"nn2poly generic — nn2poly","text":"object class nn2poly. store_coeffs = FALSE (default case), returns list item named labels list integer vectors variables index associated polynomial term, item named values contains matrix row coefficients polynomial associated output neuron. store_coeffs = TRUE, returns list length L layer contains item explained . polynomials obtained hidden layers needed represent NN can used explore method works.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly_algorithm.html","id":null,"dir":"Reference","previous_headings":"","what":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","title":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","text":"Performs full NN2Poly algorithm obtains polynomial coefficients model performs closely given already trained neural network using weights Taylor approximation activation functions.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly_algorithm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","text":"","code":"nn2poly_algorithm(   weights_list,   af_string_list,   q_taylor_vector,   all_partitions = NULL,   store_coeffs = FALSE,   forced_max_Q = NULL )"},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly_algorithm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","text":"weights_list list length L ( number hidden layers + 1) containing weights matrix layer. expected shape matrices layer L form $(h_(l-1) + 1)*(h_l)$, , number rows number neurons previous layer plus one (bias vector added first row), number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer, 0 (bias) row 1, neuron h_l row h_l +1. af_string_list list length L containing character strings names activation function used layer. q_taylor_vector vector length L containing degree (numeric) Taylor expansion performed layer. all_partitions Optional argument containing needed multipartitions list lists lists. NULL, function computes first. step can computationally expensive encouraged multipartitions stored reused possible. store_coeffs Boolean determines polynomials computed internal layers stored given output (TRUE), last layer needed (FALSE). Default FALSE. forced_max_Q Optional argument: integer determines maximum order force final polynomial, discarding terms higher order naturally arise using orders q_taylor_vector.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly_algorithm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Computes one or several polynomials to represent a given neural network\nusing the NN2Poly algorithm. — nn2poly_algorithm","text":"store_coeffs = FALSE (default case), returns list item named labels list integer vectors variables index associated polynomial term, item named values contains matrix row coefficients polynomial associated output neuron. store_coeffs = TRUE, returns list length L layer contains item explained . polynomials obtained hidden layers needed represent NN can used explore method works.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_derivatives_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Title — obtain_derivatives_list","title":"Title — obtain_derivatives_list","text":"Title","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_derivatives_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Title — obtain_derivatives_list","text":"","code":"obtain_derivatives_list(af_string_list, q_taylor_vector)"},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_derivatives_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Title — obtain_derivatives_list","text":"af_string_list List names activation function used layer string. Currently accepted values: \"softplus\", \"linear\", \"tanh\" \"sigmoid. q_taylor_vector List containing degree Taylor expansion performed layer.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_derivatives_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Title — obtain_derivatives_list","text":"list vectors derivatives","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_partitions_with_labels.html","id":null,"dir":"Reference","previous_headings":"","what":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","title":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","text":"functions generate partitions obtained Knuth's algorithm, compute labels store things list length 2.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_partitions_with_labels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","text":"","code":"obtain_partitions_with_labels(p, q_max)"},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_partitions_with_labels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","text":"p Number variables. q_max Maximum degree final polynomial.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/obtain_partitions_with_labels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"This functions will generate the partitions obtained with Knuth's algorithm,\ncompute their labels and store both things in a list of length 2. — obtain_partitions_with_labels","text":"List length 2 first element list labels second element list partitions.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a comparison between two sets of points. — plot_diagonal","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"points come predictions NN PM line (plot.line = TRUE) displayed, case method exhibit asymptotic behavior, points fall line.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"","code":"plot_diagonal(   x_axis,   y_axis,   xlab = NULL,   ylab = NULL,   title = NULL,   plot.line = TRUE )"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"x_axis Values plot x axis. y_axis Values plot y axis. xlab Lab x axis ylab Lab y axis. title Title plot. plot.line red line slope = 1 intercept = 0 plotted.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"Plot (ggplot object).","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_n_important_coeffs.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot n most important coefficients. — plot_n_important_coeffs","title":"Plot n most important coefficients. — plot_n_important_coeffs","text":"function takes polynomial (several ones) given nn2poly_algorithm, plots absolute magnitude barplots able compare important coefficients. number plotted coefficients controlled n_important_coeffs.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_n_important_coeffs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot n most important coefficients. — plot_n_important_coeffs","text":"","code":"plot_n_important_coeffs(poly, n_important_coeffs)"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_n_important_coeffs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot n most important coefficients. — plot_n_important_coeffs","text":"poly polynomial represented list \"labels\" \"values\", manner returned nn2poly_algorithm. n_important_coeffs integer denoting number coefficients plotted, ordering absolute magnitude.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_n_important_coeffs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot n most important coefficients. — plot_n_important_coeffs","text":"plot showing n important coefficients.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"Function allows take NN data input values plot distribution data activation potentials (sum input values * weights) neurons together layer Taylor expansion used activation functions. layer 'linear' (usually output), layer approximation Taylor expansion needed.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"","code":"plot_taylor_and_activation_potentials(   object,   data,   q_taylor_vector,   forced_max_Q,   my_max_norm,   taylor_interval = 1.5,   ... )"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"object list length L ( number hidden layers + 1) containing weights matrix layer. expected shape matrices layer L form $(h_(l-1) + 1)*(h_l)$, , number rows number neurons previous layer plus bias vector, number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer. names object length L character strings names activation function used layer, used nn2poly_algorithm. data Matrix data frame containing predictor variables (X) used input compute activation potentials. response variable column included. q_taylor_vector vector length L containing degree (numeric) Taylor expansion performed layer, used nn2poly_algorithm. forced_max_Q Integer determines maximum order force final polynomial, discarding terms higher order naturally arise using orders q_taylor_vector, used nn2poly_algorithm. my_max_norm List containing type norm maximum value. See documentation constrain NN weights. taylor_interval optional parameter determining interval Taylor expansion represented. Default 1.5. ... Additional parameters.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"list plots.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials_aux.html","id":null,"dir":"Reference","previous_headings":"","what":"Auxiliary function to reduce the code of the S3 methods for the\nplot_taylor_and_activation_potentials function. — plot_taylor_and_activation_potentials_aux","title":"Auxiliary function to reduce the code of the S3 methods for the\nplot_taylor_and_activation_potentials function. — plot_taylor_and_activation_potentials_aux","text":"Auxiliary function reduce code S3 methods plot_taylor_and_activation_potentials function.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials_aux.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Auxiliary function to reduce the code of the S3 methods for the\nplot_taylor_and_activation_potentials function. — plot_taylor_and_activation_potentials_aux","text":"","code":"plot_taylor_and_activation_potentials_aux(   data,   weights_list,   af_string_list,   q_taylor_vector,   forced_max_Q,   my_max_norm,   taylor_interval = 1.5 )"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials_aux.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Auxiliary function to reduce the code of the S3 methods for the\nplot_taylor_and_activation_potentials function. — plot_taylor_and_activation_potentials_aux","text":"data Matrix data frame containing predictor variables (X) used input compute activation potentials. response variable column included. weights_list list length L ( number hidden layers + 1) containing weights matrix layer. expected shape matrices layer L form $(h_(l-1) + 1)*(h_l)$, , number rows number neurons previous layer plus bias vector, number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer. af_string_list list length L containing character strings names activation function used layer, used nn2poly_algorithm. q_taylor_vector vector length L containing degree (numeric) Taylor expansion performed layer, used nn2poly_algorithm. forced_max_Q Integer determines maximum order force final polynomial, discarding terms higher order naturally arise using orders q_taylor_vector, used nn2poly_algorithm. my_max_norm List containing type norm maximum value. See documentation constrain NN weights. taylor_interval optional parameter determining interval Taylor expansion represented. Default 1.5.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials_aux.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Auxiliary function to reduce the code of the S3 methods for the\nplot_taylor_and_activation_potentials function. — plot_taylor_and_activation_potentials_aux","text":"list plots.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 method for class 'nn2poly' — predict.nn2poly","title":"S3 method for class 'nn2poly' — predict.nn2poly","text":"S3 method class 'nn2poly'","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 method for class 'nn2poly' — predict.nn2poly","text":"","code":"# S3 method for nn2poly predict(object, newdata, ...)"},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 method for class 'nn2poly' — predict.nn2poly","text":"object object class inheriting 'nn2poly'. newdata Matrix predictions made. ... Additional arguments.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"S3 method for class 'nn2poly' — predict.nn2poly","text":"matrix containing predictions. one prediction row newdata.","code":""}]
