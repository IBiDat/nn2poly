[{"path":"https://ibidat.github.io/nn2poly/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021-2023 Pablo Morala, Iñaki Úcar Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"nn2poly-package-goal","dir":"Articles","previous_headings":"","what":"nn2poly package goal","title":"Introduction to nn2poly","text":"main objective nn2poly obtain representation feed forward artificial neural network (like multilayered perceptron) terms polynomial representation. coefficients polynomials obtained applying first Taylor expansion activation function neural network. , expansions given neural network weights joint using combinatorial properties, obtaining final value polynomial coefficients. main goal new representation obtain interpretable model, serving thus eXplainable Artificial Intelligence (XAI) tool overcome black box nature neural networks means interpreting effect obtained polynomial coefficients. information theoretical insights underlying mathematical process used build relationship can found following references: Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo, Iñaki Ucar (2021). “Towards mathematical framework inform neural network modelling via polynomial regression.” Neural Networks, 142, 57-72. doi: 10.1016/j.neunet.2021.04.036 Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo, Iñaki Ucar (2023). “NNN2Poly: Polynomial Representation Deep Feed-Forward Artificial Neural Networks.” IEEE Transactions Neural Networks Learning Systems, (Early Access). doi: 10.1109/TNNLS.2023.3330328 Important remark: approximations made NN2Poly rely Taylor expansions therefore require constraints imposed training original neural network order expansions controlled. implementation constraints depends deep learning framework used train neural networks. Frameworks currently supported tensorflow torch. Details constraints applied framework covered vignette(\"nn2poly-02-supported-DL-frameworks\"). However, nn2poly can work default kind neural network manually feeding neural network weights activation functions algorithm. Therefore, nn2poly limited supported deep learning frameworks.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"this-vignette-a-first-example","dir":"Articles","previous_headings":"","what":"This vignette: a first example","title":"Introduction to nn2poly","text":"vignette present basic behavior nn2poly used default version, without specifying deep learning framework explained previous remark. matter, showcase example get weights trained neural network manually create object needed information use nn2poly. result polynomial tries approximate neural network behavior. case neural network training constraints imposed. , explained previously, final approximation polynomial may accurate enough. example focused default version, , need build NN framework, use keras tensorflow matter. case, needed parameters extracted used default version nn2poly, can extrapolated framework. particular, solve really simple regression problem using simulated data polynomial, allows us ground truth control final polynomial coefficients obtained nn2poly similar polynomial originates data. Note: classification example please refer vignette(\"nn2poly-03-classification-example\")","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"polynomial-structure-in-nn2poly","dir":"Articles","previous_headings":"This vignette: a first example","what":"Polynomial structure in nn2poly","title":"Introduction to nn2poly","text":"final output using nn2poly neural network polynomial (several ones classification problems), package uses certain structure represent polynomials also provides nn2poly:::eval_poly(), function evaluate polynomials structure. use generate simulated data example, first define polynomial using format needed nn2poly, consists list containing: * Labels: list integer vectors denoting combinations variables appear term polynomial. Variables numbered 1 p p dimension problem. example, c(1,1,3) represent term \\(x_1^2x_3\\). special case intercept term, represented 0 * Values: Vector containing numerical values coefficients denoted labels. multiple polynomials labels different coefficient values wanted, matrix can employed, row represents polynomial. create polynomial \\(4x_1 - 3 x_2x_3\\):","code":"library(nn2poly) set.seed(42) polynomial <- list() polynomial$labels <- list(c(1), c(2,3)) polynomial$values <- c(4,-3)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"simulated-data","dir":"Articles","previous_headings":"This vignette: a first example","what":"Simulated data","title":"Introduction to nn2poly","text":"said polynomial, can now generate desired data train NN example. employ normal distribution generate variables \\(x_1, x_2, x_3\\) also error term \\(\\epsilon\\). Therefore, response variable \\(y\\) generated : \\(y = 4x_1 - 3 x_2x_3 + \\epsilon\\) scale data everything \\([-1,1]\\) interval divide train test.","code":"# Define number of variables and sample size p <- 3 n_sample <- 500  # Predictor variables X <- matrix(0,n_sample,p) for (i in 1:p){   X[,i] <- rnorm(n = n_sample,0,1) }  # Response variable + small error term Y <- nn2poly:::eval_poly(poly = polynomial, newdata = X) +   stats::rnorm(n_sample, 0, 0.1)  # Store all as a data frame data <- as.data.frame(cbind(X, Y)) head(data) #>           V1           V2         V3          Y #> 1  1.3709584  1.029140719  2.3250585 -1.7547416 #> 2 -0.5646982  0.914774868  0.5241222 -3.7107357 #> 3  0.3631284 -0.002456267  0.9707334  1.3609395 #> 4  0.6328626  0.136009552  0.3769734  2.4608270 #> 5  0.4042683 -0.720153545 -0.9959334 -0.6141076 #> 6 -0.1061245 -0.198124330 -0.5974829 -0.7455793 # Data scaling to [-1,1] maxs <- apply(data, 2, max) mins <- apply(data, 2, min) data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"original-neural-network","dir":"Articles","previous_headings":"This vignette: a first example","what":"Original neural network","title":"Introduction to nn2poly","text":"simulated data ready, can train neural network. method expected applied given trained densely connected feed forward neural network (NN now ), also referred multilayer perceptron (MLP). Therefore, explained , method can used train NN nn2poly needs weights activation functions. use keras/tensorflow train , manually build needed object weights activation functions fed nn2poly algorithm show trained framework. However, recall keras/tensorflow luz/torch models specific support nn2poly user-friendly approach default case covered manually build weights activation functions object. information supported frameworks refer vignette(\"nn2poly-02-supported-DL-frameworks\"). First, build model. Compile model: train : can visualize training process:  can also visualize NN predictions vs original Y values.  Note: Recall NN performance addressed nn2poly, meaning performance either good bad nn2poly’s goal still represent NN behavior predict good bad NN.","code":"library(keras)  # This sets all needed seeds tensorflow::set_random_seed(42) nn <- keras_model_sequential()  nn %>% layer_dense(units = 10,                   activation = \"tanh\",                   input_shape = p)  nn %>% layer_dense(units = 10,                   activation = \"tanh\")  nn %>% layer_dense(units = 1,                   activation = \"linear\")  nn #> Model: \"sequential\" #> ____________________________________________________________________________________________________________________ #>  Layer (type)                                       Output Shape                                  Param #            #> ==================================================================================================================== #>  dense (Dense)                                      (None, 10)                                    40                 #>  dense_1 (Dense)                                    (None, 10)                                    110                #>  dense_2 (Dense)                                    (None, 1)                                     11                 #> ==================================================================================================================== #> Total params: 161 (644.00 Byte) #> Trainable params: 161 (644.00 Byte) #> Non-trainable params: 0 (0.00 Byte) #> ____________________________________________________________________________________________________________________ compile(nn,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\") history <- fit(nn,                train_x,                train_y,                verbose = 0,                epochs = 250,                validation_split = 0.3 ) plot(history) # Obtain the predicted values with the NN to compare them prediction_NN <- predict(nn, test_x) #> 4/4 - 0s - 64ms/epoch - 16ms/step  # Diagonal plot implemented in the package to quickly visualize and compare predictions nn2poly:::plot_diagonal(x_axis =  prediction_NN, y_axis =  test_y, xlab = \"NN prediction\", ylab = \"Original Y\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"building-the-needed-input-for-default-nn2poly","dir":"Articles","previous_headings":"This vignette: a first example","what":"Building the needed input for default nn2poly","title":"Introduction to nn2poly","text":"NN trained, using chosen method user, default version using nn2poly requires set weight matrices activation functions neural network expected input form. list matrices : weight matrix per layer. weights matrices dimension \\(((1+input) * output)\\) first row corresponds bias vector, rest rows correspond ordered vector weights associated neuron input. name element list (.e. weight matrix) name activation function employed layer. Currently supported activation functions \"tanh\", \"sigmoid\", \"softplus\", \"linear\". , total size list equal number hidden layers plus one. particular, keras framework default separates kernel weights matrices dimension (input * output) bias vectors (1 * output), need add bias first row matrix ((1+input) * output). Note: Please note keras/tensorflow luz/torch models specific support nn2poly user-friendly approach manually building weights activation functions list. information supported frameworks refer vignette(\"nn2poly-02-supported-DL-frameworks\").","code":"keras_weights <- keras::get_weights(nn)  # Due to keras giving weights separated from the bias, we have twice the # elements that we want: n <- length(keras_weights)/2 nn_weights <- vector(mode = \"list\", length = n) for (i in 1:n){   nn_weights[[i]] <- rbind(keras_weights[[2*i]], keras_weights[[2*i-1]]) }  # The activation functions stored as strings: af_string_names <- c(\"tanh\",\"tanh\", \"linear\")  weights_object <- nn_weights names(weights_object) <- af_string_names  weights_object #> $tanh #>            [,1]       [,2]      [,3]       [,4]        [,5]       [,6]       [,7]        [,8]        [,9] #> [1,]  0.2468006 -0.1615576 0.4128721 -0.2022066  0.04867206 -0.3397017 -0.1973533 -0.08864177 -0.06979843 #> [2,]  0.2653632  0.1291278 0.1464561  0.5171605 -0.47153816  0.1952459  0.6991890 -0.34414759 -0.01077842 #> [3,] -0.7783106 -0.1182964 0.8215331  0.0756740  0.06914742  0.7492308 -0.8363205 -0.46379340  0.04437404 #> [4,] -0.7159964  0.5072741 0.6033612 -0.4585148  0.49232438 -0.7474106  0.1091831  0.34101799  0.04305385 #>             [,10] #> [1,] -0.359030366 #> [2,]  0.006373413 #> [3,] -0.698476076 #> [4,] -0.571668506 #>  #> $tanh #>              [,1]          [,2]        [,3]        [,4]        [,5]       [,6]        [,7]        [,8] #>  [1,] -0.06682093  1.354177e-02  0.41062176  0.06204464  0.08612972  0.2199472  0.07406761  0.16231309 #>  [2,]  0.61209673 -1.587261e-01  0.33286688  0.16685191  0.25638032 -0.2960541  0.06451954 -0.04405669 #>  [3,] -0.24969149 -5.748887e-02 -0.46696794  0.12622431 -0.03608320 -0.3931088  0.28291187  0.24243380 #>  [4,]  0.38323382  4.771161e-01  0.11585876  0.44199020  0.16203447  0.6357560  0.01491554 -0.29491946 #>  [5,] -0.24755152  6.043483e-06  0.41534948  0.27495798 -0.46539015 -0.3829005 -0.53592098 -0.42729110 #>  [6,] -0.26591998  5.250556e-01 -0.10562975  0.37414369  0.26212201  0.4499114 -0.41681677 -0.25272584 #>  [7,]  0.49481559  1.359816e-01  0.67004895 -0.15929574  0.16280667  0.4475754 -0.20691858  0.40582910 #>  [8,]  0.40330765 -4.343324e-01 -0.92225003 -0.15942611  0.01812540  0.2977643 -0.04585903 -0.37514219 #>  [9,]  0.51166564  1.184353e-01 -0.82071930 -0.02945352  0.31787610  0.2762571  0.48814669 -0.39470875 #> [10,]  0.27744713 -1.118216e-01  0.02931483  0.35466376  0.24984393 -0.1250943 -0.32097501  0.09834206 #> [11,] -0.50523925  4.479433e-01 -0.55034703  0.48515356 -0.03854835 -0.8001245  0.15069254 -0.20994137 #>              [,9]        [,10] #>  [1,]  0.08528626  0.074458212 #>  [2,] -0.57139808 -0.314788640 #>  [3,] -0.47687724 -0.002060457 #>  [4,]  0.09033196 -0.017912282 #>  [5,] -0.48928314 -0.197884247 #>  [6,] -0.28060704 -0.249755934 #>  [7,] -0.11089712 -0.387193263 #>  [8,]  0.24891976 -0.395170867 #>  [9,]  0.01584071 -0.538033605 #> [10,] -0.06148605 -0.403377086 #> [11,]  0.23820342  0.375222772 #>  #> $linear #>              [,1] #>  [1,] -0.07325488 #>  [2,]  0.92378414 #>  [3,] -0.08239315 #>  [4,] -0.32572412 #>  [5,] -0.43841833 #>  [6,] -0.08093655 #>  [7,]  0.59904134 #>  [8,] -0.65850627 #>  [9,] -0.04606140 #> [10,] -0.79061949 #> [11,] -0.89735550"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"polynomial-obtained-with-nn2poly-from-weights-and-activation-functions","dir":"Articles","previous_headings":"This vignette: a first example","what":"Polynomial obtained with nn2poly from weights and activation functions","title":"Introduction to nn2poly","text":"setting NN information desired input shape, ready employ nn2poly. last parameter need specify final order desired polynomial, max_order. integer value denoting maximum order terms computed polynomial. Usually 2 3 enough real data default value set 2, capturing pairwise interactions. Note higher orders suppose explosion possible combinations variables therefore number terms polynomial. example set max_order = 3 obtain final polynomial: can glimpse coefficients polynomial stored. Note structure explained polynomial generated data, list labels values. case, obtained polynomial order 3.","code":"final_poly <- nn2poly(object = weights_object,                       max_order = 3) final_poly #> $labels #> $labels[[1]] #> [1] 0 #>  #> $labels[[2]] #> [1] 1 #>  #> $labels[[3]] #> [1] 2 #>  #> $labels[[4]] #> [1] 3 #>  #> $labels[[5]] #> [1] 1 1 #>  #> $labels[[6]] #> [1] 1 2 #>  #> $labels[[7]] #> [1] 1 3 #>  #> $labels[[8]] #> [1] 2 2 #>  #> $labels[[9]] #> [1] 2 3 #>  #> $labels[[10]] #> [1] 3 3 #>  #> $labels[[11]] #> [1] 1 1 1 #>  #> $labels[[12]] #> [1] 1 1 2 #>  #> $labels[[13]] #> [1] 1 1 3 #>  #> $labels[[14]] #> [1] 1 2 2 #>  #> $labels[[15]] #> [1] 1 2 3 #>  #> $labels[[16]] #> [1] 1 3 3 #>  #> $labels[[17]] #> [1] 2 2 2 #>  #> $labels[[18]] #> [1] 2 2 3 #>  #> $labels[[19]] #> [1] 2 3 3 #>  #> $labels[[20]] #> [1] 3 3 3 #>  #>  #> $values #>               [,1] #>  [1,] -0.152201327 #>  [2,]  0.750790396 #>  [3,]  0.096754449 #>  [4,] -0.055312169 #>  [5,]  0.087070907 #>  [6,]  0.184542875 #>  [7,]  0.056719718 #>  [8,]  0.117868180 #>  [9,] -2.316887421 #> [10,] -0.111600389 #> [11,] -0.106978125 #> [12,]  0.335722744 #> [13,]  0.007464716 #> [14,]  0.580397844 #> [15,] -0.072672555 #> [16,]  0.064910409 #> [17,]  0.274290026 #> [18,] -0.474785464 #> [19,] -0.242252638 #> [20,]  0.013870973 #>  #> attr(,\"class\") #> [1] \"nn2poly\""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"predictions-using-the-obtained-polynomial","dir":"Articles","previous_headings":"This vignette: a first example","what":"Predictions using the obtained polynomial","title":"Introduction to nn2poly","text":"obtained polynomial coefficients, can use predict response variable \\(Y\\) using polynomial. can done using predcit output nn2poly (object class \"nn2poly\") together desired values predictor variables.","code":"# Obtain the predicted values for the test data with our polynomial prediction_poly <- predict(object = final_poly,                            newdata = test_x)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-01-introduction.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"This vignette: a first example","what":"Visualizing the results","title":"Introduction to nn2poly","text":"Note: note , order avoid asymptotic behavior method, important impose kind constraints training neural network weights, something simplify first example. Details depend chosen deep learning framework covered next vignettes. advisable always check predictions obtained new polynomial close original NN predictions (case differ, can also try find checking Taylor expansions). help , couple functions included allow us plot results. simple plot comparing polynomial NN predictions can obtained nn2poly:::plot_diagonal(), red diagonal line represents perfect relationship NN polynomial predictions obtained. example, theoretical weight constraints imposed, can observe approximation perfect.  can also plot \\(n\\) important coefficients absolute value compare variables interactions relevant polynomial. Note , data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant. case can see two important obtained coefficients 2,3 1, precisely two terms appearing original polynomial \\(4x_1 - 3 x_2x_3\\). However, interactions order 3 appear also relevant, caused Taylor expansions controlled imposed constraints neural network weights training.  Another convenient plot show algorithm affected layer can obtained nn2poly:::plot_taylor_and_activation_potentials(), activation potentials neuron computed presented Taylor expansion approximation activation function layer. case, used constraints NN training, activation potentials strictly centered around zero.","code":"nn2poly:::plot_diagonal(x_axis =  prediction_NN, y_axis =  prediction_poly, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") plot(final_poly, n=8) nn2poly:::plot_taylor_and_activation_potentials(object = nn,                                                 data = train,                                                 max_order = 3,                                                 constraints = FALSE) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"data-generation","dir":"Articles","previous_headings":"","what":"Data generation","title":"Supported DL frameworks","text":"goal show impose needed constraints framework, replicate polynomial data generation vignette(\"nn2poly-01-introduction\") solve later regression problem different deep learning frameworks. Refer vignette details. Create polynomial \\(4x_1 - 3 x_2x_3\\): Generate data: Scale data everything \\([-1,1]\\) interval divide train test.","code":"library(nn2poly) set.seed(42) polynomial <- list() polynomial$labels <- list(c(1), c(2,3)) polynomial$values <- c(4,-3) # Define number of variables and sample size p <- 3 n_sample <- 500  # Predictor variables X <- matrix(0,n_sample,p) for (i in 1:p){   X[,i] <- rnorm(n = n_sample,0,1) }  # Response variable + small error term Y <- nn2poly:::eval_poly(poly = polynomial, newdata = X) +   stats::rnorm(n_sample, 0, 0.1)  # Store all as a data frame data <- as.data.frame(cbind(X, Y)) head(data) #>           V1           V2         V3          Y #> 1  1.3709584  1.029140719  2.3250585 -1.7547416 #> 2 -0.5646982  0.914774868  0.5241222 -3.7107357 #> 3  0.3631284 -0.002456267  0.9707334  1.3609395 #> 4  0.6328626  0.136009552  0.3769734  2.4608270 #> 5  0.4042683 -0.720153545 -0.9959334 -0.6141076 #> 6 -0.1061245 -0.198124330 -0.5974829 -0.7455793 # Data scaling to [-1,1] maxs <- apply(data, 2, max) mins <- apply(data, 2, min) data <- as.data.frame(scale(data, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2))  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"supported-frameworks-examples","dir":"Articles","previous_headings":"","what":"Supported frameworks examples","title":"Supported DL frameworks","text":"common data generated, now ready delve different deep learning frameworks. case solve regression problem previously generated polynomial data, two neural networks framework. neural networks structure one constrained (nn_con) one unconstrained (nn_uncon) showcase differences.","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"kerastensorflow","dir":"Articles","previous_headings":"Supported frameworks examples","what":"keras/tensorflow","title":"Supported DL frameworks","text":"section show use nn2poly keras neural network impose needed weight constraints training.","code":"library(nn2poly) library(keras)  # This sets all needed seeds tensorflow::set_random_seed(42)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"model-definition","dir":"Articles","previous_headings":"Supported frameworks examples > keras/tensorflow","what":"Model definition","title":"Supported DL frameworks","text":"First set structure neural networks using sequential model keras. following function used create constrained unconstrained neural networks keras example. Now define NNs. needed step impose implemented weight constraints keras model use function add_constraints() train usually. constraints currently accept two possible types, \"l1_norm\" \"l2_norm\". cases, norm weight vector (including bias) incident neuron constrained less equal 1. Using data scaled \\([-1,1]\\) interval, l1-norm one guarantees best results. Also, important note weight constraints imposed layers except last one, expected linear need Taylor expansion. Note: implementation differs slightly constraints provided keras implementation join bias rest weights keras default constraints allow bias kernel constraints. implementation uses custom callback applied end batch.","code":"keras_model <- function() {   tensorflow::set_random_seed(42)    nn <- keras::keras_model_sequential()   nn <- keras::layer_dense(nn, units = 100, activation = \"tanh\", input_shape = p)   nn <- keras::layer_dense(nn, units = 100, activation = \"tanh\")   nn <- keras::layer_dense(nn, units = 100, activation = \"tanh\")   nn <- keras::layer_dense(nn, units = 1, activation = \"linear\")    nn } nn_uncon <- keras_model()  nn_con <- keras_model() nn_con <- add_constraints(nn_con, constraint_type = \"l1_norm\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"training","dir":"Articles","previous_headings":"Supported frameworks examples > keras/tensorflow","what":"Training","title":"Supported DL frameworks","text":"Now can compile train NNs using standard keras approach. Note increase number needed epochs constrained NN learn properly.","code":"compile(nn_uncon,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\")  history1 <- fit(nn_uncon,                train_x,                train_y,                verbose = 0,                epochs = 150,                batch_size = 50,                validation_split = 0.2 )  plot(history1) compile(nn_con,         loss = \"mse\",         optimizer = optimizer_adam(),         metrics = \"mse\")  history2 <- fit(nn_con,                train_x,                train_y,                verbose = 0,                epochs = 700,                batch_size = 50,                validation_split = 0.2 )  plot(history2)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"nn-predictions","dir":"Articles","previous_headings":"Supported frameworks examples > keras/tensorflow","what":"NN predictions","title":"Supported DL frameworks","text":"can visualize NN predictions vs original Y values neural networks observe provide accurate predictions (values fall near “perfect” diagonal red line).","code":"# Obtain the predicted values with the NN to compare them prediction_nn_uncon <- predict(nn_uncon, test_x) #> 4/4 - 0s - 48ms/epoch - 12ms/step  # Diagonal plot implemented in the package to quickly visualize and compare predictions nn2poly:::plot_diagonal(x_axis =  prediction_nn_uncon, y_axis =  test_y, xlab = \"Unconstrained NN prediction\", ylab = \"Original Y\") # Obtain the predicted values with the NN to compare them prediction_nn_con <- predict(nn_con, test_x) #> 4/4 - 0s - 91ms/epoch - 23ms/step  # Diagonal plot implemented in the package to quickly visualize and compare predictions nn2poly:::plot_diagonal(x_axis =  prediction_nn_con, y_axis =  test_y, xlab = \"Constrained NN prediction\", ylab = \"Original Y\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"using-nn2poly","dir":"Articles","previous_headings":"Supported frameworks examples > keras/tensorflow","what":"Using nn2poly","title":"Supported DL frameworks","text":"NNs trained, can directly call nn2poly keras model. can visualize polynomial predictions versus NN predictions   can clearly see constrained NN allows obtain close approximation nn2poly polynomial obtained unconstrained one good representation. can also represent obtained polynomial coefficients observe constrained case clearly terms 2,3 1 sign original polynomial rest close 0. Note: coefficients values scale original polynomial due fact scaled data training, even response variable Y. Furthermore, data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant","code":"# Polynomial for nn_uncon final_poly_uncon <- nn2poly(object = nn_uncon,                       max_order = 3)  # Polynomial for nn_con final_poly_con <- nn2poly(object = nn_con,                       max_order = 3) # Obtain the predicted values for the test data with our two polynomials prediction_poly_uncon <- predict(object = final_poly_uncon, newdata = test_x) prediction_poly_con <- predict(object = final_poly_con, newdata = test_x)  nn2poly:::plot_diagonal(x_axis =  prediction_nn_uncon, y_axis =  prediction_poly_uncon, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for nn_uncon\") nn2poly:::plot_diagonal(x_axis =  prediction_nn_con, y_axis =  prediction_poly_con, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for nn_con\") plot(final_poly_uncon, n = 8) plot(final_poly_con, n = 8)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"luztorch","dir":"Articles","previous_headings":"Supported frameworks examples","what":"luz/torch","title":"Supported DL frameworks","text":"section show use nn2poly torch neural network, built higher level API luz, impose needed weight constraints training. Furthermore, explain use luz_model_sequential(), helper needed create torch model adequate manner can easily recognized nn2poly.","code":"library(torch) library(luz)  set.seed(42)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"data-loader-for-torch","dir":"Articles","previous_headings":"Supported frameworks examples > luz/torch","what":"Data loader for torch","title":"Supported DL frameworks","text":"framework, need manipulate bit data use input torch model. can done dividing train_x data intro train validation matrices using luz::as_dataloader()","code":"# Divide in only train and validation all_indices   <- 1:nrow(train_x) only_train_indices <- sample(all_indices, size = round(nrow(train_x)) * 0.8) val_indices   <- setdiff(all_indices, only_train_indices)  # Create lists with x and y values to feed luz::as_dataloader() only_train_x <- as.matrix(train_x[only_train_indices,]) only_train_y <- as.matrix(train_y[only_train_indices,]) val_x <- as.matrix(train_x[val_indices,]) val_y <- as.matrix(train_y[val_indices,])  only_train_list <- list(x = only_train_x, y = only_train_y) val_list <- list(x = val_x, y = val_y)  torch_data <- list(   train = luz::as_dataloader(only_train_list, batch_size = 50, shuffle = TRUE),   valid = luz::as_dataloader(val_list, batch_size = 50) )"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"model-definition-1","dir":"Articles","previous_headings":"Supported frameworks examples > luz/torch","what":"Model definition","title":"Supported DL frameworks","text":"Now set structure neural networks using sequential model torch. implemented helper luz_model_sequential(), lets user stack linear layers similar way keras allows nn2poly() directly used model without need extract weights activation functions manually default example vignette(\"nn2poly-01-introduction\"). following function used create constrained unconstrained neural networks torch example. Now define NNs. case, differing keras example, impose constraints later follow approach luz documentation. However, use add_constraints() analogous.","code":"luz_nn <- function() {   torch::torch_manual_seed(42)    luz_model_sequential(     torch::nn_linear(p,100),     torch::nn_tanh(),     torch::nn_linear(100,100),     torch::nn_tanh(),     torch::nn_linear(100,100),     torch::nn_tanh(),     torch::nn_linear(100,1)   ) } nn_uncon <- luz_nn() nn_con <- luz_nn()"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"training-1","dir":"Articles","previous_headings":"Supported frameworks examples > luz/torch","what":"Training","title":"Supported DL frameworks","text":"First train unconstrained NN using ans standard luz approach.  , train constrained NN using function function add_constraints() calling fit(). constraints currently accept two possible types, \"l1_norm\" \"l2_norm\". cases, norm weight vector (including bias) incident neuron constrained less equal 1. Using data scaled \\([-1,1]\\) interval, l1-norm one guarantees best results. Also, important note weight constraints imposed layers except last one, expected linear need Taylor expansion. Note increase number needed epochs constrained NN learn properly.  Note: implementation uses custom callback applied end batch.","code":"fitted_uncon <- nn_uncon %>%     luz::setup(       loss = torch::nn_mse_loss(),       optimizer = torch::optim_adam,       metrics = list(         luz::luz_metric_mse()       )     ) %>%     luz::fit(torch_data$train, epochs = 50, valid_data = torch_data$valid)  fitted_uncon %>% plot() fitted_con <- nn_con %>%   luz::setup(     loss = torch::nn_mse_loss(),     optimizer = torch::optim_adam,   ) %>%   add_constraints(\"l1_norm\") %>%   fit(torch_data$train, epochs = 700, valid_data = torch_data$valid)  fitted_con %>% plot()"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"nn-predictions-1","dir":"Articles","previous_headings":"Supported frameworks examples > luz/torch","what":"NN predictions","title":"Supported DL frameworks","text":"can visualize NN predictions vs original Y values neural networks observe provide accurate predictions (values fall near “perfect” diagonal red line).","code":"# Obtain the predicted values with the NN to compare them prediction_NN_uncon <- as.array(predict(fitted_uncon, test_x))  # Diagonal plot implemented in the package to quickly visualize and compare predictions nn2poly:::plot_diagonal(x_axis =  prediction_NN_uncon, y_axis =  test_y, xlab = \"Unconstrained NN prediction\", ylab = \"Original Y\") # Obtain the predicted values with the NN to compare them prediction_NN_con <- as.array(predict(fitted_con, test_x))  # Diagonal plot implemented in the package to quickly visualize and compare predictions nn2poly:::plot_diagonal(x_axis =  prediction_NN_con, y_axis =  test_y, xlab = \"Constrained NN prediction\", ylab = \"Original Y\")"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-02-supported-DL-frameworks.html","id":"using-nn2poly-1","dir":"Articles","previous_headings":"Supported frameworks examples > luz/torch","what":"Using nn2poly","title":"Supported DL frameworks","text":"NNs trained, can directly call nn2poly luz model. can visualize polynomial predictions versus NN predictions   can clearly see constrained NN allows obtain close approximation nn2poly polynomial obtained unconstrained one good representation. can also represent obtained polynomial coefficients observe constrained case clearly terms 2,3 1 sign original polynomial rest close 0. Note: coefficients values scale original polynomial due fact scaled data training, even response variable Y. Furthermore, data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant","code":"# Polynomial for nn_uncon final_poly_uncon <- nn2poly(object = fitted_uncon,                       max_order = 3)  # Polynomial for nn_con final_poly_con <- nn2poly(object = fitted_con,                       max_order = 3) # Obtain the predicted values for the test data with our two polynomials prediction_poly_uncon <- predict(object = final_poly_uncon, newdata = test_x) prediction_poly_con <- predict(object = final_poly_con, newdata = test_x)  nn2poly:::plot_diagonal(x_axis =  prediction_nn_uncon, y_axis =  prediction_poly_uncon, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for nn_uncon\") nn2poly:::plot_diagonal(x_axis =  prediction_nn_con, y_axis =  prediction_poly_con, xlab = \"NN prediction\", ylab = \"Polynomial prediction\") + ggplot2::ggtitle(\"Polynomial for nn_con\") plot(final_poly_uncon, n = 8) plot(final_poly_con, n = 8)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-classification-example.html","id":"multiclass-classification-example","dir":"Articles","previous_headings":"","what":"Multiclass classification example","title":"Classification example using tensorflow","text":"showing use nn2poly regression setting vignette(\"nn2poly-01-introduction\") vignette(\"nn2poly-02-supported-DL-frameworks\"), see multiclass classification example using iris dataset showcase nn2poly obtains polynomial class (output neuron). order train model use case keras/tensorflow framework, benefit provided support nn2poly saw vignette(\"nn2poly-02-supported-DL-frameworks\").","code":"library(nn2poly) library(keras)  # This sets all needed seeds tensorflow::set_random_seed(42)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-classification-example.html","id":"data-preparation","dir":"Articles","previous_headings":"Multiclass classification example","what":"Data preparation","title":"Classification example using tensorflow","text":"First load iris dataset scale data \\([-1,1]\\) interval:","code":"# Load the data data(iris)  # Change response to numeric. In this case, Species was already numeric, # but this step is needed if it is a factor variable. iris$Species <- as.numeric(iris$Species)  # Define dimension p (number of predictor variables) p <- dim(iris)[2] - 1  # Define objective classes n_class <- max(iris[,(p+1)])  # Move objective classes from (1:3) to (0:2), needed for tensorflow iris[,(p+1)] <- iris[,(p+1)] - 1 # Scale the data in the [-1,1] interval and separate train and test # Only the predictor variables are scaled, not the response as those will be # the different classes. iris_x <- iris[,-(p+1)] maxs <- apply(iris_x, 2, max) mins <- apply(iris_x, 2, min) data_x_scaled <- as.data.frame(scale(iris_x, center = mins + (maxs - mins) / 2, scale = (maxs - mins) / 2)) data <- cbind(data_x_scaled, iris[,(p+1)])  # Divide in train (0.75) and test (0.25) index <- sample(1:nrow(data), round(0.75 * nrow(data))) train <- data[index, ] test <- data[-index, ]  train_x <- as.matrix(train[,-(p+1)]) train_y <- as.matrix(train[,(p+1)])  test_x <- as.matrix(test[,-(p+1)]) test_y <- as.matrix(test[,(p+1)])"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-classification-example.html","id":"neural-network","dir":"Articles","previous_headings":"Multiclass classification example","what":"Neural network","title":"Classification example using tensorflow","text":"section create train (imposing weight constraints) original neural network later explained nn2poly().","code":""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-classification-example.html","id":"build-and-train-the-nn","dir":"Articles","previous_headings":"Multiclass classification example > Neural network","what":"Build and train the NN","title":"Classification example using tensorflow","text":"First, build model. Note: case NN linear output number neurons number classes predict (3 species, n_class). , linear output transformed probability find probable class step done training. Therefore, nn2poly used obtain polynomial approximates nn linear outputs results also transformed probabilities predict highest probability class. compile train model, using categorical crossentropy loss accuracy chosen metric.","code":"keras_model <- function() {   tensorflow::set_random_seed(42)    nn <- keras::keras_model_sequential()   nn <- keras::layer_dense(nn, units = 100, activation = \"tanh\", input_shape = p)   nn <- keras::layer_dense(nn, units = 100, activation = \"tanh\")   nn <- keras::layer_dense(nn, units = n_class, activation = \"linear\")    nn }  nn <- keras_model() # Impose weight constraints provided by nn2poly package nn <- add_constraints(nn, constraint_type = \"l1_norm\") nn #> Model: \"sequential\" #> ________________________________________________________________________________ #>  Layer (type)                       Output Shape                    Param #      #> ================================================================================ #>  dense (Dense)                      (None, 100)                     500          #>  dense_1 (Dense)                    (None, 100)                     10100        #>  dense_2 (Dense)                    (None, 3)                       303          #> ================================================================================ #> Total params: 10903 (42.59 KB) #> Trainable params: 10903 (42.59 KB) #> Non-trainable params: 0 (0.00 Byte) #> ________________________________________________________________________________ compile(nn,         loss = loss_sparse_categorical_crossentropy(from_logits = TRUE),         optimizer = optimizer_adam(),         metrics = \"accuracy\")  history <- fit(nn,                train_x,                train_y,                verbose = 0,                epochs = 200,                validation_split = 0.3 )  plot(history)"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-classification-example.html","id":"nn-predictions","dir":"Articles","previous_headings":"Multiclass classification example > Neural network","what":"NN Predictions","title":"Classification example using tensorflow","text":"case, asses NNs accuracy transform output probability, using layer_activation_softmax() choosing class highest probability k_argmax. Note: said , nn2poly() approximate linear output neural network, need perform transformation also polynomial predictions obtain classification response. can also compute linear output model (converting probability) store compare polynomial output later. Finally can use confusion matrix visualize results, can see NN correctly predicts almost classes test data:","code":"# Define the model probability model using our previously trained nn probability_model <- keras_model_sequential() %>%   nn() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax)  # Obtain the predicted classes with the NN to compare them prediction_NN_class <- predict(probability_model, test_x) #> 2/2 - 0s - 70ms/epoch - 35ms/step # Also, the linear output can be predicted before the probability model prediction_NN <- predict(nn, test_x) #> 2/2 - 0s - 40ms/epoch - 20ms/step # Create a confusion matrix cm <- caret::confusionMatrix(as.factor(prediction_NN_class), as.factor(test_y)) cm #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 11  0  0 #>          1  0 11  1 #>          2  0  1 14 #>  #> Overall Statistics #>                                            #>                Accuracy : 0.9474           #>                  95% CI : (0.8225, 0.9936) #>     No Information Rate : 0.3947           #>     P-Value [Acc > NIR] : 7.82e-13         #>                                            #>                   Kappa : 0.9203           #>                                            #>  Mcnemar's Test P-Value : NA               #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   0.9167   0.9333 #> Specificity            1.0000   0.9615   0.9565 #> Pos Pred Value         1.0000   0.9167   0.9333 #> Neg Pred Value         1.0000   0.9615   0.9565 #> Prevalence             0.2895   0.3158   0.3947 #> Detection Rate         0.2895   0.2895   0.3684 #> Detection Prevalence   0.2895   0.3158   0.3947 #> Balanced Accuracy      1.0000   0.9391   0.9449"},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-classification-example.html","id":"using-nn2poly","dir":"Articles","previous_headings":"Multiclass classification example > Polynomial representation","what":"Using nn2poly","title":"Classification example using tensorflow","text":"NN trained, can directly call nn2poly() model, need choose value max_order limit order computed coefficients final polynomial. case set 3. Note case, 3 output neurons, 3 output polynomials. polynomials stored way regression case, list labels values, case values matrix 3 rows, row polynomial obtained output neuron.","code":"# Polynomial for nn final_poly <- nn2poly(object = nn,                       max_order = 3) final_poly #> $labels #> $labels[[1]] #> [1] 0 #>  #> $labels[[2]] #> [1] 1 #>  #> $labels[[3]] #> [1] 2 #>  #> $labels[[4]] #> [1] 3 #>  #> $labels[[5]] #> [1] 4 #>  #> $labels[[6]] #> [1] 1 1 #>  #> $labels[[7]] #> [1] 1 2 #>  #> $labels[[8]] #> [1] 1 3 #>  #> $labels[[9]] #> [1] 1 4 #>  #> $labels[[10]] #> [1] 2 2 #>  #> $labels[[11]] #> [1] 2 3 #>  #> $labels[[12]] #> [1] 2 4 #>  #> $labels[[13]] #> [1] 3 3 #>  #> $labels[[14]] #> [1] 3 4 #>  #> $labels[[15]] #> [1] 4 4 #>  #> $labels[[16]] #> [1] 1 1 1 #>  #> $labels[[17]] #> [1] 1 1 2 #>  #> $labels[[18]] #> [1] 1 1 3 #>  #> $labels[[19]] #> [1] 1 1 4 #>  #> $labels[[20]] #> [1] 1 2 2 #>  #> $labels[[21]] #> [1] 1 2 3 #>  #> $labels[[22]] #> [1] 1 2 4 #>  #> $labels[[23]] #> [1] 1 3 3 #>  #> $labels[[24]] #> [1] 1 3 4 #>  #> $labels[[25]] #> [1] 1 4 4 #>  #> $labels[[26]] #> [1] 2 2 2 #>  #> $labels[[27]] #> [1] 2 2 3 #>  #> $labels[[28]] #> [1] 2 2 4 #>  #> $labels[[29]] #> [1] 2 3 3 #>  #> $labels[[30]] #> [1] 2 3 4 #>  #> $labels[[31]] #> [1] 2 4 4 #>  #> $labels[[32]] #> [1] 3 3 3 #>  #> $labels[[33]] #> [1] 3 3 4 #>  #> $labels[[34]] #> [1] 3 4 4 #>  #> $labels[[35]] #> [1] 4 4 4 #>  #>  #> $values #>              [,1]         [,2]         [,3] #>  [1,]  0.04067109  2.838697004 -2.366766912 #>  [2,] -1.01692952  0.922284435  0.306762048 #>  [3,]  3.82618130 -1.532152815 -2.683000514 #>  [4,] -4.86343269 -0.367029700  5.201883882 #>  [5,] -6.05908248 -2.026009803  7.701675045 #>  [6,]  0.02216647 -0.068650500  0.031318449 #>  [7,] -0.09821698  0.115922745  0.007638440 #>  [8,]  0.08146315 -0.072443601 -0.025525980 #>  [9,]  0.04337399 -0.019371395 -0.029198248 #> [10,]  0.08763212 -0.122314949  0.007196786 #> [11,] -0.13090934  0.160297202  0.008898167 #> [12,] -0.03982065  0.118363240 -0.047742059 #> [13,] -0.03990296 -0.186114002  0.182448046 #> [14,] -0.22518863 -0.383706458  0.517483976 #> [15,] -0.21539519 -0.340739130  0.476053215 #> [16,]  0.01640856 -0.015861053 -0.004179781 #> [17,] -0.08649215  0.054035395  0.045407899 #> [18,]  0.08657797 -0.020808458 -0.071458194 #> [19,]  0.10626455  0.016833308 -0.120917295 #> [20,] -0.09226002  0.058721711  0.047211597 #> [21,] -0.12782353  0.099224893  0.053681832 #> [22,] -0.14300636  0.061378209  0.099556359 #> [23,]  0.08681554 -0.019495659 -0.072698532 #> [24,]  0.07906020 -0.071096510 -0.027609755 #> [25,]  0.10751970  0.019949727 -0.124596164 #> [26,] -0.12832404  0.057077780  0.086244551 #> [27,]  0.31410761 -0.112571487 -0.233324113 #> [28,]  0.39822386 -0.072171099 -0.351003640 #> [29,]  0.32366110 -0.115551628 -0.240170974 #> [30,] -0.64179985  0.150429244  0.539640723 #> [31,]  0.40593582 -0.069258417 -0.360602426 #> [32,]  0.17561230  0.009076263 -0.185280705 #> [33,]  0.51767960  0.109300583 -0.609898697 #> [34,]  0.51398607  0.101899652 -0.600694175 #> [35,]  0.34526975  0.146409617 -0.461815984 #>  #> attr(,\"class\") #> [1] \"nn2poly\""},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-classification-example.html","id":"obtaining-polynomial-predictions","dir":"Articles","previous_headings":"Multiclass classification example > Polynomial representation","what":"Obtaining polynomial predictions","title":"Classification example using tensorflow","text":"said , obtained polynomial represents neural network including softmax function computing class assigned observation. , need define keras sequential model includes class computation polynomial output. polynomial output obtained predict() used nn2poly() output, case matrix form, 3 polynomials evaluated time:","code":"# Obtain the predicted values for the test data with our Polynomial Regression prediction_poly_matrix <- predict(object = final_poly, newdata = test_x)  # Define probability model with keras for the polynomial outputs probability_poly <- keras_model_sequential() %>%   layer_activation_softmax() %>%   layer_lambda(k_argmax)  # Class prediction with the polynomial outputs prediction_poly_class <- predict(probability_poly,                                  prediction_poly_matrix) #> 2/2 - 0s - 121ms/epoch - 60ms/step"},{"path":"https://ibidat.github.io/nn2poly/articles/nn2poly-03-classification-example.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Multiclass classification example > Polynomial representation","what":"Visualizing the results","title":"Classification example using tensorflow","text":"polynomial predictions, two options. can represent diagonal line linear outputs obtained directly polynomial NN predictions, compare assigned classes employing probability models. Please note compare predictions (linear classes) polynomials NN predictions original data, nn2poly’s goal faithfully represent NN behavior independently well NN predicts. First, let’s observe confusion matrix assigned classes: polynomials obtain results original NN. , can extract diagonal plot polynomials obtained, total \\(3\\) diagonal plots.  can observe polynomials obtain quite similar predictions equivalent NN predictions. can also plot \\(n\\) important coefficients absolute value compare variables interactions relevant polynomial. case, 3 plots NN , one per polynomial output neuron. case, obtained coefficients represent important variables assigning probability class.  Note: coefficients values scale original polynomial due fact scaled data training, even response variable Y. Furthermore, data scaled \\([-1,1]\\) interval, interactions order 2 higher usually need higher absolute value lower order coefficients relevant Finally, problem Taylor expansion can checked following plots, layer represented activation function, Taylor expansion, error also density activation potentials activation functions receives layer. can clearly seen activation potentials density, green, kept close zero, thus accurate Taylor expansion around zero hidden layer.","code":"# Confussion matrix between NN class prediction and polynomial class prediction cm_poly <- caret::confusionMatrix(as.factor(prediction_NN_class), as.factor(prediction_poly_class)) cm_poly #> Confusion Matrix and Statistics #>  #>           Reference #> Prediction  0  1  2 #>          0 11  0  0 #>          1  0 12  0 #>          2  0  0 15 #>  #> Overall Statistics #>                                       #>                Accuracy : 1           #>                  95% CI : (0.9075, 1) #>     No Information Rate : 0.3947      #>     P-Value [Acc > NIR] : 4.568e-16   #>                                       #>                   Kappa : 1           #>                                       #>  Mcnemar's Test P-Value : NA          #>  #> Statistics by Class: #>  #>                      Class: 0 Class: 1 Class: 2 #> Sensitivity            1.0000   1.0000   1.0000 #> Specificity            1.0000   1.0000   1.0000 #> Pos Pred Value         1.0000   1.0000   1.0000 #> Neg Pred Value         1.0000   1.0000   1.0000 #> Prevalence             0.2895   0.3158   0.3947 #> Detection Rate         0.2895   0.3158   0.3947 #> Detection Prevalence   0.2895   0.3158   0.3947 #> Balanced Accuracy      1.0000   1.0000   1.0000 for (i in 1:3){   print(     nn2poly:::plot_diagonal(x_axis =  prediction_NN[,i],                   y_axis =  prediction_poly_matrix[,i],                   xlab = \"NN prediction\",                   ylab = \"Polynomial prediction\")         ) } plot(final_poly, n = 8) nn2poly:::plot_taylor_and_activation_potentials(object = nn,                                                 data = train,                                                 max_order = 3,                                                 constraints = TRUE) #> [[1]] #>  #> [[2]] #>  #> [[3]]"},{"path":"https://ibidat.github.io/nn2poly/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Pablo Morala. Author, maintainer. Iñaki Ucar. Author. Jose Ignacio Diez. Contractor.","code":""},{"path":"https://ibidat.github.io/nn2poly/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Morala P, Cifuentes J, Lillo R, Ucar (2021). “Towards mathematical framework inform neural network modelling via polynomial regression.” Neural Networks, 142, 57–72. doi:10.1016/j.neunet.2021.04.036. Morala P, Cifuentes J, Lillo R, Ucar (2023). “NNN2Poly: Polynomial Representation Deep Feed-Forward Artificial Neural Networks.” IEEE Transactions Neural Networks Learning Systems. doi:10.1109/TNNLS.2023.3330328.","code":"@Article{,   title = {Towards a mathematical framework to inform neural network modelling via polynomial regression},   author = {Pablo Morala and J. Alexandra Cifuentes and Rosa E. Lillo and I{\\~n}aki Ucar},   journal = {Neural Networks},   month = {October},   year = {2021},   volume = {142},   pages = {57--72},   doi = {10.1016/j.neunet.2021.04.036}, } @Article{,   title = {NNN2Poly: A Polynomial Representation for Deep Feed-Forward Artificial Neural Networks},   author = {Pablo Morala and J. Alexandra Cifuentes and Rosa E. Lillo and I{\\~n}aki Ucar},   journal = {IEEE Transactions on Neural Networks and Learning Systems},   year = {2023},   doi = {10.1109/TNNLS.2023.3330328}, }"},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"nn2poly-transforming-neural-networks-into-polynomials","dir":"","previous_headings":"","what":"Neural Network Weights Transformation into Polynomial Coefficients","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"nn2poly package implements NN2Poly method allows transform already trained deep feed-forward fully connected neural network polynomial representation predicts similar possible original neural network. obtained polynomial coefficients can used explain features (interactions) importance neural network, therefore working tool interpretability eXplainable Artificial Intelligence (XAI).","code":""},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"related-papers","dir":"","previous_headings":"","what":"Related Papers:","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo, Iñaki Ucar (2021). “Towards mathematical framework inform neural network modelling via polynomial regression.” Neural Networks, 142, 57-72. doi: 10.1016/j.neunet.2021.04.036 Pablo Morala, J. Alexandra Cifuentes, Rosa E. Lillo, Iñaki Ucar (2023). “NN2Poly: Polynomial Representation Deep Feed-Forward Artificial Neural Networks.” IEEE Transactions Neural Networks Learning Systems, (Early Access). doi: 10.1109/TNNLS.2023.3330328","code":""},{"path":"https://ibidat.github.io/nn2poly/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Neural Network Weights Transformation into Polynomial Coefficients","text":"installation GitHub requires remotes package.","code":"# install.packages(\"remotes\") remotes::install_github(\"IBiDat/nn2poly\")"},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":null,"dir":"Reference","previous_headings":"","what":"Add constraints to a neural network — add_constraints","title":"Add constraints to a neural network — add_constraints","text":"function sets neural network object constraints required nn2poly algorithm. Currently supported neural network frameworks keras/tensorflow luz/torch.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add constraints to a neural network — add_constraints","text":"","code":"add_constraints(object, type = c(\"l1_norm\", \"l2_norm\"), ...)"},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add constraints to a neural network — add_constraints","text":"object neural network object sequential form one supported frameworks. type Constraint type. Currently, l1_norm l2_norm supported. ... Additional arguments (unused).","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add constraints to a neural network — add_constraints","text":"nn2poly neural network object.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add constraints to a neural network — add_constraints","text":"Constraints added model object using callbacks specific framework. callbacks used training calling fit model. Specifically using callbacks applied end train batch. Models luz/torch need use luz_model_sequential helper order sequential model appropriate form.","code":""},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/reference/add_constraints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Add constraints to a neural network — add_constraints","text":"","code":"if (FALSE) { if (requireNamespace(\"keras\", quietly=TRUE)) {   # ---- Example with a keras/tensorflow network ----   # Build a small nn:   nn <- keras::keras_model_sequential()   nn <- keras::layer_dense(nn, units = 10, activation = \"tanh\", input_shape = 2)   nn <- keras::layer_dense(nn, units = 1, activation = \"linear\")    # Add constraints   nn_constrained <- add_constraints(nn, constraint_type = \"l1_norm\")    # Check that class of the constrained nn is \"nn2poly\"   class(nn_constrained)[1] }  if (requireNamespace(\"luz\", quietly=TRUE)) {   # ---- Example with a luz/torch network ----    # Build a small nn   nn <- luz_model_sequential(     torch::nn_linear(2,10),     torch::nn_tanh(),     torch::nn_linear(10,1)   )    # With luz/torch we need to setup the nn before adding the constraints   nn <- luz::setup(module = nn,     loss = torch::nn_mse_loss(),     optimizer = torch::optim_adam,   )    # Add constraints   nn <- add_constraints(nn)    # Check that class of the constrained nn is \"nn2poly\"   class(nn)[1] } }"},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":null,"dir":"Reference","previous_headings":"","what":"Polynomial evaluation — eval_poly","title":"Polynomial evaluation — eval_poly","text":"Evaluates one several polynomials given data.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Polynomial evaluation — eval_poly","text":"","code":"eval_poly(poly, newdata)"},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Polynomial evaluation — eval_poly","text":"poly List containing 2 items: labels values. labels: List integer vectors length (number cols) values, integer vector denotes combination variables associated coefficient value stored position values. , monomials polynomial. Note variables numbered 1 p, intercept represented 0. values: Matrix (can also vector single polynomial), column represents polynomial, number rows length labels, containing row value coefficient monomial given equivalent label position. Example: labels contains integer vector c(1,1,3) position 5, value stored values row 5 coefficient associated term x_1^2*x_3. newdata Input data matrix, vector dataframe. Number columns (elements vector) number variables polynomial (dimension p). Response variable predicted included.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Polynomial evaluation — eval_poly","text":"Returns matrix containing evaluation polynomials. column corresponds polynomial used row observation, meaning column vector corresponds results evaluating given data polynomial.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/eval_poly.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Polynomial evaluation — eval_poly","text":"Note function unstable subject change. Therefore exported documentations left available users can use needed simulate data using nn2poly:::eval_poly()","code":""},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a luz model composed of a linear stack of layers — luz_model_sequential","title":"Build a luz model composed of a linear stack of layers — luz_model_sequential","text":"Helper function build luz models sequential model, feeding stack luz layers.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a luz model composed of a linear stack of layers — luz_model_sequential","text":"","code":"luz_model_sequential(...)"},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a luz model composed of a linear stack of layers — luz_model_sequential","text":"... Sequence modules added.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a luz model composed of a linear stack of layers — luz_model_sequential","text":"nn_sequential module.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Build a luz model composed of a linear stack of layers — luz_model_sequential","text":"step needed can get activation functions layers neurons architecture easily nn2poly:::get_parameters(). Furthermore, step also needed able impose needed constraints using luz/torch framework.","code":""},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/reference/luz_model_sequential.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build a luz model composed of a linear stack of layers — luz_model_sequential","text":"","code":"if (FALSE) { if (requireNamespace(\"luz\", quietly=TRUE)) { # Create a NN using luz/torch as a sequential model # with 3 fully connected linear layers, # the first one with input = 5 variables, # 100 neurons and tanh activation function, the second # one with 50 neurons and softplus activation function # and the last one with 1 linear output. nn <- luz_model_sequential(   torch::nn_linear(5,100),   torch::nn_tanh(),   torch::nn_linear(100,50),   torch::nn_softplus(),   torch::nn_linear(50,1) )  nn  # Check that the nn is of class nn_squential class(nn) } }"},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain polynomial representation — nn2poly","title":"Obtain polynomial representation — nn2poly","text":"Implements main NN2Poly algorithm obtain polynomial representation trained neural network using weights Taylor expansion activation functions.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain polynomial representation — nn2poly","text":"","code":"nn2poly(   object,   max_order = 2,   keep_layers = FALSE,   taylor_orders = 8,   ...,   all_partitions = NULL )"},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain polynomial representation — nn2poly","text":"object object computation NN2Poly algorithm desired. Currently supports models following deep learning frameworks: tensorflow/keras models built sequential model. torch/luz models built sequential model. also supports named list input allows introduce hand model source. list length L (number hidden layers + 1) containing weights matrix layer. element list named activation function used layer. Currently supported activation functions \"tanh\", \"softplus\", \"sigmoid\" \"linear\". layer \\(l\\), expected shape matrices form \\((h_{(l-1)} + 1)*(h_l)\\), , number rows number neurons previous layer plus bias vector, number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer. bias vector first row. max_order integer determines maximum order forced final polynomial, discarding terms higher order naturally arise considering Taylor expansions allowed taylor_orders. keep_layers Boolean determines polynomials computed internal layers stored given output (TRUE), polynomials last layer needed (FALSE). Default set FALSE. taylor_orders integer vector length L sets degree Taylor expansion truncated layer. single value used, value set non linear layer 1 linear layer activation function. Default set 8. ... Ignored. all_partitions Optional argument containing needed multipartitions list lists lists. set NULL, nn2poly compute said multipartitions. step can computationally expensive chosen polynomial order dimension high. cases, encouraged multipartitions stored reused possible. Default set NULL.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Obtain polynomial representation — nn2poly","text":"Returns object class nn2poly. keep_layers = FALSE (default case), returns list two items: item named labels list integer vectors. vectors represent monomial polynomial, integer vector represents time one original variables appears term. example, vector c(1,1,2) represents term \\(x_1^2x_2\\). Note variables numbered 1 p, intercept represented  item named values contains matrix column contains coefficients polynomial associated output neuron. , neural network single output unit, matrix values single column multiple output units, matrix values several columns. row coefficient associated label position labels list. keep_layers = TRUE, returns list length number layers (represented layer_i), one another list input output elements. elements contains item explained . last layer output item element keep_layers = FALSE. polynomials obtained hidden layers needed represent NN can used explore insights NN.","code":""},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/reference/nn2poly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Obtain polynomial representation — nn2poly","text":"","code":"# Build a NN estructure with random weights, with 2 (+ bias) inputs, # 4 (+bias) neurons in the first hidden layer with \"tanh\" activation # function, 4 (+bias) neurons in the second hidden layer with \"softplus\", # and 1 \"linear\" output unit  weights_layer_1 <- matrix(rnorm(12), nrow = 3, ncol = 4) weights_layer_2 <- matrix(rnorm(20), nrow = 5, ncol = 4) weights_layer_3 <- matrix(rnorm(5), nrow = 5, ncol = 1)  # Set it as a list with activation functions as names nn_object = list(\"tanh\" = weights_layer_1,                  \"softplus\" = weights_layer_2,                  \"linear\" = weights_layer_3)  # Obtain the polynomial representation (order = 3) of that neural network final_poly <- nn2poly(nn_object, max_order = 3)  # Change the last layer to have 3 outputs (as in a multiclass classification) # problem weights_layer_4 <- matrix(rnorm(20), nrow = 5, ncol = 4)  # Set it as a list with activation functions as names nn_object = list(\"tanh\" = weights_layer_1,                  \"softplus\" = weights_layer_2,                  \"linear\" = weights_layer_4) # Obtain the polynomial representation of that neural network # In this case the output is formed by several polynomials with the same # structure but different coefficient values final_poly <- nn2poly(nn_object, max_order = 3)  # Polynomial representation of each hidden neuron is given by final_poly <- nn2poly(nn_object, max_order = 3, keep_layers = TRUE)"},{"path":"https://ibidat.github.io/nn2poly/reference/plot.nn2poly.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot method for nn2poly objects. — plot.nn2poly","title":"Plot method for nn2poly objects. — plot.nn2poly","text":"function takes polynomial (several ones) given nn2poly algorithm, plots absolute magnitude barplots able compare important coefficients.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot.nn2poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot method for nn2poly objects. — plot.nn2poly","text":"","code":"# S3 method for nn2poly plot(x, ..., n = NULL)"},{"path":"https://ibidat.github.io/nn2poly/reference/plot.nn2poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot method for nn2poly objects. — plot.nn2poly","text":"x nn2poly object, returned nn2poly algorithm. ... Ignored. n integer denoting number coefficients plotted, ordering absolute magnitude.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot.nn2poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot method for nn2poly objects. — plot.nn2poly","text":"plot showing n important coefficients.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot.nn2poly.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Plot method for nn2poly objects. — plot.nn2poly","text":"plot method represents polynomials final layer, even x generated using nn2poly() keep_layers=TRUE.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot.nn2poly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot method for nn2poly objects. — plot.nn2poly","text":"","code":"# --- Single polynomial output --- # Build a NN structure with random weights, with 2 (+ bias) inputs, # 4 (+bias) neurons in the first hidden layer with \"tanh\" activation # function, 4 (+bias) neurons in the second hidden layer with \"softplus\", # and 2 \"linear\" output units  weights_layer_1 <- matrix(rnorm(12), nrow = 3, ncol = 4) weights_layer_2 <- matrix(rnorm(20), nrow = 5, ncol = 4) weights_layer_3 <- matrix(rnorm(5), nrow = 5, ncol = 1)  # Set it as a list with activation functions as names nn_object = list(\"tanh\" = weights_layer_1,                  \"softplus\" = weights_layer_2,                  \"linear\" = weights_layer_3)  # Obtain the polynomial representation (order = 3) of that neural network final_poly <- nn2poly(nn_object, max_order = 3)  # Plot all the coefficients, one plot per output unit plot(final_poly)   # Plot only the 5 most important coeffcients (by absolute magnitude) # one plot per output unit plot(final_poly, n = 5)   # --- Multiple output polynomials --- # Build a NN structure with random weights, with 2 (+ bias) inputs, # 4 (+bias) neurons in the first hidden layer with \"tanh\" activation # function, 4 (+bias) neurons in the second hidden layer with \"softplus\", # and 2 \"linear\" output units  weights_layer_1 <- matrix(rnorm(12), nrow = 3, ncol = 4) weights_layer_2 <- matrix(rnorm(20), nrow = 5, ncol = 4) weights_layer_3 <- matrix(rnorm(10), nrow = 5, ncol = 2)  # Set it as a list with activation functions as names nn_object = list(\"tanh\" = weights_layer_1,                  \"softplus\" = weights_layer_2,                  \"linear\" = weights_layer_3)  # Obtain the polynomial representation (order = 3) of that neural network final_poly <- nn2poly(nn_object, max_order = 3)  # Plot all the coefficients, one plot per output unit plot(final_poly)   # Plot only the 5 most important coeffcients (by absolute magnitude) # one plot per output unit plot(final_poly, n = 5)"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots a comparison between two sets of points. — plot_diagonal","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"points come predictions NN PM line (plot.line = TRUE) displayed, case method exhibit asymptotic behavior, points fall line.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"","code":"plot_diagonal(   x_axis,   y_axis,   xlab = NULL,   ylab = NULL,   title = NULL,   plot.line = TRUE )"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"x_axis Values plot x axis. y_axis Values plot y axis. xlab Lab x axis ylab Lab y axis. title Title plot. plot.line red line slope = 1 intercept = 0 plotted.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_diagonal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots a comparison between two sets of points. — plot_diagonal","text":"Plot (ggplot object).","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":null,"dir":"Reference","previous_headings":"","what":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"Function allows take NN data input values plot distribution data activation potentials (sum input values * weights) neurons together layer Taylor expansion used activation functions. layer 'linear' (usually output), layer approximation Taylor expansion needed.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"","code":"plot_taylor_and_activation_potentials(   object,   data,   max_order,   taylor_orders = 8,   constraints,   taylor_interval = 1.5,   ... )"},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"object object computation NN2Poly algorithm desired. Currently supports models following deep learning frameworks: tensorflow/keras models built sequential model. torch/luz models built sequential model. also supports named list input allows introduce hand model source. list length L (number hidden layers + 1) containing weights matrix layer. element list named activation function used layer. Currently supported activation functions \"tanh\", \"softplus\", \"sigmoid\" \"linear\". layer \\(l\\), expected shape matrices form \\((h_{(l-1)} + 1)*(h_l)\\), , number rows number neurons previous layer plus bias vector, number columns number neurons current layer L. Therefore, column corresponds weight vector affecting neuron layer. bias vector first row. data Matrix data frame containing predictor variables (X) used input compute activation potentials. response variable column included. max_order integer determines maximum order forced final polynomial, discarding terms higher order naturally arise considering Taylor expansions allowed taylor_orders. taylor_orders integer vector length L sets degree Taylor expansion truncated layer. single value used, value set non linear layer 1 linear layer activation function. Default set 8. constraints Boolean parameter determining NN constrained (TRUE) (FALSE). modifies plots title show \"constrained\" \"unconstrained\" respectively. taylor_interval optional parameter determining interval Taylor expansion represented. Default 1.5. ... Additional parameters.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/plot_taylor_and_activation_potentials.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plots activation potentials and Taylor expansion. — plot_taylor_and_activation_potentials","text":"list plots.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict method for nn2poly objects. — predict.nn2poly","title":"Predict method for nn2poly objects. — predict.nn2poly","text":"Predicted values obtained nn2poly object given data.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict method for nn2poly objects. — predict.nn2poly","text":"","code":"# S3 method for nn2poly predict(object, newdata, layers = NULL, ...)"},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict method for nn2poly objects. — predict.nn2poly","text":"object Object class inheriting 'nn2poly'. newdata Input data matrix, vector dataframe. Number columns (elements vector) number variables polynomial (dimension p). Response variable predicted included. layers Vector containing chosen layers object evaluated. set NULL, layers computed. Default set NULL. ... arguments passed methods.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict method for nn2poly objects. — predict.nn2poly","text":"Returns matrix list matrices evaluation polynomial layer given provided object class nn2poly. object contains polynomials last layer, given nn2poly(object, keep_layers = FALSE), output matrix evaluation data point polynomial. matrix, column represents evaluation polynomial column corresponds point new data evaluated. object contains internal polynomials also, given nn2poly(object, keep_layers = TRUE), output list layers (represented layer_i), one another list input output elements, one contains matrix evaluation \"input\" \"output\" polynomial given layer, explained case without internal polynomials.","code":""},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict method for nn2poly objects. — predict.nn2poly","text":"Internally uses eval_poly() obtain predictions. However, works objects class nn2poly eval_poly() can used manually created polynomial list form. object contains internal polynomials also, given nn2poly(object, keep_layers = TRUE), important note two polynomial items per layer (input/output). polynomial items also contain several polynomials structure, one per neuron layer, stored matrix rows $values. Please see NN2Poly original paper details. Note also \"linear\" layers contain input output results Taylor expansion used thus polynomials also . , situation evaluating multiple layers provide final layer \"input\" \"output\" even , consistency.","code":""},{"path":[]},{"path":"https://ibidat.github.io/nn2poly/reference/predict.nn2poly.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict method for nn2poly objects. — predict.nn2poly","text":"","code":"# Build a NN structure with random weights, with 2 (+ bias) inputs, # 4 (+bias) neurons in the first hidden layer with \"tanh\" activation # function, 4 (+bias) neurons in the second hidden layer with \"softplus\", # and 1 \"linear\" output unit  weights_layer_1 <- matrix(rnorm(12), nrow = 3, ncol = 4) weights_layer_2 <- matrix(rnorm(20), nrow = 5, ncol = 4) weights_layer_3 <- matrix(rnorm(5), nrow = 5, ncol = 1)  # Set it as a list with activation functions as names nn_object = list(\"tanh\" = weights_layer_1,                  \"softplus\" = weights_layer_2,                  \"linear\" = weights_layer_3)  # Obtain the polynomial representation (order = 3) of that neural network final_poly <- nn2poly(nn_object, max_order = 3)  # Define some new data, it can be vector, matrix or dataframe newdata <- matrix(rnorm(10), ncol = 2, nrow = 5)  # Predict using the obtained polynomial predict(object = final_poly, newdata = newdata) #> [1] -21.344227  -1.110583   4.760369  17.520142   3.465613  # Change the last layer to have 3 outputs (as in a multiclass classification) # problem weights_layer_4 <- matrix(rnorm(20), nrow = 5, ncol = 4)  # Set it as a list with activation functions as names nn_object = list(\"tanh\" = weights_layer_1,                  \"softplus\" = weights_layer_2,                  \"linear\" = weights_layer_4)  # Obtain the polynomial representation of that neural network # Polynomial representation of each hidden neuron is given by final_poly <- nn2poly(nn_object, max_order = 3, keep_layers = TRUE)  # Define some new data, it can be vector, matrix or dataframe newdata <- matrix(rnorm(10), ncol = 2, nrow = 5)  # Predict using the obtained polynomials (for all layers) predict(object = final_poly, newdata = newdata) #> $layer_1 #> $layer_1$input #>            [,1]       [,2]       [,3]       [,4] #> [1,] -2.2982951  6.8322228  1.8701077 -6.3762748 #> [2,] -0.3059160 -2.3812127 -0.6939258  0.6545819 #> [3,] -1.2929450  0.1069224 -0.7602996 -1.1709467 #> [4,] -0.8006397  1.8367722  1.1853521 -2.6322438 #> [5,] -2.2192979 -0.8395190 -2.9351919 -0.2643359 #>  #> $layer_1$output #>            [,1]        [,2]       [,3]       [,4] #> [1,]  1.4387492 -23.4022635 -4.1317274 137.490301 #> [2,] -0.2997964   1.3914959 -0.5290092  -5.177951 #> [3,] -0.7009298   0.5874071 -0.6269358  -1.741965 #> [4,] -0.6390632   1.4971654  0.6800348   4.604232 #> [5,]  0.7243468  11.9023351  1.4406594 -26.317729 #>  #>  #> $layer_2 #> $layer_2$input #>            [,1]        [,2]      [,3]       [,4] #> [1,] -72.727031 110.2325201 13.617263 -17.218756 #> [2,]   5.065385  -2.9620557  1.477528  -1.842932 #> [3,]   2.989167  -0.5401141  2.421410  -1.633603 #> [4,]  -1.901821   4.0391400  4.420749  -4.052163 #> [5,]  16.343112 -19.1144520  3.197571  -7.799348 #>  #> $layer_2$output #>             [,1]       [,2]      [,3]       [,4] #> [1,] -52.1173831 46.8111395 46.594401 21.1520303 #> [2,]   4.2759609 -0.5658585  1.216993 -0.3421481 #> [3,]   2.9900519  0.3873725  2.675544  0.1314888 #> [4,]  -0.8047415  2.5322573  5.052194  0.2698241 #> [5,]  15.0024067 -9.3772319  5.580790 -2.0441074 #>  #>  #> $layer_3 #> $layer_3$input #>          [,1]        [,2]      [,3]        [,4] #> [1,] 31.41444 -126.616883 91.769585  77.6168398 #> [2,] 11.92136    5.368994  3.080517  -1.9868663 #> [3,] 13.45481    2.070456  5.517163  -0.3701399 #> [4,] 11.80241   -5.082487  8.698053   3.9712499 #> [5,] 44.04145   18.314612  4.046083 -18.4261342 #>  #> $layer_3$output #>          [,1]        [,2]      [,3]        [,4] #> [1,] 31.41444 -126.616883 91.769585  77.6168398 #> [2,] 11.92136    5.368994  3.080517  -1.9868663 #> [3,] 13.45481    2.070456  5.517163  -0.3701399 #> [4,] 11.80241   -5.082487  8.698053   3.9712499 #> [5,] 44.04145   18.314612  4.046083 -18.4261342 #>  #>   # Predict using the obtained polynomials (for chosen layers) predict(object = final_poly, newdata = newdata, layers = c(2,3)) #> $layer_2 #> $layer_2$input #>            [,1]        [,2]      [,3]       [,4] #> [1,] -72.727031 110.2325201 13.617263 -17.218756 #> [2,]   5.065385  -2.9620557  1.477528  -1.842932 #> [3,]   2.989167  -0.5401141  2.421410  -1.633603 #> [4,]  -1.901821   4.0391400  4.420749  -4.052163 #> [5,]  16.343112 -19.1144520  3.197571  -7.799348 #>  #> $layer_2$output #>             [,1]       [,2]      [,3]       [,4] #> [1,] -52.1173831 46.8111395 46.594401 21.1520303 #> [2,]   4.2759609 -0.5658585  1.216993 -0.3421481 #> [3,]   2.9900519  0.3873725  2.675544  0.1314888 #> [4,]  -0.8047415  2.5322573  5.052194  0.2698241 #> [5,]  15.0024067 -9.3772319  5.580790 -2.0441074 #>  #>  #> $layer_3 #> $layer_3$input #>          [,1]        [,2]      [,3]        [,4] #> [1,] 31.41444 -126.616883 91.769585  77.6168398 #> [2,] 11.92136    5.368994  3.080517  -1.9868663 #> [3,] 13.45481    2.070456  5.517163  -0.3701399 #> [4,] 11.80241   -5.082487  8.698053   3.9712499 #> [5,] 44.04145   18.314612  4.046083 -18.4261342 #>  #> $layer_3$output #>          [,1]        [,2]      [,3]        [,4] #> [1,] 31.41444 -126.616883 91.769585  77.6168398 #> [2,] 11.92136    5.368994  3.080517  -1.9868663 #> [3,] 13.45481    2.070456  5.517163  -0.3701399 #> [4,] 11.80241   -5.082487  8.698053   3.9712499 #> [5,] 44.04145   18.314612  4.046083 -18.4261342 #>  #>"},{"path":"https://ibidat.github.io/nn2poly/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics fit","code":""},{"path":"https://ibidat.github.io/nn2poly/news/index.html","id":"nn2poly-010","dir":"Changelog","previous_headings":"","what":"nn2poly 0.1.0","title":"nn2poly 0.1.0","text":"Initial release package.","code":""}]
